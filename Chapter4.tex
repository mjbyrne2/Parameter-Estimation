\chapter{Parameter estimation methods} \label{ch:Parameter estimation methods}

Perhaps the simplest method of selecting a regularization parameter is to use
\begin{equation}
\regparam_{\text{best}} := \argmin_{\regparam \geq 0} \|\fVec - \regf\|^2.
\label{eq:Minimize Error}
\end{equation}
Here ``best'' is used to indicate the fact that the parameter is chosen to minimize the error of the regularized solution. The primary disadvantage of using this method is that the true solution $\fVec$ must be known. Not only would knowing the true solution render the process of finding a regularized solution pointless, but in practice a true solution is not known. This motivates the use of other methods, which do not rely upon knowledge of a true solution. The three such methods considered are the unbiased predictive risk estimator method (Section \ref{sec:Unbiased Predictive Risk Estimator}), the generalized cross validation method (Section \ref{sec:Generalized Cross Validation}), and the discepancy principle method (Section \ref{sec:Discrepancy Principle}). Since true solutions are known for use in the numerical examples, the method on line \eqref{eq:Minimize Error} will be used as a benchmark for comparing the other three methods.

\section{Unbiased Predictive Risk Estimator} \label{sec:Unbiased Predictive Risk Estimator}
The unbiased predictive risk estimator (UPRE) method \cite{Mallows1973} is derived by considering the quantity
\[\pVec(\regparam) := \kMat(\regf - \fVec).\]
This quantity $\pVec(\regparam)$ is known as the \textit{predictive error}, and is an alternative to solution error defined as $\regf - \fVec$. Given the above definition, the mean squared norm of the predictive error is
\[\frac{1}{n}\|\pVec(\regparam)\|^2 = \frac{1}{n}\|\kMat(\regf - \fVec)\|^2\]
which is called the predictive risk.  As a first step in deriving the UPRE method, assume that the noise $\noiseVec$ is a random vector, instead of a realization of a random vector. Direct consequences of this assumption are that $\gVec$ and $\regf$ are random vectors and the predictive risk $(1/n)\|\pVec(\regparam)\|^2$ is a random variable. \par
Next, an $n \times n$ matrix $\A$ is defined as $\A = \kMat\R$ where $\R$ is a regularization matrix. The notation $\A$ is chosen to indicate that the matrix depends upon the regularization parameter contained in $\R$. Using the influence matrix with $\regf = \R\gnoiseVec$, the predictive error can be rewritten:
\begin{align*}
\pVec(\regparam) &= \kMat\regf - \kMat\fVec \\
&= \A\gnoiseVec - \kMat\fVec \\
&= \A(\kMat\fVec + \noiseVec) - \kMat\fVec \\
&= (\A - I)\kMat\fVec + \A\noiseVec
\end{align*}
By the assumption that $\noiseVec$ is a discrete white noise vector, the Trace Lemma can be utilized to obtain an expression for the expected value of predictive risk.

\begin{lemma}[{{\cite[p.~98]{Vogel:2002}}}]
Let $f \in \mathcal{H}$, where $\mathcal{H}$ is a deterministic real Hilbert space, let $\noiseVec$ be a discrete noise vector with $\noiseVec \sim \mathcal{N}(0,\noiseSD^2)$, and let $B: \mathbb{R}^n \rightarrow \mathcal{H}$ be a bounded linear operator. Then
\[\E(\|f + B\noise\|_{\mathcal{H}}^2) = \|f\|_{\mathcal{H}}^2 + \noiseSD^2\trace({B^*}B)\]
where $B^*$ denotes the adjoint of $B$.
\end{lemma}
\begin{proof}
By the linearity of inner products and the expected value operator,
\[\E(\|f + B\noiseVec\|_{\mathcal{H}}^2) = \E(\langle f + B\noiseVec, f + B\noiseVec\rangle_{\mathcal{H}}) = \E(\|f\|_{\mathcal{H}}^2) + 2\E(\langle f, B\noiseVec\rangle_{\mathcal{H}}) + \E(\langle B\noiseVec, B\noiseVec\rangle_{\mathcal{H}}).\]
The term $\E(\|f\|_{\mathcal{H}}^2)$ reduces to $\|f\|_{\mathcal{H}}^2$ because $f$ is an element of a deterministic Hilbert space. Next, the inner products can be rewritten using the adjoint of $B$:
\begin{align*}
\E(\|f + B\noiseVec\|_{\mathcal{H}}^2) &= \|f\|_{\mathcal{H}}^2 + 2\E(\langle f, B\noiseVec\rangle_{\mathcal{H}}) + \E(\langle B\noiseVec, B\noiseVec\rangle_{\mathcal{H}}) \\
&= \|f\|_{\mathcal{H}}^2 + 2\E(\trans{({\ctrans{B}}f)}\noiseVec) + \E({\trans{\noiseVec}}{\ctrans{B}}B\noiseVec) \\
&= \|f\|_{\mathcal{H}}^2 + 2\sum_{j=1}^n ({B^*}f)_j \E(\noise_j) + \sum_{j=1}^n\sum_{k=1}^n ({\ctrans{B}}B)_{jk} \E({\noise_j}{\noise_k})
\end{align*}
Since $\noiseVec \sim \mathcal{N}(0,\noiseSD^2)$, the expected values of $\noise_j$ and ${\noise_j}{\noise_k}$ are zero and $\noiseSD^2\delta_{jk}$, respectively. Therefore the second term above is zero and the third term is a summation expression for $\noiseSD^2\trace({B^*}B)$.
\end{proof}

% Complex version
%\begin{proof}
%By the linearity of inner products and the expected value operator,
%\begin{align*}
%\E(\|f + B\dft{\noiseVec}\|_{\mathcal{H}}^2) &= \E(\langle f + B\noiseVec, f + B\noiseVec\rangle_{\mathcal{H}}) \\
%&= \E(\|f\|_{\mathcal{H}}^2) + \E\left(\langle f, B\noiseVec\rangle_{\mathcal{H}} + \overline{\langle f, B\noiseVec\rangle_{\mathcal{H}}}\right) + \E(\langle B\noiseVec, B\noiseVec\rangle_{\mathcal{H}}) \\
%&= \E(\|f\|_{\mathcal{H}}^2) + 2\E\left(\Re(\langle f, B\noiseVec\rangle_{\mathcal{H}})\right) + \E(\langle B\noiseVec, B\noiseVec\rangle_{\mathcal{H}}) \\
%&= \E(\|f\|_{\mathcal{H}}^2) + 2\Re\left(\E(\langle f, B\noiseVec\rangle_{\mathcal{H}})\right) + \E(\langle B\noiseVec, B\noiseVec\rangle_{\mathcal{H}}).
%\end{align*}
%The term $\E(\|f\|_{\mathcal{H}}^2)$ reduces to $\|f\|_{\mathcal{H}}^2$ because $f$ is an element of a deterministic Hilbert space. The inner products can be rewritten using the adjoint of $B$:
%\begin{align*}
%\E(\|f + B\noiseVec\|_{\mathcal{H}}^2) &= \|f\|_{\mathcal{H}}^2 + 2\Re\left(\E(\langle f, B\noiseVec\rangle_{\mathcal{H}})\right) + \E(\langle B\noiseVec, B\noiseVec\rangle_{\mathcal{H}}) \\
%&= \|f\|_{\mathcal{H}}^2 + 2\Re\left(\E(\langle \trans{B}f, \noiseVec\rangle)\right) + \E(\langle \trans{B}B\noiseVec,\noiseVec\rangle) \\
%&= \|f\|_{\mathcal{H}}^2 + 2\Re\left(\E(\trans{\noiseVec}\trans{B}f)\right) + \E(\trans{\noiseVec}{\trans{B}}B\noiseVec) \\
%&= \|f\|_{\mathcal{H}}^2 + 2\sum_{j=1}^{N} \E(\noise_j)\Re((\trans{B}f)_j) + \sum_{j=1}^{N}\sum_{k=0}^{N-1} \E(\noise_j\noise_k)(\trans{B}B)_{j,k}.
%\end{align*}
%Since $\noiseVec \sim \mathcal{N}(\bm{\mu},\Sigma)$, $\E(\noise_j) = \mu_j$ and $\E(\noise_j\noise_k) = \Sigma_{j,k}$. Thus
%\[\E(\|f + B\noiseVec\|_{\mathcal{H}}^2) = \|f\|_{\mathcal{H}}^2 + 2\sum_{j=1}^{N} \mu_j\Re((\trans{B}f)_j) + \sum_{j=1}^{N}\sum_{k=1}^{N} \Sigma_{j,k}(\trans{B}B)_{j,k}.\]
%The double summation can be written using the Frobenius inner product as $\langle \Sigma, \trans{B}B\rangle_F := \trace(\trans{\Sigma}\trans{B}B) = \trace(\trans{\Sigma}\trans{B}B)$. The symmetry of $\Sigma$ and the cyclic property of the trace operator yield the final result:
%\[\E(\|f + B\noiseVec\|_{\mathcal{H}}^2) = \|f\|_{\mathcal{H}}^2 + 2\sum_{j=1}^{N} \mu_j\Re((\trans{B}f)_j) + \trace(B\Sigma\trans{B}).\]
%\end{proof}

\noindent Applying the Trace Lemma to the expression for predictive risk yields
\[\E\left(\frac{1}{n}\|\pVec(\regparam)\|^2\right) = \frac{1}{n}\E\left(\|(\A-I)\kMat\fVec + \A\noiseVec\|^2\right) = \frac{1}{n}\|(\A-I)\kMat\fVec\|^2 + \frac{\noiseSD^2}{n}\trace({\trans{\A}}\A).\]
If Tikhonov regularization is used, then the influence matrix $\A$ is $\kMat(\ctrans{\kMat}\kMat + \regparam{\ctrans{D}}D)^{-1}\ctrans{\kMat}$. The matrix $(\ctrans{\kMat}\kMat + \regparam{\ctrans{D}}D)^{-1}$ is symmetric as a result of $\ctrans{\kMat}\kMat$ and $\regparam\ctrans{D}D$ being individually symmetric, and thus the corresponding influence matrix $\A$ is symmetric.  With a symmetric matrix $\A$, the expected value of predictive risk is simplified to
\begin{equation}
\label{eq:PR}
\E\left(\frac{1}{n}\|\pVec(\regparam)\|^2\right) = \frac{1}{n}\|(\A-I)\kMat\fVec\|^2 + \frac{\noiseSD^2}{n}\trace(\A^2).
\end{equation}
\indent The last step in the derivation of the UPRE method is to introduce the \textit{regularized residual}, which is defined as $\regres = \kMat\regf - \gnoiseVec$. The regularized residual is important because it is also used in the derivation of the generalized cross validation and discrepancy principle methods. Using the influence matrix $\A$, the expression for $\regres$ can also be written as
\[\regres = (\A-I)\gnoiseVec = (\A-I)(\kMat\fVec + \noiseVec) = (\A-I)\kMat\fVec + (\A-I)\noiseVec.\]
By the Trace Lemma and the expression for $\regres$, the expected value of $(1/n)\|\regres\|^2$ is
\[\E\left(\frac{1}{n}\|\regres\|^2\right) = \frac{1}{n}\|(\A-I)\kMat\fVec\|^2 + \frac{\noiseSD^2}{n}\trace({\trans{(\A-I)}}(\A-I))\]
For symmetric $\A$, the term $\trans{(\A-I)}(\A-I)$ becomes $(\A-I)^2 = \A^2 - 2\A + I$ and so by the linearity of the trace operator,
\begin{equation}
\label{eq:RR}
\E\left(\frac{1}{n}\|\regres\|^2\right) = \frac{1}{n}\|(\A-I)\kMat\fVec\|^2 + \frac{\noiseSD^2}{n}\trace(\A^2) - \frac{2\noiseSD^2}{n}\trace(\A) + \noiseSD^2.
\end{equation}
By comparing \eqref{eq:PR} and \eqref{eq:RR}, the equation for the expected value of $(1/n)\|\pVec(\regparam)\|^2$ can be expressed as
\[\E\left(\frac{1}{n}\|\pVec(\regparam)\|^2\right) = \E\left(\frac{1}{n}\|\regres\|^2\right) + \frac{2\noiseSD^2}{n}\trace(\A) - \noiseSD^2.\]
The UPRE is defined to be
\begin{equation}
\label{eq:UPRE}
\U_n(\regparam) = \frac{1}{n}\|\regres\|^2 + \frac{2\noiseSD^2}{n}\trace(\A) - \noiseSD^2
\end{equation}
and the UPRE method is to pick $\regparam_{\text{UPRE}} = \argmin_{\regparam \geq 0} \U(\regparam)$. The subscript of $\U_n(\regparam)$ indicates the function also depends upon $n$. \par
% We first derive the UPRE function for Tikhonov regularization under the more general condition that $\noiseVec \sim \mathcal{N}(\bm{0},\Sigma)$. To this end, we can then use the following lemma, which is a modification of the Trace Lemma stated in \cite[p.~98]{Vogel:2002}.
% \begin{lemma}
% \label{lem:Generalized Trace Lemma}
% Let $\fVec \in \mathbb{R}^m$ be a constant vector, $\noiseVec$ be a real random $n$-vector with $\noiseVec \sim \mathcal{N}(\bm{\mu},\Sigma)$, $B \in \mathbb{R}^{m \times n}$, and let $\langle \cdot,\cdot \rangle$ be the standard Euclidean inner product. Then
% \[\E(\|\fVec + B\noiseVec\|_2^2) = \|\fVec\|_2^2 + 2\sum_{j=1}^{n} (\trans{\fVec}B)_j \mu_j + \trace\left(B\Sigma\trans{B}\right).\]
% \end{lemma}
% \begin{proof}
% By the linearity of the expected value operator and inner product,
% \[\E(\|\fVec + B\noiseVec\|_2^2) = \E(\langle \fVec + B\noiseVec, \fVec + B\noiseVec\rangle) =  \E(\|\fVec\|_2^2) + 2\E(\langle \fVec, B\noiseVec\rangle) + \E(\|B\noiseVec\|_2^2).\]
% $\E(\|\fVec\|_2^2) = \|\fVec\|_2^2$ because $\fVec$ is a constant vector. Moreover, the definition of the Euclidean inner product can be used to write $\E(\langle \fVec, B\noiseVec\rangle)$ as $\E(\sum_{j=1}^n (\trans{\fVec}B)_j \noise_j) = \sum_{j=1}^n (\trans{\fVec}B)_j \E(\noise_j)$. Thus,
% \begin{align*}
% \E(\|\fVec + B\noiseVec\|_2^2) &= \|\fVec\|_2^2 + 2\sum_{j=1}^{n} (\trans{\fVec}B)_j \E(\noise_j) + \E(\|B\noiseVec\|_2^2) \\
% &= \|\fVec\|_2^2 + 2\sum_{j=1}^{n} (\trans{\fVec}B)_j \mu_j + \E(\|B\noiseVec\|_2^2).
% \end{align*}
% Focusing on $\E(\|B\noiseVec\|_2^2)$, we can write
% \[\E(\|B\noiseVec\|_2^2) = \E\left(\sum_{j=1}^n (B\noiseVec)_j^2\right) = \sum_{j=1}^n \E((B\noiseVec)_j^2) = \sum_{j=1}^n \E(y_j^2)\]
% where $\mathbf{y} = B\noiseVec$.
% Since $\noiseVec \sim \mathcal{N}(\bm{\mu},\Sigma)$, $\mathbf{y} \sim \mathcal{N}(B\bm{\mu},B\Sigma\trans{B})$ \cite{Rao1973}. Lastly, $\E(y_j^2) = \Var(y_j) = (B\Sigma\trans{B})_{j,j}$ for each $j = 1,\ldots,n$. Therefore $\sum_{j=1}^n \E(y_j^2) = \sum_{j=1}^n \Var(y_j) = \trace(B\Sigma\trans{B})$ and
% \[\E(\|\fVec + B\noiseVec\|_2^2) = \|\fVec\|_2^2 + 2\sum_{j=1}^{n} (\trans{\fVec}B)_j \mu_j + \trace(B\Sigma\trans{B}).\]
% \end{proof}

% \noindent Applying Lemma \ref{lem:Generalized Trace Lemma} to the norm of $\pVec(\regparam)$ and noting that $\E(\noiseVec) = \zeroVec$ produces
% \begin{equation}
% \label{eq:New Predicitive Risk 1}
% \E\left(\frac{1}{\mA}\left\|\pVec(\regparam)\right\|_2^2\right) = \frac{1}{\mA}\left\|\left(\A - I_{\mA}\right)A\xVec\right\|_2^2 + \frac{1}{\mA}\trace\left(\A\Sigma\trans{A}(\regparam)\right).
% \end{equation}
% The regularized residual $\rVec(\regparam)$ can be rewritten as
% \begin{equation}
% \label{eq:New Regularized Residual}
% \rVec(\regparam) = \left(\A - I_{\mA}\right)A\xVec + \left(\A - I_{\mA}\right)\noiseVec,
% \end{equation}
% and so applying Lemma \ref{lem:Generalized Trace Lemma} to the norm of $\rVec(\regparam)$ yields
% \[\E\left(\frac{1}{\mA}\left\|\rVec(\regparam)\right\|_2^2\right) = \frac{1}{\mA}\left\|\left(\A - I_{\mA}\right)A\xVec\right\|_2^2 + \frac{1}{\mA}\trace\left(\trans{\left(\A - I_{\mA}\right)}\Sigma\left(\A - I_{\mA}\right)\right).\]
% The trace term can be expanded as
% \begin{align*}
%     &\trace\left(\trans{\left(\A - I_{\mA}\right)}\Sigma\left(\A - I_{\mA}\right) \right) \\
%     &= \trace\left(\trans{A}(\regparam)\Sigma\A\right) - \trace\left(\trans{A}(\regparam)\Sigma\right) - \trace\left(\Sigma\A\right) + \trace\left(\Sigma\right).
% \end{align*}
% The cyclic property of the trace operator and the fact that $\Sigma$ and $\A = A(\trans{A}A + \regparam^2\trans{L}L)^{-1}\trans{A}$ are symmetric matrices give
% \[\trace\left(\trans{\left(\A - I_{\mA}\right)}\Sigma\left(\A - I_{\mA}\right)\right) = \trace\left(\A\Sigma\trans{A}(\regparam)\right) - 2\trace\left(\Sigma\A\right) + \trace\left(\Sigma\right),\]
% and so \eqref{eq:New Predicitive Risk 1} can be expressed as
% \begin{equation}
% \label{eq:New Predictive Risk 2}
% \E\left(\frac{1}{\mA}\left\|\pVec(\regparam)\right\|_2^2\right) = \E\left(\frac{1}{\mA}\left\|\rVec(\regparam)\right\|_2^2\right) + \frac{2}{\mA}\trace\left(\Sigma\A\right) - \frac{1}{\mA}\trace\left(\Sigma\right).
% \end{equation}
% Analogous to the standard UPRE function, we can define $\U(\regparam)$ as
% \begin{equation}
% \label{eq:UPRE 2}
% \U(\regparam) = \frac{1}{\mA}\left\|\rVec(\regparam)\right\|_2^2 + \frac{2}{\mA}\trace\left(\Sigma\A\right) - \frac{1}{\mA}\trace\left(\Sigma\right).
% \end{equation}
% so that $\UBig(\regparam)$ is an unbiased estimator of predictive risk. The standard UPRE function \eqref{eq:UPRE} is recovered from \eqref{eq:UPRE 2} if $\Sigma = \noiseSD^2 I_{\mA}$ (which is Assumption \ref{Assumption_Noise} with $R = 1$).
Since the DFT is a the primary tool in the report, a spectral form of the UPRE function (one that involves DFT's) is desirable, as are spectral forms of the GCV and discrepancy principal functions in Sections \ref{sec:Generalized Cross Validation} and \ref{sec:Discrepancy Principle}. To derive a spectral form of \eqref{eq:UPRE}, first recall that for Tikhonov regularization, $\A = \kMat(\ctrans{\kMat}\kMat + \regparam\ctrans{D}D)^{-1}\ctrans{\kMat}$. Assuming $\kMat$ is circulant, \eqref{eq:CircDiag} gives $\kMat = \ctrans{F}\Delta{F}$ where $\Delta = \diag(\sqrt{n}\dft{\kVec})$. If $D = \ctrans{F}\Lambda{F}$ as well (with $\Lambda = \diag(\sqrt{n}\dft{\dVec})$), then
\begin{align*}
\A &= \kMat(\ctrans{\kMat}\kMat + \regparam{\ctrans{D}}D)^{-1}\ctrans{\kMat} \\
&= \ctrans{F}\Delta{F}(\ctrans{(\ctrans{F}\Delta{F})} \ctrans{F}\Delta{F} + \regparam\ctrans{(\ctrans{F}\Lambda{F})} \ctrans{F}\Lambda{F})^{-1}\ctrans{(\ctrans{F}\Delta{F})} \\
&= \ctrans{F}\Delta{F}(\ctrans{F}\ctrans{\Delta}\Delta{F} + \regparam{\ctrans{F}\ctrans{\Lambda}\Lambda{F}})^{-1}\ctrans{F}\ctrans{\Delta}{F} \\
&= \ctrans{F}\Delta{F}(\ctrans{F}(\ctrans{\Delta}\Delta + \regparam\ctrans{\Lambda}\Lambda)F)^{-1}\ctrans{F}\ctrans{\Delta}{F} \\
&= \ctrans{F}\Delta{F}\ctrans{F}(\ctrans{\Delta}\Delta + \regparam\ctrans{\Lambda}\Lambda)^{-1}F\ctrans{F}\ctrans{\Delta}{F} \\
&= \ctrans{F}\Delta(\ctrans{\Delta}\Delta + \regparam\ctrans{\Lambda}\Lambda)^{-1}\ctrans{\Delta}{F}.
\end{align*}
The matrix $\Delta(\ctrans{\Delta}\Delta + \regparam\ctrans{\Lambda}\Lambda)^{-1}\ctrans{\Delta}$ is diagonal, and so its diagonal entries are the the eigenvalues of $\A$. Then by definition of $\Delta$ and $\Lambda$, the $j$th diagonal entry of $\Delta(\ctrans{\Delta}\Delta + \regparam\ctrans{\Lambda}\Lambda)^{-1}\ctrans{\Delta}$ is $|\dft{k}_j|^2/(|\dft{k}_j|^2 + \regparam|\dft{d}_j|^2)$. Therefore,
\begin{equation}
\trace(\A) = \sum_{j = 0}^{n-1} \frac{|\dft{k}_j|^2}{|\dft{k}_j|^2 + \regparam|\dft{d}_j|^2} = \sum_{j = 0}^{n-1} \filt(\regparam,|\dft{k}_j|)
\label{eq:TraceUPRE}
\end{equation}
where $\filt$ is the Tikhonov filter function \eqref{eq:TikFilt}. Since the operator $D$ is fixed, the vector $\dft{\dVec}$ can be pre-computed; this is reflected by the notation $\filt(\regparam,|\dft{k}_j|)$. \par
Next, the definition of $\regres$ gives
\[\frac{1}{n}\|\regres\|^2 = \frac{1}{n}\|\kMat\regf - \gnoiseVec\|^2 = \frac{1}{n}\sum_{j = 0}^{n-1} |\dft{k}_j\dft{(\regf)}_j - \dft{\gnoiseVec}_j|^2,\]
and \eqref{eq:TikFiltPsi} and \eqref{eq:TikSol} then produce
\begin{equation}
\frac{1}{n}\|\regres\|^2 = \frac{1}{n}\sum_{j = 0}^{n-1} |\filt(\regparam,|\dft{k}_j|)\dft{\gnoiseVec}_j - \dft{\gnoiseVec}_j|^2 = \frac{1}{n}\sum_{j = 0}^{n-1} |\dft{\gnoiseVec}_j|^2(\mfilt(\regparam,|\dft{k}_j|))^2.
\label{eq:RegResNorm}
\end{equation}
Combining \eqref{eq:TraceUPRE} and \eqref{eq:RegResNorm} produces the spectral form of the UPRE function:
\begin{equation}
\U(\regparam) = \frac{1}{n}\sum_{j = 0}^{n-1} |\dft{\gnoiseVec}_j|^2(\mfilt(\regparam,|\dft{k}_j|))^2 + \frac{2\noiseSD^2}{n}\sum_{j = 0}^{n-1} \filt(\regparam,|\dft{k}_j|) - \noiseSD^2.
\label{eq:SpectralUPRE}
\end{equation} 
Since the UPRE method relies on finding a minimum of \eqref{eq:SpectralUPRE}, the last term $-\noiseSD^2$ can be ignored during implementation. \par
The spectral form of the UPRE function can also be derived by starting with the DFT of the predictive error:
\[\dft{\pVec(\regparam)} = F\kMat(\regf - \fVec) = \Delta(\dft{\regf} - \dft{\fVec}).\]
Since the influence matrix $\A$ is defined as $\A = \kMat\R = \ctrans{F}\Delta{F}\R$ with regularization matrix $\R$, conjugation of $\A$ by $F$ gives $F\A{\ctrans{F}} = \Delta{F}\R{\ctrans{F}}$. Combined with $\dft{\regf} = F\R\gnoiseVec = F\R{\ctrans{F}}\dft{\gnoiseVec}$, the predictive error can be rewritten as
\begin{align*}
\dft{\pVec(\regparam)} &= \Delta\dft{\regf} - \Delta\dft{\fVec} \\
&= F\A{\ctrans{F}}\dft{\gnoiseVec} - \Delta\dft{\fVec} \\
&= F\A{\ctrans{F}}(\Delta\dft{\fVec} + \dft{\noiseVec}) - \Delta\dft{\fVec} \\
&= [F(\A - I)\ctrans{F}]\Delta\dft{\fVec} + [F\A{\ctrans{F}}]\dft{\noiseVec}.
\end{align*}
Since the components of $\dft{\fVec}$ and $\dft{\noiseVec}$ are not guaranteed to be real, the Trace Lemma as previously stated can no longer be directly applied. Instead, the Trace Lemma can be modified to accommodate the existence of complex components.

\begin{lemma}
Let $f \in \mathcal{H}$, where $\mathcal{H}$ is a deterministic complex Hilbert space, let $\noiseVec$ be a discrete white noise vector with $\noiseVec \sim \mathcal{N}(\mathbf{0},\noiseSD^2{I})$, and let $B: \mathbb{C}^n \rightarrow  \mathcal{H}$ be a bounded linear operator. Furthermore let $\dft{\noiseVec} = F\noiseVec$ where $F$ is the $n \times n$ unitary DFT matrix. Then
\[\E(\|f + B\dft{\noiseVec}\|_{\mathcal{H}}^2) = \|f\|_{\mathcal{H}}^2 + \noiseSD^2\trace(\ctrans{B}{B})\]
where $\ctrans{B}$ denotes the adjoint of $B$.
\end{lemma}
\begin{proof}
By the linearity of inner products and the expected value operator,
\begin{align*}
\E(\|f + B\dft{\noiseVec}\|_{\mathcal{H}}^2) &= \E(\langle f + B\dft{\noiseVec}, f + B\dft{\noiseVec}\rangle_{\mathcal{H}}) \\
&= \E(\|f\|_{\mathcal{H}}^2) + \E\left(\langle f, B\dft{\noiseVec}\rangle_{\mathcal{H}} + \overline{\langle f, B\dft{\noiseVec}\rangle_{\mathcal{H}}}\right) + \E(\langle B\dft{\noiseVec}, B\dft{\noiseVec}\rangle_{\mathcal{H}}) \\
&= \E(\|f\|_{\mathcal{H}}^2) + 2\E\left(\Re(\langle f, B\dft{\noiseVec}\rangle_{\mathcal{H}})\right) + \E(\langle B\dft{\noiseVec}, B\dft{\noiseVec}\rangle_{\mathcal{H}}).
\end{align*}
This difference between the real and complex versions of the Trace Lemma comes from the fact that the inner product on a complex Hilbert space is a sesquilinear form instead of a bilinear form. Again the term $\E(\|f\|_{\mathcal{H}}^2)$ reduces to $\|f\|_{\mathcal{H}}^2$ because $f$ is an element of a deterministic Hilbert space. The inner products can be rewritten using $\dft{\noiseVec} = F\noiseVec$ and the adjoint of $B$:
\begin{align*}
\E(\|f + B\dft{\noiseVec}\|_{\mathcal{H}}^2) &= \E(\|f\|_{\mathcal{H}}^2) + 2\E\left(\Re(\langle f, B\dft{\noiseVec}\rangle_{\mathcal{H}})\right) + \E(\langle B\dft{\noiseVec}, B\dft{\noiseVec}\rangle_{\mathcal{H}}) \\
&= \E(\|f\|_{\mathcal{H}}^2) + 2\E\left(\Re(\ctrans{(\dft{\noiseVec})}{\ctrans{B}}f)\right) + \E(\trans{\noiseVec}{\ctrans{F}}{\ctrans{B}}BF\noiseVec) \\
&= \E(\|f\|_{\mathcal{H}}^2) + 2\sum_{j=0}^{n-1} \E(\trans{\noiseVec}_j)(\ctrans{F}{\ctrans{B}}f)_j + \sum_{j=0}^{n-1}\sum_{k=0}^{n-1} ({\ctrans{F}}{\ctrans{B}}BF)_{j,k}\E(\noise_j\noise_k).
\end{align*}
Since $\noiseVec \sim \mathcal{N}(\mathbf{0},\noiseSD^2{I})$, the expected values of $\noise_j$ and ${\noise_j}{\noise_k}$ are zero and $\noiseSD^2\delta_{j,k}$, respectively. Therefore the second term above is zero and the third term is a summation expression for $\noiseSD^2\trace(\ctrans{F}{B^*}BF)$. Lastly since $F$ is unitary and the trace operation is invariant under similarity transformations, $\noiseSD^2\trace(\ctrans{F}{B^*}BF) = \noiseSD^2\trace(\ctrans{B}{B})$.
\end{proof}

Applying the DFT version of the Trace Lemma to the DFT of the predictive error yields
\begin{align*}
\E\left(\frac{1}{n}\left\|\dft{\pVec(\regparam)}\right\|^2\right) &= \frac{1}{n}\E\left(\left\|[F(\A - I)\ctrans{F}]\Delta\dft{\fVec} + [F\A{\ctrans{F}}]\dft{\noiseVec}\right\|^2\right) \\
&= \frac{1}{n}\left\|[F(\A - I)\ctrans{F}]\Delta\dft{\fVec}\right\|^2 + \frac{\noiseSD^2}{n}\trace\left(\ctrans{(F\A{\ctrans{F}})}{F\A{\ctrans{F}}}\right) \\
&= \frac{1}{n}\left\|[F(\A - I)\ctrans{F}]\Delta\dft{\fVec}\right\|^2 + \frac{\noiseSD^2}{n}\trace\left(\ctrans{\A}\A\right).
\end{align*}
The DFT of the regularized residual $\regres$ is
\[\dft{\regres} = F(\kMat\regf - \gnoiseVec) = F\ctrans{F}\Delta{F}\regf - F\gnoiseVec = \Delta\dft{\regf} - \dft{\gnoiseVec}.\]
Using $F{\A}\ctrans{F}$, the expression for $\dft{\regres}$ can be rewritten as
\[\dft{\regres} = F(\A - I){\ctrans{F}}\dft{\gnoiseVec} = F(\A - I){\ctrans{F}}(\Delta\dft{\fVec} + \dft{\noiseVec}) = [F(\A - I){\ctrans{F}}]\Delta\dft{\fVec} + [F(\A - I){\ctrans{F}}]\dft{\noiseVec}.\]
Applying the DFT version of the Trace Lemma produces
\begin{align*}
\E\left(\frac{1}{n}\left\|\dft{\regres}\right\|^2\right) &= \frac{1}{n}\left\|[F(\A - I)\ctrans{F}]\Delta\dft{\fVec}\right\|^2 + \frac{\noiseSD^2}{n}\trace\left(F\ctrans{(\A - I)}(\A - I){\ctrans{F}}\right) \\
&= \frac{1}{n}\left\|[F(\A - I)\ctrans{F}]\Delta\dft{\fVec}\right\|^2 + \frac{\noiseSD^2}{n}\trace\left(F(\ctrans{\A}\A - (\ctrans{\A} + \A) + I){\ctrans{F}}\right) \\
&= \frac{1}{n}\left\|[F(\A - I)\ctrans{F}]\Delta\dft{\fVec}\right\|^2 + \frac{\noiseSD^2}{n}\trace\left(\ctrans{\A}\A\right) - \frac{2\noiseSD^2}{n}\trace\left(\Re(\A)\right) + \noiseSD^2.
\end{align*}
Thus the $\E(\|\dft{\pVec(\regparam)}\|^2/n)$ can be expressed as
\[\E\left(\frac{1}{n}\left\|\dft{\pVec(\regparam)}\right\|^2\right) = \E\left(\frac{1}{n}\left\|\dft{\regres}\right\|^2\right) + \frac{2\noiseSD^2}{n}\trace\left(\Re(\A)\right) - \noiseSD^2.\]
The last step needed to obtain \eqref{eq:SpectralUPRE} to use the fact that the Tikhonov regularization matrix $\A$ is real and the assumption that $\A$ can be written as $\A = \kMat(\ctrans{\kMat}\kMat + \regparam{\ctrans{D}}D)^{-1}\ctrans{\kMat}$. \par
The same derivation can be applied for the DCT-version of the UPRE function under the assumption that $\kMat$ can be diagonalized by the DCT. Since the DCT maps real vectors to real vectors, the standard Trace Lemma can be utilized. The DCT-version of the UPRE function is
\begin{equation}
\U(\regparam) = \frac{1}{n}\sum_{j = 0}^{n-1} |\dct{\gnoiseVec}_j|^2(\mfilt(\regparam,|\dct{k}_j|))^2 + \frac{2\noiseSD^2}{n}\sum_{j = 0}^{n-1} \filt(\regparam,|\dct{k}_j|) - \noiseSD^2.
\label{eq:UPRE DCT}
\end{equation}
In order to apply Corollary \ref{cor:gnoise Mean and Var} to \eqref{eq:SpectralUPRE}, the UPRE function must be modified. Let the factors $M_j$ for $j = 0,\ldots,n-1$ be defined by
\[M_j = \begin{cases}
1/\noiseSD & j = 0, n/2 \\
\sqrt{2}/\noiseSD & \text{otherwise}
\end{cases},\]
where $n/2$ is ignored if $n$ is odd. Then \eqref{eq:SpectralUPRE} can be rewritten as
\begin{equation}
\label{eq:SpectralUPREnModified}
\U_n(\regparam) = \sum_{j = 0}^{n-1} |M_j\dft{\gnoiseVec}_j|^2\left(\frac{\mfilt(\regparam,|\dft{k}_j|)}{M_j}\right)^2 + \frac{2\noiseSD^2}{n}\sum_{j = 0}^{n-1} \filt(\regparam,|\dft{k}_j|) - \noiseSD^2.
\end{equation}
By the properties of expected value,
\begin{align*}
\E(\U_n(\regparam)) &= \E\left(\sum_{j = 0}^{n-1} |M_j\dft{\gnoiseVec}_j|^2\left(\frac{\mfilt(\regparam,|\dft{k}_j|)}{M_j}\right)^2 + \frac{2\noiseSD^2}{n}\sum_{j = 0}^{n-1} \filt(\regparam,|\dft{k}_j|) - \noiseSD^2\right) \\
&= \sum_{j = 0}^{n-1} \E\left(|M_j\dft{\gnoiseVec}_j|^2\right)\left(\frac{\mfilt(\regparam,|\dft{k}_j|)}{M_j}\right)^2 + \frac{2\noiseSD^2}{n}\sum_{j = 0}^{n-1} \filt(\regparam,|\dft{k}_j|) - \noiseSD^2.
\end{align*}
Corollary \ref{cor:gnoise Mean and Var} then gives
\begin{equation}
\begin{split}
\E(\U_n(\regparam)) &= \sum_{j = 0}^{n-1} \frac{\noiseSD^2}{2}\left(2 + \frac{|\dft{g}_j|^2}{\noiseSD^2}\right)(\mfilt(\regparam,|\dft{k}_j|))^2 + \frac{2\noiseSD^2}{n}\sum_{j = 0}^{n-1} \filt(\regparam,|\dft{k}_j|) - \noiseSD^2 \\
&= \frac{1}{2}\sum_{j = 0}^{n-1} |\dft{\gnoiseVec}_j|^2(\mfilt(\regparam,|\dft{k}_j|))^2 + \noiseSD^2\sum_{j = 0}^{n-1} (\mfilt(\regparam,|\dft{k}_j|))^2 + \frac{2\noiseSD^2}{n}\sum_{j = 0}^{n-1} \filt(\regparam,|\dft{k}_j|) - \noiseSD^2.
\end{split}
\label{eq:UPRE Expected Value}
\end{equation}
The variance of $\U_n$ can be determined as well. The second and third terms in both \eqref{eq:SpectralUPRE} and \eqref{eq:SpectralUPREnModified} are deterministic, and so
\begin{equation}
\label{eq:UPRE Var Sum}
\begin{split}
\Var\left(\U_n(\regparam)\right) &= \Var\left(\sum_{j = 0}^{n-1} |M_j\dft{\gnoiseVec}_j|^2\left(\frac{\mfilt(\regparam,|\dft{k}_j|)}{M_j}\right)^2\right) \\
&= \sum_{j=0}^{n-1} \sum_{\ell=0}^{n-1} \left(\frac{\mfilt(\regparam,|\dft{k}_j|)\mfilt(\regparam,|\dft{k}_\ell|)}{M_jM_\ell}\right)^2 \Cov\left(|M_j\dft{\gnoiseVec}_j|^2,|M_\ell\dft{\gnoiseVec}_\ell|^2\right).
\end{split}
\end{equation}
By Theorem \ref{thm:Cov of mag squared}, the covariance terms are only nonzero when $j = \ell$ or $j + \ell = n$. In either case,
\[\left(\frac{\mfilt(\regparam,|\dft{k}_j|)\mfilt(\regparam,|\dft{k}_\ell|)}{M_jM_\ell}\right)^2 \Cov\left(|M_j\dft{\gnoiseVec}_j|^2,|M_\ell\dft{\gnoiseVec}_\ell|^2\right) = \left(\frac{\mfilt(\regparam,|\dft{k}_j|)}{M_j}\right)^4 \Var\left(|M_j\dft{\gnoiseVec}_j|^2\right),\]
Let $J = \{0,n/2\}$ if $n$ is even and $J = \{0\}$ if $n$ is odd. Corollary \ref{cor:gnoise Mean and Var} states that
\[\left(\frac{\mfilt(\regparam,|\dft{k}_j|)}{M_j}\right)^4 \Var\left(|M_j\dft{\gnoiseVec}_j|^2\right) = 2\noiseSD^4\left(1 + \dfrac{|\dft{g}_j|^2}{\noiseSD^2}\right)\left(\mfilt(\regparam,|\dft{k}_j|)\right)^4, \qquad j \in J\]
and similarly
\[\left(\frac{\mfilt(\regparam,|\dft{k}_j|)}{M_j}\right)^4 \Var\left(|M_j\dft{\gnoiseVec}_j|^2\right) = \noiseSD^4\left(1 + \dfrac{|\dft{g}_j|^2}{\noiseSD^2}\right)\left(\mfilt(\regparam,|\dft{k}_j|)\right)^4, \qquad j \not\in J\]
If $j \in J$, $\ell = j$ is the only $0 \leq \ell \leq n-1$ such that $j + \ell = n$. On the other hand, if $j \not\in J$ then there are two values of $\ell$ such that $j + \ell = n$: $\ell = j$ and $\ell = n - j$. Thus \eqref{eq:UPRE Var Sum} is
\begin{equation}
\label{eq:UPRE Var Sum Simple}
\Var\left(\U_n(\regparam)\right) = 2\noiseSD^4\sum_{j=0}^{n-1} \left(1 + \dfrac{|\dft{g}_j|^2}{\noiseSD^2}\right)\left(\mfilt(\regparam,|\dft{k}_j|)\right)^4.
\end{equation}

%The problem of finding a minimizer of \eqref{eq:SpectralUPRE} can be recast as a root-finding problem by using the derivative of the UPRE function, which is
%\begin{equation}
%\U'_n(\regparam) = 2\regparam^2\left[\sum_{j = 0}^{n-1} \frac{|\dft{\gnoiseVec}_j|^2|\dft{k}_j|^2}{(|\dft{k}_j|^2 + \regparam^2)^3} - \frac{2\noiseSD^2}{n}\sum_{j = 0}^{n-1} \frac{|\dft{k}_j|^2}{(|\dft{k}_j|^2 + \regparam^2)^2}\right].
%\label{eq:SpectralUPREnDeriv}
%\end{equation}
%It is clear that $\regparam = 0$ is a root of \eqref{eq:SpectralUPREnDeriv} regardless of the data and operator spectra, so this root should be ignored (since a solution generated for $\regparam = 0$ is in fact a non-regularized solution). Therefore any meaningful regularization parameter selected from the UPRE method will be a root of
%\begin{equation}
%V_n(\regparam) := \sum_{j = 0}^{n-1} \frac{|\dft{\gnoiseVec}_j|^2|\dft{k}_j|^2}{(|\dft{k}_j|^2 + \regparam^2)^3} - \frac{2\noiseSD^2}{n}\sum_{j = 0}^{n-1} \frac{|\dft{k}_j|^2}{(|\dft{k}_j|^2 + \regparam^2)^2}.
%\label{eq:SpectralUPREnDeriv2}
%\end{equation}
Now consider the case where multiple data sets are available, which can arise from repeated observations of some time-invariant event. As an alternative to finding a regularization parameter for each data set, a single regularization parameter can be obtained by constructing an averaged version of \eqref{eq:SpectralUPRE}, assuming that $n$ is constant across all data sets to be considered. It is reasonable to expect that this single regularization parameter will perform worse that each individual parameter with respect to their corresponding data sets. However, computational time/resources could be saved because the method would involved solving a single minimization problem instead of solving a minimization problem for each data set. To bring this idea to fruition, some notation will be expanded. Let $R$ be the number of available data sets, and denote the $j$th data set by $\gnoiseVec^j$. Similarly, let $\U_n^j(\regparam)$ be the UPRE function associated with the $j$th data set.  The average of the UPRE functions is then 
\begin{align*}
\frac{1}{R}\sum_{j=1}^R \U_n^j(\regparam) &= \frac{1}{R}\sum_{j=1}^R \left(\sum_{\ell = 0}^{n-1} |\dft{\gnoiseVec^j}_\ell|^2(\mfilt(\regparam,|\dft{k}_\ell|))^2 + \frac{2\noiseSD^2}{n}\sum_{\ell = 0}^{n-1} \filt(\regparam,|\dft{k}_\ell|) - \noiseSD^2\right) \\
&= \frac{1}{R}\sum_{j=1}^R \left(\sum_{\ell = 0}^{n-1} |\dft{\gnoiseVec^j}_\ell|^2(\mfilt(\regparam,|\dft{k}_\ell|))^2\right) + \frac{1}{R}\sum_{j=1}^R \left(\frac{2\noiseSD^2}{n}\sum_{\ell = 0}^{n-1} \filt(\regparam,|\dft{k}_\ell|)\right) - \noiseSD^2.
\end{align*}
By factoring terms from the first sum and noting that the summand of the second sum does not depend upon $j$, the function simplifies to
\begin{equation}
\frac{1}{R}\sum_{j=1}^R \U_n^j(\regparam) =  \sum_{\ell = 0}^{n-1} \left(\frac{1}{R}\sum_{j=1}^R |\dft{\gnoiseVec^j}_\ell|^2\right)(\mfilt(\regparam,|\dft{k}_\ell|))^2 + \frac{2\noiseSD^2}{n}\sum_{\ell = 0}^{n-1} \filt(\regparam,|\dft{k}_\ell|) - \noiseSD^2.
\label{eq:SpectralUPREavg}
\end{equation}
Numerically, \eqref{eq:SpectralUPREavg} can be readily obtained by summing the DFT's of the data sets; note that $\dft{\kVec}$ is unchanged across data sets.

\section{Generalized Cross Validation} \label{sec:Generalized Cross Validation}
The UPRE method requires knowledge of the variance $\noiseSD^2$ of the noise vector $\noiseVec$. In contrast, the generalized cross validation (GCV) method \cite{Wahba1977,Wahba1990} does not require knowledge of $\noiseSD^2$. The GCV function is
\begin{equation}
\label{eq:GCV}
\GCV(\regparam) = \frac{\frac{1}{n}\|\regres\|^2}{\left[\frac{1}{n}\trace(I-\A)\right]^2},
\end{equation}
where $\regres$ is the regularized residual defined in the derivation of the UPRE method. Similarities between the GCV and UPRE methods are that both functions are estimators of the predictive risk, and the regularization parameter $\regparam$ is chosen as the minimizers of these functions. \par 
By the linearity of the trace operator, $\trace(I-\A) = \trace(I)-\trace(\A) = n - \trace(\A)$. Then by \eqref{eq:TikFiltPsi} and \eqref{eq:TraceUPRE} and assuming that $\kMat$ can be diagonalized by the DFT,
\begin{equation}
\trace(I-\A) = n - \sum_{j = 0}^{n-1} \filt(\regparam,|\dft{k}_j|) = \sum_{j = 0}^{n-1} 1 - \filt(\regparam,|\dft{k}_j|) = \sum_{j = 0}^{n-1} \mfilt(\regparam,|\dft{k}_j|).
\label{eq:TraceGCV}
\end{equation}
Substituting \eqref{eq:RegResNorm} and \eqref{eq:TraceGCV} into \eqref{eq:GCV} produces the spectral form of the GCV function:
\begin{equation}
\GCV(\regparam) = \frac{\sum_{j = 0}^{n-1} |\dft{\gnoiseVec}_j|^2(\mfilt(\regparam,|\dft{k}_j|))^2}{(\frac{1}{n}\sum_{j = 0}^{n-1} \mfilt(\regparam,|\dft{k}_j|))^2} = \frac{n^2\sum_{j = 0}^{n-1} |\dft{\gnoiseVec}_j|^2(\mfilt(\regparam,|\dft{k}_j|))^2}{(\sum_{j = 0}^{n-1} \mfilt(\regparam,|\dft{k}_j|))^2}.
\label{eq:SpectralGCV}
\end{equation}
If $\kMat$ can be diagonalized by the DCT, the DCT-version of the GCV function is
\begin{equation}
\GCV(\regparam) = \frac{\sum_{j = 0}^{n-1} |\dct{\gnoiseVec}_j|^2(\mfilt(\regparam,|\dct{k}_j|))^2}{(\frac{1}{n}\sum_{j = 0}^{n-1} \mfilt(\regparam,|\dct{k}_j|))^2} = \frac{n^2\sum_{j = 0}^{n-1} |\dct{\gnoiseVec}_j|^2(\mfilt(\regparam,|\dct{k}_j|))^2}{(\sum_{j = 0}^{n-1} \mfilt(\regparam,|\dct{k}_j|))^2}.
\label{eq:GCV DCT}
\end{equation}

The case where multiple data sets are available will now be considered using notation analogous to that introduced in Section \ref{sec:Unbiased Predictive Risk Estimator}; let $\GCV_n^j(\regparam)$ be the GCV function associated with the $j$th data set $\gnoiseVec^j$ for $j = 1,\ldots,R$. Since the denominator in \eqref{eq:SpectralGCV} is independent of the data,
\begin{equation}
\frac{1}{R}\sum_{j=1}^R \GCV_n^j(\regparam)  = \frac{n^2\sum_{j = 0}^{n-1} \left(\sum_{j=1}^R |\dft{\gnoiseVec^j}_j|^2\right)(\mfilt(\regparam,|\dft{k}_j|))^2}{R(\sum_{j = 0}^{n-1} \mfilt(\regparam,|\dft{k}_j|))^2}.
\label{eq:SpectralGCVsum}
\end{equation}

\section{Discrepancy Principle} \label{sec:Discrepancy Principle}
As a start to a stochastic derivation of the discrepancy principle method \cite{Morozov1966} (for a deterministic derivation, see \cite[p.~8-9]{Vogel:2002}), consider the case where $\regf \approx \fVec$. In this case,
\[\regres = \kMat\regf - \gnoiseVec \approx \kMat\fVec - \gnoiseVec = \noiseVec.\]
with a direct consequence being that $\E((1/n)\|\regres\|^2) \approx \E((1/n)\|\noiseVec\|^2) =\noiseSD^2$. Thus the discrepancy principle is to choose $\regparam$ such that $(1/n)\|\regres\|^2 = \noiseSD^2$. A similarity exists between the discrepancy principle and the UPRE method in that the variance of the noise in the data must be known for both methods. \par 
Implementation of this method requires finding a solution of $\D_n(\regparam) = 0$, where $\D_n(\regparam)$ is defined to be
\begin{equation}
\label{eq:DP}
\D_n(\regparam) = \frac{1}{n}\|\regres\|^2 - \noiseSD^2.
\end{equation}
In other words, implementation of the discrepancy principle method is equivalent to finding a root of $\D_n(\regparam)$. The spectral form of the discrepancy principle function is obtained directly from \eqref{eq:RegResNorm} by substituting the regularized residual term:
\begin{equation}
\D_n(\regparam) = \sum_{j = 0}^{n-1} |\dft{\gnoiseVec}_j|^2(\mfilt(\regparam,|\dft{k}_j|))^2 - \noiseSD^2.
\label{eq:SpectralDP}
\end{equation}
Comparing the UPRE function $\U_n$ on \eqref{eq:SpectralUPRE} with $\D_n(\regparam)$, it can be seen that
\begin{equation}
\label{eq:UPRE DP Comp}
\D_n(\regparam) = \U_n(\regparam) - \frac{2\noiseSD^2}{n}\sum_{j = 0}^{n-1} \filt(\regparam,|\dft{k}_j|).
\end{equation}
If $\kMat$ can be diagonalized by the DCT, then the DCT-version of the discrepancy principle function is
\begin{equation}
\D_n(\regparam) = \sum_{j = 0}^{n-1} |\dct{\gnoiseVec}_j|^2(\mfilt(\regparam,|\dct{k}_j|))^2 - \noiseSD^2.
\label{eq:DP DCT}
\end{equation}

The function $\D_n(\regparam)$ will be near zero when the sum in \eqref{eq:SpectralDP} is close to $\noiseSD^2$.  Furthermore, $\D_n(\regparam)$ is monotone increasing on $(0,\infty)$ because for all $\regparam > 0$,
\[\frac{d}{d\regparam}\left\{\D_n(\regparam)\right\} = 4\sum_{j = 0}^{n-1} |\dft{\gnoiseVec}_j|^2\frac{|\dft{k}_j|^2\regparam^3}{\left(|\dft{k}_j|^2 + \regparam^2\right)^3} \geq 0.\]
If the range of $\regparam$ being considered for roots of \eqref{eq:SpectralDP} is not chosen carefully, it is possible that no regularization parameter will be obtained. If \eqref{eq:SpectralDP} does not have a root in $[0,\infty)$ (recall that by definition, $\regparam \geq 0$), there is only one possibility because $\D_n(0) = -\noiseSD^2 < 0$. Since the function is monotone increasing, the function must have a root or approach some negative horizontal asymptote. By looking at \eqref{eq:TikFiltPsi}, it is clear that $(\mfilt(\regparam,|\dft{k}_i|))^2 \leq 1$ for all $\regparam \geq 0$. Thus,
\[\sum_{j = 0}^{n-1} |\dft{\gnoiseVec}_j|^2(\mfilt(\regparam,|\dft{k}_j|))^2 - \noiseSD^2 \leq \sum_{j = 0}^{n-1} |\dft{\gnoiseVec}_j|^2 - \noiseSD^2,\]
and so \eqref{eq:SpectralDP} will approach a negative horizontal asymptote if $\sum_{j = 0}^{n-1} |\dft{\gnoiseVec}_j|^2 < \noiseSD^2$. \par
Analogous to \eqref{eq:SpectralUPREnModified}, the spectral form of the $\D_n(\regparam)$ can be rewritten as
\begin{equation}
\label{eq:SpectralDPModified}
\D_n(\regparam) = \sum_{j = 0}^{n-1} |M_j\dft{\gnoiseVec}_j|^2\left(\frac{\mfilt(\regparam,|\dft{k}_j|}{M_j}\right)^2 - \noiseSD^2.
\end{equation}
As with the UPRE method, the rescaling of $|\dft{\gnoiseVec}_j|^2$ is justified by the fact that the discrepancy principle is based on prior knowledge of $\noiseSD^2$. The benefit of \eqref{eq:SpectralDPModified} is that the statistics of $\D_n(\regparam)$ can be analyzed using Corollary \ref{cor:gnoise Mean and Var}. By the properties of expected value,
\[\E(\D_n(\regparam)) = \E\left(\sum_{j = 0}^{n-1} |M_j\dft{\gnoiseVec}_j|^2\left(\frac{\mfilt(\regparam,|\dft{k}_j|}{M_j}\right)^2 - \noiseSD^2\right) = \sum_{j = 0}^{n-1} \E\left(|M_j\dft{\gnoiseVec}_j|^2\right)\left(\frac{\mfilt(\regparam,|\dft{k}_j|)}{M_j}\right)^2 - \noiseSD^2.\]
Corollary \ref{cor:gnoise Mean and Var} then gives
\begin{align*}
\E(\D_n(\regparam)) &= \sum_{j = 0}^{n-1} \frac{\noiseSD^2}{2}\left(2 + \frac{|\dft{g}_j|^2}{\noiseSD^2}\right)(\mfilt(\regparam,|\dft{k}_j|))^2 - \noiseSD^2 \\
&= \frac{1}{2}\sum_{j = 0}^{n-1} |\dft{\gnoiseVec}_j|^2(\mfilt(\regparam,|\dft{k}_j|))^2 + \noiseSD^2\sum_{j = 0}^{n-1} (\mfilt(\regparam,|\dft{k}_j|))^2 - \noiseSD^2.
\end{align*}
The derivative of $\E(\D_n(\regparam))$ with respect to $\regparam$ is
\begin{align*}
\frac{d}{d\regparam}\left\{\E\left(\D_n(\regparam)\right)\right\} &= \frac{1}{2}\sum_{j = 0}^{n-1} |\dft{\gnoiseVec}_j|^2 \frac{d}{d\regparam}\left\{(\mfilt(\regparam,|\dft{k}_j|))^2\right\} + \noiseSD^2\sum_{j = 0}^{n-1} \frac{d}{d\regparam}\left\{(\mfilt(\regparam,|\dft{k}_j|))^2\right\} \\
&= 2\sum_{j = 0}^{n-1} |\dft{\gnoiseVec}_j|^2\frac{|\dft{k}_j|^2\regparam^3}{\left(|\dft{k}_j|^2 + \regparam^2\right)^3} + 4\noiseSD^2 \sum_{j = 0}^{n-1} \frac{|\dft{k}_j|^2\regparam^3}{\left(|\dft{k}_j|^2 + \regparam^2\right)^3},
\end{align*}
which is nonnegative for all $\regparam > 0$. Thus $\E(\D_n(\regparam))$ is monotone increasing on $(0,\infty)$. \par
Since the last term $-\noiseSD^2$ in \eqref{eq:SpectralDPModified} is deterministic, the variance of $\D_n(\regparam)$ is reduced to
\[\Var(\D_n(\regparam)) = \Var\left(\sum_{j = 0}^{n-1} |M_j\dft{\gnoiseVec}_j|^2\left(\frac{\mfilt(\regparam,|\dft{k}_j|)}{M_j}\right)^2\right),\]
and so the variance is the same as the UPRE function given by \eqref{eq:UPRE Var Sum Simple}. This is interesting since the functions  $\D_n(\regparam)$ and $\U_n(\regparam)$ have different properties; for example, $\D_n(\regparam)$ is monotone increasing while $\U_n(\regparam)$ is not. \par 
Again adopting the notation introduced in Section \ref{sec:Unbiased Predictive Risk Estimator}, let $\D_n^j(\regparam)$ be the MDP function associated with the $j$th data set $\gnoiseVec^j$ for $j = 1,\ldots,R$. Then 
\begin{equation}
\frac{1}{R}\sum_{j=1}^R \D_n^j(\regparam)  = \sum_{\ell = 0}^{n-1} \left(\frac{1}{R} \sum_{j=1}^R |\dft{\gnoiseVec^j}_\ell|^2\right)(\mfilt(\regparam,|\dft{k}_\ell|))^2 - \noiseSD^2. 
\label{eq:SpectralDPavg}
\end{equation}
As previously stated, the discrepancy function \eqref{eq:SpectralDP} is monotone increasing, and so the scaled sum \eqref{eq:SpectralDPavg} is monotone increasing as well. If there are data realizations that would otherwise result in discrepancy principle functions that prove difficult for finding a root, averaging these functions with better-behaved functions could provide a single function with a meaningful root (here ``better-behaved" means that the function has a root located within an interval that is not excessively large). The hope is that the poorly-behaved functions are outliers so that the average of the functions can be expected to have a root. If for some reason the better-behaved functions are themselves the outliers, then this averaging approach would not be expected to yield meaningful results.

%Unlike the UPRE and GCV functions, the monotonicity of the MDP function can used to at least provide an interval for a root of \eqref{eq:SpectralDPavg}. 
%\begin{lemma}
%Let $f_1,\ldots,f_R$ be a family of continuous, monotone increasing functions on an interval $I = [a,b]$ having roots $x_1,\ldots x_R \in I$, respectively. Then the function $\overline{f} = (1/R)\sum_{j=1}^R f_i$ has a root in $[x_m,x_M]$, where $x_m = \min\{x_1,\ldots x_R\}$ and $x_M = \max\{x_1,\ldots x_R\}$.
%\end{lemma}
%\begin{proof}
%Let the family of functions $f_1,\ldots,f_R$ be as required. Relabel the functions so that their respective roots are such that $x_1 \leq \ldots \leq x_R$. Then $x_m = \min\{x_1,\ldots,x_R\} = x_1$ and $x_M = \max\{x_1,\ldots,x_R\} = x_R$. Thus it must be shown that $\overline{f} = (1/R)\sum_{j=1}^R f_i$ has a root in $[x_1,x_R]$. Since each $f_i$ is monotone increasing on $[a,b] \supseteq [x_1,x_R]$ and $x_1 \leq \ldots \leq x_R$, $f_\ell(x_1) \leq 0$ for $i = 1,\ldots,R$. Similarly, $f_\ell(x_R) \geq 0$ for $\ell = 1,\ldots,R$. Thus $\overline{f}(x_1) \leq 0$ and $\overline{f}(x_R) \geq 0$, with equality only when $x_1 = \ldots = x_R$. However if this were the case, $\overline{f}(x_1) = \overline{f}(x_R) = 0$ and so certainly $\overline{f}$ has a root in $[x_1,x_R] = \{x_1\} = \ldots = \{x_R\}$. \par 
%If roots are not equal, then $\overline{f}(x_1) < 0$ and $\overline{f}(x_R) > 0$ since there would exist some $\ell = 1,\ldots,k$ such that $f_\ell(x_1) < 0$ or $f_\ell(x_R) > 0$, again following from the monotoncity of each function in the family. Since $\overline{f}$ is a linear combination of continuous functions, $\overline{f}$ itself is continuous. Thus having $\overline{f}(x_1) < 0$ and $\overline{f}(x_R) > 0$ implies that $\overline{f}$ has a root in $[x_1,x_R]$ by Bolzano's theorem (a corollary to the intermediate value theorem).
%\end{proof}

%\subsection{Numerical implementation} \label{sec:Implementation}
%In an effort to streamline a discussion on the implementation of the parameter selection methods, we will use the version of the GSVD outlined in Section \ref{sec:Introduction} (which uses the assumption that the system matrix in \eqref{eq:TikSol3} has full column rank). Specifically, we assume that
%\begin{equation}
%\label{eq:Fourier diagonalization}
%A^{(\ell)} = U{\Delta^{(\ell)}}\trans{X}, \quad L^{(\ell)} = V{\Lambda^{(\ell)}}\trans{X}, \qquad \ell = 1,\ldots,R
%\end{equation}
%While this is an extremely strong and unrealistic assumption (the belief that such a factorization does not exist in general is expressed in \cite{Brezinski2003}), it certainly becomes realistic if $A^{(\ell)} = A$ and $L^{(\ell)} = L$ for all $\ell = 1,\ldots,R$. We use \eqref{eq:Fourier diagonalization} to simply present the following derivation in the most general setting. An analogous approach could be taken if the matrices $A^{(\ell)}$ and $L^{(\ell)}$ can be simultaneously diagonalized with respect to a orthogonal/unitary transformation, such as the discrete cosine transform (DCT) or discrete Fourier transform (DFT); test problems using the DCT and DFT are considered in Section \ref{sec:Validation}. \par
%As motivation for the following derivation, observe that the UPRE, MDP, and GCV methods all involve terms $\frac{1}{n}\|\rVec^{(\ell)}(\regparam)\|_2^2$. In addition, the UPRE and GCV methods involve $\trace(\A^{(\ell)})$. Thus for the implementation of these methods, we consider different representations of $\frac{1}{n}\|\rVec^{(\ell)}(\regparam)\|_2^2$ and $\trace(\A^{(\ell)})$. Using \eqref{eq:Fourier diagonalization}, the matrix $\A^{(\ell)}$ given by \eqref{eq:Influence matrix} can be expressed as
%\begin{equation}
%\label{eq:Fourier diagonalization 2}
%\A^{(\ell)} = U\Delta^{(\ell)}\left[\trans{(\Delta^{(\ell)})}\Delta^{(\ell)} + \regparam^2\trans{(\Lambda^{(\ell)})}\Lambda^{(\ell)}\right]^{-1}\trans{(\Delta^{(\ell)})}\trans{U}, \qquad \ell = 1,\ldots,R.
%\end{equation}
%If $\Sigma^{(\ell)} = \noiseSD_\ell^2I$ for all $\ell = 1,\ldots,R,$ then the similarity invariance of the trace operator gives
%\begin{align}
%\trace\left(\Sigma^{(\ell)}\A^{\ell}\right) &= \noiseSD_\ell^2 \trace\left(\Delta^{(\ell)}\left[\trans{(\Delta^{(\ell)})}\Delta^{(\ell)} + \regparam^2\trans{(\Lambda^{(\ell)})}\Lambda^{(\ell)}\right]^{-1}\trans{(\Delta^{(\ell)})}\right) \nonumber \\
%&= \noiseSD_\ell^2 \sum_{j=1}^{n} \frac{\bm{\delta}_j^{(\ell)}}{\bm{\delta}_j^{(\ell)} + \regparam^2 \bm{\lambda}_j^{(\ell)}}
%\label{eq:Trace}
%\end{align}
%where $\bm{\delta}^{(\ell)} = \diag(\trans{(\Delta^{(\ell)})}\Delta^{(\ell)})$ and $\bm{\lambda}^{(\ell)} = \diag(\trans{(\Lambda^{(\ell)})}\Lambda^{(\ell)})$. Another benefit of using \eqref{eq:Fourier diagonalization 2} is that we can write
%\begin{align}
%\frac{1}{n}\|\rVec^{(\ell)}(\regparam)\|_2^2 &= \frac{1}{n}\left\|U\left(\Delta^{(\ell)}\left[\trans{(\Delta^{(\ell)})}\Delta^{(\ell)} + \regparam^2\trans{(\Lambda^{(\ell)})}\Lambda^{(\ell)}\right]^{-1}\trans{(\Delta^{(\ell)})} - I\right)\trans{U}\dVec^{(\ell)}\right\|_2^2 \nonumber \\
%&= \frac{1}{n}\left\|\left(\Delta^{(\ell)}\left[\trans{(\Delta^{(\ell)})}\Delta^{(\ell)} + \regparam^2\trans{(\Lambda^{(\ell)})}\Lambda^{(\ell)}\right]^{-1}\trans{(\Delta^{(\ell)})} - I\right)\dft{\dVec}^{(\ell)}\right\|_2^2 \nonumber \\
%&= \frac{1}{n}\sum_{j=1}^{n} \left(\frac{-\regparam^2\bm{\lambda}_j^{(\ell)}}{\bm{\delta}_j^{(\ell)} + \regparam^2\bm{\lambda}_j^{(\ell)}}\dft{\dVec}_j^{(\ell)}\right)^2,
%\label{eq:Fourier regularized residual}
%\end{align}
%where $\dft{\dVec} = \trans{U}\dVec$. \par 
%Using \eqref{eq:Filter functions}, the trace term \eqref{eq:Trace} can be written as
%\begin{equation}
%\label{eq:Trace filter}
%\trace\left(\Sigma^{(\ell)}\A^{\ell}\right) = \noiseSD_\ell^2 \sum_{j=1}^{n} \filt_j^{(\ell)}
%\end{equation}
%and the regularized residual term \eqref{eq:Fourier regularized residual} can be written as
%\begin{equation}
%\label{eq:Fourier regularized residual filter}
%\frac{1}{n}\|\rVec^{(\ell)}(\regparam)\|_2^2 = \frac{1}{n}\sum_{j=1}^{n} \left(-\mfilt_j^{(\ell)}\dft{\dVec}_j^{(\ell)}\right)^2 = \frac{1}{n}\sum_{j=1}^{n} \left(\mfilt_j^{(\ell)}\right)^2\left(\dft{\dVec}_j^{(\ell)}\right)^2.
%\end{equation}
%The representations \eqref{eq:Trace filter} and \eqref{eq:Fourier regularized residual filter}, or equivalently \eqref{eq:Trace} and \eqref{eq:Fourier regularized residual}, provide a means to explicitly describe the UPRE, MDP, and GCV functions in terms of the spectra of the system and penalty matrices as well as the components of $\dft{\dVec}$. \par 
%One application of these representation is the differentiation of the UPRE and GCV functions, which is useful for turning the UPRE and GCV methods from minimization problems to root-finding problems. The derivative of \eqref{eq:Averaged UPRE} with respect to $\regparam$ is
%\[\widetilde{U}'(\regparam) = \frac{1}{R}\sum_{\ell=1}^R \frac{d}{d\regparam}U^{(\ell)}(\regparam)\]
%where
%\begin{align}
%\frac{d}{d\regparam}U^{(\ell)}(\regparam) &= \frac{d}{d\regparam}\left\{\frac{1}{n}\|\rVec^{(\ell)}(\regparam)\|_2^2 + \frac{2}{n} \trace\left(\Sigma^{(\ell)} A_\regparam^{(\ell)}\right) - \frac{1}{n} \trace\left(\Sigma^{(\ell)}\right)\right\} \nonumber \\
%&= \frac{1}{n} \frac{d}{d\regparam}\left\{\sum_{j=1}^{n} \left(\mfilt_j^{(\ell)}\right)^2\left(\dft{\dVec}_j^{(\ell)}\right)^2 + 2 \trace\left(\Sigma^{(\ell)} A_\regparam^{(\ell)}\right) \right\} \nonumber \\
%&= \frac{4\regparam}{n} \left[\sum_{j=1}^{n} \frac{\regparam^2\bm{\delta}_j^{(\ell)}\left(\bm{\lambda}_j^{(\ell)}\right)^2}{\left(\bm{\delta}_j^{(\ell)} + \regparam^2 \bm{\lambda}_j^{(\ell)}\right)^3} \left(\dft{\dVec}_j^{(\ell)}\right)^2 - \noiseSD_\ell^2 \sum_{j=1}^{n} \frac{\bm{\delta}_j^{(\ell)}\bm{\lambda}_j^{(\ell)}}{\left(\bm{\delta}_j^{(\ell)} + \regparam^2 \bm{\lambda}_j^{(\ell)}\right)^2}\right].
%\end{align}
%Since the regularization parameter is chosen as a zero of $\widetilde{U}'(\regparam)$, the leading constants can be dropped during numerical implementation. \par 
%Differentiation of the adapted GCV function \eqref{eq:GCV Big 2} is more complicated without the assumption that $A^{(\ell)} = A$ and $L^{(\ell)} = L$ for all $\ell = 1,\ldots,R$. Writing $\widetilde{G}(\regparam)$ in terms of filter functions \eqref{eq:Filter functions} first yields
%\[\widetilde{G}(\regparam) = \frac{\frac{1}{N}\sum_{\ell=1}^R \left(\sum_{j=1}^n \left(\mfilt_j^{(\ell)}\right)^2\left(\dft{\dVec}_j^{(\ell)}\right)^2\right)}{\left[1 - \frac{1}{N}\sum_{\ell=1}^R \left(\sum_{j=1}^n \filt_j^{(\ell)}\right)\right]^2}.\]
%Since finding a zero of $\widetilde{G}'(\regparam)$ is equivalent to finding a zero of the numerator of $\widetilde{G}'(\regparam)$, focus is dedicated to just the numerator of $\widetilde{G}'(\regparam)$:
%\begin{align*}
%\frac{2}{N}\left[1 - \frac{1}{N}\sum_{\ell=1}^R \left(\sum_{j=1}^n \filt_j^{(\ell)}\right)\right]^2\left[\sum_{\ell=1}^R \left(\sum_{j=1}^n \mfilt_j^{(\ell)}\left(\frac{d}{d\regparam}\mfilt_j^{(\ell)}\right)\left(\dft{\dVec}_j^{(\ell)}\right)^2\right)\right] \\
%+ \frac{2}{N^2}\left[\sum_{\ell=1}^R \left(\sum_{j=1}^n \left(\mfilt_j^{(\ell)}\right)^2\left(\dft{\dVec}_j^{(\ell)}\right)^2\right)\right]\left[1 - \frac{1}{N}\sum_{\ell=1}^R \left(\sum_{j=1}^n \filt_j^{(\ell)}\right)\right]\left[\sum_{\ell=1}^R \left(\sum_{j=1}^n \frac{d}{d\regparam}\filt_j^{(\ell)}\right)\right].
%\end{align*}
%Factoring terms, the process of finding zeros of $\widetilde{G}'(\regparam)$ is reduced to finding solutions of 
%\begin{align*}
%0 = \left[1 - \frac{1}{N}\sum_{\ell=1}^R \left(\sum_{j=1}^n \filt_j^{(\ell)}\right)\right]\left[\sum_{\ell=1}^R \left(\sum_{j=1}^n \mfilt_j^{(\ell)}\left(\frac{d}{d\regparam}\mfilt_j^{(\ell)}\right)\left(\dft{\dVec}_j^{(\ell)}\right)^2\right)\right] \\
%+ \frac{1}{N}\left[\sum_{\ell=1}^R \left(\sum_{j=1}^n \left(\mfilt_j^{(\ell)}\right)^2\left(\dft{\dVec}_j^{(\ell)}\right)^2\right)\right]\left[\sum_{\ell=1}^R \left(\sum_{j=1}^n \frac{d}{d\regparam}\filt_j^{(\ell)}\right)\right].
%\end{align*}
%\indent We conclude Section \ref{sec:Methods} by showing a relationship between the adapted methods and the methods as simply applied to averaged data; the relationship is shown by first making some additional assumptions. The assumptions we make in addition to $\Sigma^{(\ell)} = \noiseSD_\ell^2 I$ are that $A^{(\ell)} = A$ and $L^{(\ell)} = L$ as well for all $\ell = 1,\ldots,R$. The consequence of these assumptions is that $\filt_j^{(\ell)} = \filt_j$ (and $\mfilt_j^{(\ell)} = \mfilt_j$) for all $\ell = 1,\ldots,R$. As a remark, recall from Section \ref{sec:Methods} that these assumptions are not necessary for obtaining the results \eqref{eq:Averaged UPRE} and \eqref{eq:Averaged MDP} for the UPRE and MDP methods, respectively. However, these assumptions are necessary in obtaining the corresponding result \eqref{eq:Averaged GCV} for the GCV method. We are now in a situation where all three results hold. \par 
%Though the following manipulations are done to the adapted UPRE function, they can also be done for the adapted MDP and GCV functions. Using representations \eqref{eq:Trace filter} and \eqref{eq:Fourier regularized residual filter}, we can write \eqref{eq:Averaged UPRE} as
%\begin{align}
%\label{eq:Non-average UPRE}
%\frac{1}{R} \sum_{\ell=1}^R \U^{(\ell)}(\regparam) &= \frac{1}{R} \sum_{\ell=1}^R \left[\frac{1}{n}\sum_{j=1}^{n} \left(\mfilt_j\right)^2|\dft{\dVec}_j^{(\ell)}|^2 + \frac{2\noiseSD_\ell^2}{n} \sum_{j=1}^{n} \filt_j - \noiseSD_\ell^2\right] \nonumber \\
%&= \frac{1}{R} \sum_{\ell=1}^R \left(\frac{1}{n}\sum_{j=1}^{n} \left(\mfilt_j\right)^2|\dft{\dVec}_j^{(\ell)}|^2\right) + \frac{1}{R} \sum_{\ell=1}^R \left(\frac{2\noiseSD_\ell^2}{n} \sum_{j=1}^{n} \filt_j\right) - \frac{1}{R} \sum_{\ell=1}^R\noiseSD_\ell^2 \nonumber \\
%&= \frac{1}{n}\sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\frac{1}{R} \sum_{\ell=1}^R |\dft{\dVec}_j^{(\ell)}|^2\right) + \frac{2}{n} \left(\frac{1}{R} \sum_{\ell=1}^R \noiseSD_\ell^2\right) \left(\sum_{j=1}^{n} \filt_j\right) - \frac{1}{R} \sum_{\ell=1}^R\noiseSD_\ell^2 \nonumber \\
%&= R\left[\frac{1}{n}\sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\frac{1}{R^2} \sum_{\ell=1}^R |\dft{\dVec}_j^{(\ell)}|^2\right) + \frac{2}{n} \left(\frac{1}{R^2} \sum_{\ell=1}^R \noiseSD_\ell^2\right) \left(\sum_{j=1}^{n} \filt_j\right) - \frac{1}{R^2} \sum_{\ell=1}^R\noiseSD_\ell^2\right]
%\end{align}
%where the last equality is obtained through multiplication by $\frac{R}{R}$. Note that the term $\frac{1}{R^2} \sum_{\ell=1}^R\noiseSD_\ell^2$ is the variance of $\frac{1}{R} \sum_{\ell=1}^R \noiseVec^{(\ell)}$ since the random vectors $\{\noiseVec^{(\ell)}\}_{\ell=1}^R$ are mutually independent. However, \eqref{eq:Non-average UPRE} is not equal to $R$ times the UPRE function as applied to the average of the data
%\[\aVec \coloneqq \frac{1}{R}\sum_{\ell=1}^R \dVec^{(\ell)} = \frac{1}{R} \sum_{\ell=1}^R \bVec^{(\ell)} + \frac{1}{R} \sum_{\ell=1}^R \noiseVec^{(\ell)}\] 
%because of the term $\frac{1}{R^2} \sum_{\ell=1}^R |\dft{\dVec}_j^{(\ell)}|^2$. If the UPRE method was used with the average $\aVec$, then
%\begin{equation}
%\label{eq:Coefficients of Average}
%|\dft{\aVec}_j|^2 = \frac{1}{R^2}\left|\sum_{\ell=1}^R \dft{\dVec}_j^{(\ell)}\right|^2 \leq \frac{1}{R^2} \sum_{\ell=1}^R |\dft{\dVec}_j^{(\ell)}|^2, \qquad j = 1,\ldots,n.
%\end{equation}
%Thus the result \eqref{eq:Non-average UPRE} shows that for
%\begin{equation}
%\label{eq:UPRE of Average}
%\overline{U}(\regparam) \coloneqq \frac{1}{n}\sum_{j=1}^{n} \left(\mfilt_j\right)^2|\dft{\aVec}_j|^2 + \frac{2}{n} \left(\frac{1}{R^2} \sum_{\ell=1}^R \noiseSD_\ell^2\right) \left(\sum_{j=1}^{n} \filt_j\right) - \frac{1}{R^2} \sum_{\ell=1}^R\noiseSD_\ell^2,
%\end{equation}
%i.e. $\overline{U}(\regparam)$ is the UPRE function as applied to the average $\aVec$ of the data $\{\dVec^{(\ell)}\}_{\ell=1}^R$, we have that
%\begin{equation}
%\label{eq:UPRE Bound}
%\widetilde{U}(\regparam) = \frac{1}{R} \sum_{\ell=1}^R \U^{(\ell)}(\regparam) \leq R \overline{U}(\regparam), \qquad \regparam \geq 0.
%\end{equation}
%Therefore the adapted UPRE function is truly distinct from simply applying the UPRE method to the average of the data, and the bound \eqref{eq:UPRE Bound} demonstrates a relationship between the two modalities. \par 
%Analogous bounds can be obtained for the MDP and GCV functions as well. For the GCV method,
%\[\widetilde{\G}(\regparam) = \frac{1}{R} \sum_{\ell=1}^R \G^{(\ell)}(\regparam) = \frac{1}{R}\sum_{\ell=1}^R \left[\frac{\frac{1}{n}\|\regres^{(\ell)}\|_2^2}{\left[1 - \frac{1}{n}\trace\left(\A\right)\right]^2}\right]  = R\frac{\frac{1}{n}\sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\frac{1}{R^2} \sum_{\ell=1}^R |\dft{\dVec}_j^{(\ell)}|^2\right)}{\left[1 - \frac{1}{n}\trace\left(\A\right)\right]^2},\]
%again where the last equality is obtained through multiplication by $\frac{R}{R}$. Using \eqref{eq:Coefficients of Average} and letting
%\[\overline{\G}(\regparam) = \frac{\frac{1}{n}\sum_{j=1}^{n} \left(\mfilt_j\right)^2|\dft{\aVec}_j|^2}{\left[1 - \frac{1}{n}\trace\left(\A\right)\right]^2},\]
%we have
%\begin{equation}
%\label{eq:GCV Bound}
%\widetilde{\G}(\regparam) = \frac{1}{R} \sum_{\ell=1}^R \G^{(\ell)}(\regparam) \leq R \overline{\G}(\regparam), \qquad \regparam \geq 0.
%\end{equation}
%Lastly, for the MDP method we have $\widetilde{\D}(\regparam)$ is equal to
%\[\frac{1}{R} \sum_{\ell=1}^R \D^{(\ell)}(\regparam) = \frac{1}{R}\sum_{\ell=1}^R \left[\frac{1}{n}\|\rVec^{(\ell)}(\regparam)\|_2^2 - \noiseSD_{\ell}^2\right] = R\left[\frac{1}{n}\sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\frac{1}{R^2} \sum_{\ell=1}^R |\dft{\dVec}_j^{(\ell)}|^2\right) - \frac{1}{R^2}\sum_{\ell=1}^R \noiseSD_{\ell}^2\right]\]
%so that letting
%\[\overline{\D}(\regparam) = \frac{1}{n}\sum_{j=1}^{n} \left(\mfilt_j\right)^2 |\dft{\aVec}_j|^2 - \frac{1}{R^2}\sum_{\ell=1}^R \noiseSD_{\ell}^2\]
%results in
%\begin{equation}
%\label{eq:MDP Bound}
%\widetilde{\D}(\regparam) = \frac{1}{R} \sum_{\ell=1}^R \D^{(\ell)}(\regparam) \leq R \overline{\D}(\regparam), \qquad \regparam \geq 0.
%\end{equation}

%% Fourier version
%\subsection{Numeric implementation} \label{sec:Implementation}
%In an effort to streamline a discussion on the implementation of the parameter selection methods, we assume that
%\begin{equation}
%\label{eq:Fourier diagonalization}
%A^{(\ell)} = \trans{F}{\Delta^{(\ell)}}F, \quad L^{(\ell)} = \trans{F}{\Lambda^{(\ell)}}F, \qquad \ell = 1,\ldots,R
%\end{equation}
%where $F$ is the unitary discrete Fourier transform (DFT) matrix and $\Delta^{(\ell)}$ and $\Lambda^{(\ell)}$ are diagonal matrices. This assumption is made for the sake of convenience; if the matrices $A^{(\ell)}$ and $L^{(\ell)}$ cannot be simultaneously diagonalized with respect to the DFT or any other unitary transformation, such as the discrete sine or cosine transform, then the GSVD could be utilized instead. \par
%As motivation for the following derivation, observe that the UPRE, MDP, and GCV methods all involve terms $\frac{1}{n}\|\rVec^{(\ell)}(\regparam)\|_2^2$. In addition, the UPRE and GCV methods involve $\trace(\A^{(\ell)})$. Thus for the implementation of these methods, we consider different representations of $\frac{1}{n}\|\rVec^{(\ell)}(\regparam)\|_2^2$ and $\trace(\A^{(\ell)})$. Using \eqref{eq:Fourier diagonalization}, the matrix $\A^{(\ell)}$ given by \eqref{eq:Influence matrix} can be expressed as
%\begin{equation}
%\label{eq:Fourier diagonalization 2}
%\A^{(\ell)} = \trans{F}\Delta^{(\ell)}\left[\trans{(\Delta^{(\ell)})}\Delta^{(\ell)} + \regparam^2\trans{(\Lambda^{(\ell)})}\Lambda^{(\ell)}\right]^{-1}\trans{(\Delta^{(\ell)})}F, \qquad \ell = 1,\ldots,R.
%\end{equation}
%If $\Sigma^{(\ell)} = \noiseSD_\ell^2I$ for all $\ell = 1,\ldots,R,$ then the similarity invariance of the trace operator gives
%\begin{align}
%\trace\left(\Sigma^{(\ell)}\A^{\ell}\right) &= \noiseSD_\ell^2 \trace\left(\Delta^{(\ell)}\left[\trans{(\Delta^{(\ell)})}\Delta^{(\ell)} + \regparam^2\trans{(\Lambda^{(\ell)})}\Lambda^{(\ell)}\right]^{-1}\trans{(\Delta^{(\ell)})}\right) \nonumber \\
%&= \noiseSD_\ell^2 \sum_{j=1}^{n} \frac{|\bm{\delta}_j^{(\ell)}|^2}{|\bm{\delta}_j^{(\ell)}|^2 + \regparam^2 |\bm{\lambda}_j^{(\ell)}|^2}
%\label{eq:Trace}
%\end{align}
%where $\bm{\delta}^{(\ell)} = \diag(\Delta^{(\ell)})$ and $\bm{\lambda}^{(\ell)} = \diag(\Lambda^{(\ell)})$. Another benefit of using \eqref{eq:Fourier diagonalization 2} is that we can write
%\begin{align}
%\frac{1}{n}\|\rVec^{(\ell)}(\regparam)\|_2^2 &= \frac{1}{n}\left\|\trans{F}\left(\Delta^{(\ell)}\left[\trans{(\Delta^{(\ell)})}\Delta^{(\ell)} + \regparam^2\trans{(\Lambda^{(\ell)})}\Lambda^{(\ell)}\right]^{-1}\trans{(\Delta^{(\ell)})} - I\right)F\dVec^{(\ell)}\right\|_2^2 \nonumber \\
%&= \frac{1}{n}\left\|\left(\Delta^{(\ell)}\left[\trans{(\Delta^{(\ell)})}\Delta^{(\ell)} + \regparam^2\trans{(\Lambda^{(\ell)})}\Lambda^{(\ell)}\right]^{-1}\trans{(\Delta^{(\ell)})} - I\right)\left(\sqrt{n}\right)\dft{\dVec}^{(\ell)}\right\|_2^2 \nonumber \\
%&= \sum_{j=1}^{n} \left(\frac{-\regparam^2|\bm{\lambda}_j^{(\ell)}|^2}{|\bm{\delta}_j^{(\ell)}|^2 + \regparam^2|\bm{\lambda}_j^{(\ell)}|^2}|\dft{\dVec}_j^{(\ell)}|\right)^2,
%\label{eq:Fourier regularized residual}
%\end{align}
%which uses $F\dVec = (\sqrt{n})\dft{\dVec}$ with $\dft{\dVec}$ being the standard DFT of $\dVec$ \cite{Vogel:2002}. Filter functions similar to \eqref{eq:TikFilt} can be introduced to further simplify notation; letting
%\begin{equation}
%\label{eq:Filter functions}
%\filt_j^{(\ell)} = \frac{|\bm{\delta}_j^{(\ell)}|^2}{|\bm{\delta}_j^{(\ell)}|^2 + \regparam^2 |\bm{\lambda}_j^{(\ell)}|^2}, \quad \mfilt_j^{(\ell)} = 1 - \filt_j^{(\ell)} = \frac{\regparam^2|\bm{\lambda}_j^{(\ell)}|^2}{|\bm{\delta}_j^{(\ell)}|^2 + \regparam^2 |\bm{\lambda}_j^{(\ell)}|^2},
%\end{equation}
%the trace term \eqref{eq:Trace} can be written as
%\begin{equation}
%\label{eq:Trace filter}
%\trace\left(\Sigma^{(\ell)}\A^{\ell}\right) = \noiseSD_\ell^2 \sum_{j=1}^{n} \filt_j^{(\ell)}
%\end{equation}
%and the regularized residual term \eqref{eq:Fourier regularized residual} can be written as
%\begin{equation}
%\label{eq:Fourier regularized residual filter}
%\frac{1}{n}\|\rVec^{(\ell)}(\regparam)\|_2^2 = \sum_{j=1}^{n} \left(-\mfilt_j^{(\ell)}|\dft{\dVec}_j^{(\ell)}|\right)^2 = \sum_{j=1}^{n} \left(\mfilt_j^{(\ell)}\right)^2|\dft{\dVec}_j^{(\ell)}|^2.
%\end{equation}
%The representations \eqref{eq:Trace filter} and \eqref{eq:Fourier regularized residual filter}, or equivalently \eqref{eq:Trace} and \eqref{eq:Fourier regularized residual}, provide a means to explicitly describe the UPRE, MDP, and GCV functions in terms of the spectra of the system and penalty matrices as well as the components of $\dft{\dVec}$. We conclude Section \ref{sec:Methods} by showing a relationship between the adapted methods and the methods as simply applied to averaged data; the relationship is shown by first making some additional assumptions. \par 
%The assumptions we make in addition to $\Sigma^{(\ell)} = \noiseSD_\ell^2 I$ are that $A^{(\ell)} = A$ and $L^{(\ell)} = L$ as well for all $\ell = 1,\ldots,R$. The consequence of these assumptions is that $\filt_j^{(\ell)} = \filt_j$ (and $\mfilt_j^{(\ell)} = \mfilt_j$) for all $\ell = 1,\ldots,R$. As a remark, recall from Section \ref{sec:Methods} that these assumptions are not necessary for obtaining the results \eqref{eq:Averaged UPRE} and \eqref{eq:Averaged MDP} for the UPRE and MDP methods, respectively. However, these assumptions are necessary in obtaining the corresponding result \eqref{eq:Averaged GCV} for the GCV method. We are now in a situation where all three results hold. \par 
%Though the following manipulations are done to the adapted UPRE function, they can also be done for the adapted MDP and GCV functions. Using representations \eqref{eq:Trace filter} and \eqref{eq:Fourier regularized residual filter}, we can write \eqref{eq:Averaged UPRE} as
%\begin{align}
%\label{eq:Non-average UPRE}
%\frac{1}{R} \sum_{\ell=1}^R \U^{(\ell)}(\regparam) &= \frac{1}{R} \sum_{\ell=1}^R \left[\sum_{j=1}^{n} \left(\mfilt_j\right)^2|\dft{\dVec}_j^{(\ell)}|^2 + \frac{2\noiseSD_\ell^2}{n} \sum_{j=1}^{n} \filt_j - \noiseSD_\ell^2\right] \nonumber \\
%&= \frac{1}{R} \sum_{\ell=1}^R \left(\sum_{j=1}^{n} \left(\mfilt_j\right)^2|\dft{\dVec}_j^{(\ell)}|^2\right) + \frac{1}{R} \sum_{\ell=1}^R \left(\frac{2\noiseSD_\ell^2}{n} \sum_{j=1}^{n} \filt_j\right) - \frac{1}{R} \sum_{\ell=1}^R\noiseSD_\ell^2 \nonumber \\
%&= \sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\frac{1}{R} \sum_{\ell=1}^R |\dft{\dVec}_j^{(\ell)}|^2\right) + \frac{2}{n} \left(\frac{1}{R} \sum_{\ell=1}^R \noiseSD_\ell^2\right) \left(\sum_{j=1}^{n} \filt_j\right) - \frac{1}{R} \sum_{\ell=1}^R\noiseSD_\ell^2 \nonumber \\
%&= R\left[\sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\frac{1}{R^2} \sum_{\ell=1}^R |\dft{\dVec}_j^{(\ell)}|^2\right) + \frac{2}{n} \left(\frac{1}{R^2} \sum_{\ell=1}^R \noiseSD_\ell^2\right) \left(\sum_{j=1}^{n} \filt_j\right) - \frac{1}{R^2} \sum_{\ell=1}^R\noiseSD_\ell^2\right]
%\end{align}
%where the last equality is obtained through multiplication by $\frac{R}{R}$. Note that the term $\frac{1}{R^2} \sum_{\ell=1}^R\noiseSD_\ell^2$ is the variance of $\frac{1}{R} \sum_{\ell=1}^R \noiseVec^{(\ell)}$ since the random vectors $\{\noiseVec^{(\ell)}\}_{\ell=1}^R$ are mutually independent. However, \eqref{eq:Non-average UPRE} is not equal to $R$ times the UPRE function as applied to the average of the data
%\[\aVec \coloneqq \frac{1}{R}\sum_{\ell=1}^R \dVec^{(\ell)} = \frac{1}{R} \sum_{\ell=1}^R \bVec^{(\ell)} + \frac{1}{R} \sum_{\ell=1}^R \noiseVec^{(\ell)}\] 
%because of the term $\frac{1}{R^2} \sum_{\ell=1}^R |\dft{\dVec}_j^{(\ell)}|^2$. If the UPRE method was used with the average $\aVec$, then
%\begin{equation}
%\label{eq:Coefficients of Average}
%|\dft{\aVec}_j|^2 = \frac{1}{R^2}\left|\sum_{\ell=1}^R \dft{\dVec}_j^{(\ell)}\right|^2 \leq \frac{1}{R^2} \sum_{\ell=1}^R |\dft{\dVec}_j^{(\ell)}|^2, \qquad j = 1,\ldots,n.
%\end{equation}
%Thus the result \eqref{eq:Non-average UPRE} shows that for
%\begin{equation}
%\label{eq:UPRE of Average}
%\overline{U}(\regparam) \coloneqq \sum_{j=1}^{n} \left(\mfilt_j\right)^2|\dft{\aVec}_j|^2 + \frac{2}{n} \left(\frac{1}{R^2} \sum_{\ell=1}^R \noiseSD_\ell^2\right) \left(\sum_{j=1}^{n} \filt_j\right) - \frac{1}{R^2} \sum_{\ell=1}^R\noiseSD_\ell^2,
%\end{equation}
%i.e. $\overline{U}(\regparam)$ is the UPRE function as applied to the average $\aVec$ of the data $\{\dVec^{(\ell)}\}_{\ell=1}^R$, we have that
%\begin{equation}
%\label{eq:UPRE Bound}
%\widetilde{U}(\regparam) = \frac{1}{R} \sum_{\ell=1}^R \U^{(\ell)}(\regparam) \leq R \overline{U}(\regparam), \qquad \regparam \geq 0.
%\end{equation}
%Therefore the adapted UPRE function is truly distinct from simply applying the UPRE method to the average of the data, and the bound \eqref{eq:UPRE Bound} demonstrates a relationship between the two modalities. Analogous bounds can be obtained for the MDP and GCV functions as well.
%
%In an effort to streamline a discussion on the implementation of the parameter selection methods, we assume that
%\begin{equation}
%\label{eq:Fourier diagonalization}
%A^{(\ell)} = \trans{F}{\Delta^{(\ell)}}F, \quad L^{(\ell)} = \trans{F}{\Lambda^{(\ell)}}F, \qquad \ell = 1,\ldots,R
%\end{equation}
%where $F$ is the unitary discrete Fourier transform (DFT) matrix and $\Delta^{(\ell)}$ and $\Lambda^{(\ell)}$ are diagonal matrices. This assumption is made for the sake of convenience; if the matrices $A^{(\ell)}$ and $L^{(\ell)}$ cannot be simultaneously diagonalized with respect to the DFT or any other unitary transformation, such as the discrete sine or cosine transform, then the GSVD could be utilized instead. \par
%As motivation for the following derivation, observe that the UPRE, MDP, and GCV methods all involve terms $\frac{1}{n}\|\rVec^{(\ell)}(\regparam)\|_2^2$. In addition, the UPRE and GCV methods involve $\trace(\A^{(\ell)})$. Thus for the implementation of these methods, we consider different representations of $\frac{1}{n}\|\rVec^{(\ell)}(\regparam)\|_2^2$ and $\trace(\A^{(\ell)})$. Using \eqref{eq:Fourier diagonalization}, the matrix $\A^{(\ell)}$ given by \eqref{eq:Influence matrix} can be expressed as
%\begin{equation}
%\label{eq:Fourier diagonalization 2}
%\A^{(\ell)} = \trans{F}\Delta^{(\ell)}\left[\trans{(\Delta^{(\ell)})}\Delta^{(\ell)} + \regparam^2\trans{(\Lambda^{(\ell)})}\Lambda^{(\ell)}\right]^{-1}\trans{(\Delta^{(\ell)})}F, \qquad \ell = 1,\ldots,R.
%\end{equation}
%If $\Sigma^{(\ell)} = \noiseSD_\ell^2I$ for all $\ell = 1,\ldots,R,$ then the similarity invariance of the trace operator gives
%\begin{align}
%\trace\left(\Sigma^{(\ell)}\A^{\ell}\right) &= \noiseSD_\ell^2 \trace\left(\Delta^{(\ell)}\left[\trans{(\Delta^{(\ell)})}\Delta^{(\ell)} + \regparam^2\trans{(\Lambda^{(\ell)})}\Lambda^{(\ell)}\right]^{-1}\trans{(\Delta^{(\ell)})}\right) \nonumber \\
%&= \noiseSD_\ell^2 \sum_{j=1}^{n} \frac{|\bm{\delta}_j^{(\ell)}|^2}{|\bm{\delta}_j^{(\ell)}|^2 + \regparam^2 |\bm{\lambda}_j^{(\ell)}|^2}
%\label{eq:Trace}
%\end{align}
%where $\bm{\delta}^{(\ell)} = \diag(\Delta^{(\ell)})$ and $\bm{\lambda}^{(\ell)} = \diag(\Lambda^{(\ell)})$. Another benefit of using \eqref{eq:Fourier diagonalization 2} is that we can write
%\begin{align}
%\frac{1}{n}\|\rVec^{(\ell)}(\regparam)\|_2^2 &= \frac{1}{n}\left\|\trans{F}\left(\Delta^{(\ell)}\left[\trans{(\Delta^{(\ell)})}\Delta^{(\ell)} + \regparam^2\trans{(\Lambda^{(\ell)})}\Lambda^{(\ell)}\right]^{-1}\trans{(\Delta^{(\ell)})} - I\right)F\dVec^{(\ell)}\right\|_2^2 \nonumber \\
%&= \frac{1}{n}\left\|\left(\Delta^{(\ell)}\left[\trans{(\Delta^{(\ell)})}\Delta^{(\ell)} + \regparam^2\trans{(\Lambda^{(\ell)})}\Lambda^{(\ell)}\right]^{-1}\trans{(\Delta^{(\ell)})} - I\right)\left(\sqrt{n}\right)\dft{\dVec}^{(\ell)}\right\|_2^2 \nonumber \\
%&= \sum_{j=1}^{n} \left(\frac{-\regparam^2|\bm{\lambda}_j^{(\ell)}|^2}{|\bm{\delta}_j^{(\ell)}|^2 + \regparam^2|\bm{\lambda}_j^{(\ell)}|^2}|\dft{\dVec}_j^{(\ell)}|\right)^2,
%\label{eq:Fourier regularized residual}
%\end{align}
%which uses $F\dVec = (\sqrt{n})\dft{\dVec}$ with $\dft{\dVec}$ being the standard DFT of $\dVec$ \cite{Vogel:2002}. Filter functions similar to \eqref{eq:TikFilt} can be introduced to further simplify notation; letting
%\begin{equation}
%\label{eq:Filter functions}
%\filt_j^{(\ell)} = \frac{|\bm{\delta}_j^{(\ell)}|^2}{|\bm{\delta}_j^{(\ell)}|^2 + \regparam^2 |\bm{\lambda}_j^{(\ell)}|^2}, \quad \mfilt_j^{(\ell)} = 1 - \filt_j^{(\ell)} = \frac{\regparam^2|\bm{\lambda}_j^{(\ell)}|^2}{|\bm{\delta}_j^{(\ell)}|^2 + \regparam^2 |\bm{\lambda}_j^{(\ell)}|^2},
%\end{equation}
%the trace term \eqref{eq:Trace} can be written as
%\begin{equation}
%\label{eq:Trace filter}
%\trace\left(\Sigma^{(\ell)}\A^{\ell}\right) = \noiseSD_\ell^2 \sum_{j=1}^{n} \filt_j^{(\ell)}
%\end{equation}
%and the regularized residual term \eqref{eq:Fourier regularized residual} can be written as
%\begin{equation}
%\label{eq:Fourier regularized residual filter}
%\frac{1}{n}\|\rVec^{(\ell)}(\regparam)\|_2^2 = \sum_{j=1}^{n} \left(-\mfilt_j^{(\ell)}|\dft{\dVec}_j^{(\ell)}|\right)^2 = \sum_{j=1}^{n} \left(\mfilt_j^{(\ell)}\right)^2|\dft{\dVec}_j^{(\ell)}|^2.
%\end{equation}
%The representations \eqref{eq:Trace filter} and \eqref{eq:Fourier regularized residual filter}, or equivalently \eqref{eq:Trace} and \eqref{eq:Fourier regularized residual}, provide a means to explicitly describe the UPRE, MDP, and GCV functions in terms of the spectra of the system and penalty matrices as well as the components of $\dft{\dVec}$. We conclude Section \ref{sec:Methods} by showing a relationship between the adapted methods and the methods as simply applied to averaged data; the relationship is shown by first making some additional assumptions. \par 
%The assumptions we make in addition to $\Sigma^{(\ell)} = \noiseSD_\ell^2 I$ are that $A^{(\ell)} = A$ and $L^{(\ell)} = L$ as well for all $\ell = 1,\ldots,R$. The consequence of these assumptions is that $\filt_j^{(\ell)} = \filt_j$ (and $\mfilt_j^{(\ell)} = \mfilt_j$) for all $\ell = 1,\ldots,R$. As a remark, recall from Section \ref{sec:Methods} that these assumptions are not necessary for obtaining the results \eqref{eq:Averaged UPRE} and \eqref{eq:Averaged MDP} for the UPRE and MDP methods, respectively. However, these assumptions are necessary in obtaining the corresponding result \eqref{eq:Averaged GCV} for the GCV method. We are now in a situation where all three results hold. \par 
%Though the following manipulations are done to the adapted UPRE function, they can also be done for the adapted MDP and GCV functions. Using representations \eqref{eq:Trace filter} and \eqref{eq:Fourier regularized residual filter}, we can write \eqref{eq:Averaged UPRE} as
%\begin{align}
%\label{eq:Non-average UPRE}
%\frac{1}{R} \sum_{\ell=1}^R \U^{(\ell)}(\regparam) &= \frac{1}{R} \sum_{\ell=1}^R \left[\sum_{j=1}^{n} \left(\mfilt_j\right)^2|\dft{\dVec}_j^{(\ell)}|^2 + \frac{2\noiseSD_\ell^2}{n} \sum_{j=1}^{n} \filt_j - \noiseSD_\ell^2\right] \nonumber \\
%&= \frac{1}{R} \sum_{\ell=1}^R \left(\sum_{j=1}^{n} \left(\mfilt_j\right)^2|\dft{\dVec}_j^{(\ell)}|^2\right) + \frac{1}{R} \sum_{\ell=1}^R \left(\frac{2\noiseSD_\ell^2}{n} \sum_{j=1}^{n} \filt_j\right) - \frac{1}{R} \sum_{\ell=1}^R\noiseSD_\ell^2 \nonumber \\
%&= \sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\frac{1}{R} \sum_{\ell=1}^R |\dft{\dVec}_j^{(\ell)}|^2\right) + \frac{2}{n} \left(\frac{1}{R} \sum_{\ell=1}^R \noiseSD_\ell^2\right) \left(\sum_{j=1}^{n} \filt_j\right) - \frac{1}{R} \sum_{\ell=1}^R\noiseSD_\ell^2 \nonumber \\
%&= R\left[\sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\frac{1}{R^2} \sum_{\ell=1}^R |\dft{\dVec}_j^{(\ell)}|^2\right) + \frac{2}{n} \left(\frac{1}{R^2} \sum_{\ell=1}^R \noiseSD_\ell^2\right) \left(\sum_{j=1}^{n} \filt_j\right) - \frac{1}{R^2} \sum_{\ell=1}^R\noiseSD_\ell^2\right]
%\end{align}
%where the last equality is obtained through multiplication by $\frac{R}{R}$. Note that the term $\frac{1}{R^2} \sum_{\ell=1}^R\noiseSD_\ell^2$ is the variance of $\frac{1}{R} \sum_{\ell=1}^R \noiseVec^{(\ell)}$ since the random vectors $\{\noiseVec^{(\ell)}\}_{\ell=1}^R$ are mutually independent. However, \eqref{eq:Non-average UPRE} is not equal to $R$ times the UPRE function as applied to the average of the data
%\[\aVec \coloneqq \frac{1}{R}\sum_{\ell=1}^R \dVec^{(\ell)} = \frac{1}{R} \sum_{\ell=1}^R \bVec^{(\ell)} + \frac{1}{R} \sum_{\ell=1}^R \noiseVec^{(\ell)}\] 
%because of the term $\frac{1}{R^2} \sum_{\ell=1}^R |\dft{\dVec}_j^{(\ell)}|^2$. If the UPRE method was used with the average $\aVec$, then
%\begin{equation}
%\label{eq:Coefficients of Average}
%|\dft{\aVec}_j|^2 = \frac{1}{R^2}\left|\sum_{\ell=1}^R \dft{\dVec}_j^{(\ell)}\right|^2 \leq \frac{1}{R^2} \sum_{\ell=1}^R |\dft{\dVec}_j^{(\ell)}|^2, \qquad j = 1,\ldots,n.
%\end{equation}
%Thus the result \eqref{eq:Non-average UPRE} shows that for
%\begin{equation}
%\label{eq:UPRE of Average}
%\overline{U}(\regparam) \coloneqq \sum_{j=1}^{n} \left(\mfilt_j\right)^2|\dft{\aVec}_j|^2 + \frac{2}{n} \left(\frac{1}{R^2} \sum_{\ell=1}^R \noiseSD_\ell^2\right) \left(\sum_{j=1}^{n} \filt_j\right) - \frac{1}{R^2} \sum_{\ell=1}^R\noiseSD_\ell^2,
%\end{equation}
%i.e. $\overline{U}(\regparam)$ is the UPRE function as applied to the average $\aVec$ of the data $\{\dVec^{(\ell)}\}_{\ell=1}^R$, we have that
%\begin{equation}
%\label{eq:UPRE Bound}
%\widetilde{U}(\regparam) = \frac{1}{R} \sum_{\ell=1}^R \U^{(\ell)}(\regparam) \leq R \overline{U}(\regparam), \qquad \regparam \geq 0.
%\end{equation}
%Therefore the adapted UPRE function is truly distinct from simply applying the UPRE method to the average of the data, and the bound \eqref{eq:UPRE Bound} demonstrates a relationship between the two modalities. Analogous bounds can be obtained for the MDP and GCV functions as well.

%It is from \eqref{eq:Coefficients of Average} and \eqref{eq:UPRE Bound} that we base a heuristic advocating the use of $\widetilde{U}(\regparam)$ for the selection of a regularization parameter for multiple data sets. If the data are similar, in that
%\begin{equation}
%\label{eq:Heuristic Assumption}
%\frac{1}{R^2}\left|\sum_{\ell=1}^R \dft{\dVec}_j^{(\ell)}\right|^2 \approx \frac{1}{R^2} \sum_{\ell=1}^R |\dft{\dVec}_j^{(\ell)}|^2, \qquad j = 1,\ldots,n,
%\end{equation}
%then $\widetilde{U}(\regparam) \approx R \overline{U}(\regparam)$ which would suggest that the minimizers of these functions could be close in value.

%is that we can express terms in the UPRE, GCV, and MDP functions in a way that is more tractable for differentiation with respect to $\regparam$. \par
%Since the MDP method relies on finding a root, the UPRE and GCV functions will be cast as in a similar way. Starting with the UPRE method from Section \ref{sec:UPRE}, we now write \eqref{eq:Individual UPRE} as
%\begin{equation}
%\label{eq:Individual UPRE 2}
%\U^{(\ell)}(\regparam) = \sum_{j=1}^{n} \left(\mfilt_j^{(\ell)}(\regparam)\right)^2|\dft{\dVec}_j^{(\ell)}|^2 + \frac{2\noiseSD_\ell^2}{n}\sum_{j=1}^{n} \filt_j^{(\ell)}(\regparam) - \noiseSD_{\ell}^2
%\end{equation}
%for $\ell = 1,\ldots,R$. The first and second derivatives of \eqref{eq:Individual UPRE 2} with respect to $\regparam$ are, respectively,
%\begin{align}
%\frac{d}{d\regparam}\U^{(\ell)}(\regparam) &= 2\sum_{j=1}^{n} \mfilt_j^{(\ell)}(\regparam)\left(\frac{d}{d\regparam}\mfilt_j^{(\ell)}(\regparam)\right)|\dft{\dVec}_j^{(\ell)}|^2 + \frac{2\noiseSD_\ell^2}{n} \sum_{j=1}^{n} \frac{d}{d\regparam}\left(\filt_j^{(\ell)}(\regparam)\right), \\
%\frac{d^2}{d\regparam^2}\U^{(\ell)}(\regparam) &= 2\sum_{j=1}^{n} \left[\left(\frac{d}{d\regparam}\mfilt_j^{(\ell)}(\regparam)\right)^2 + \mfilt_j^{(\ell)}(\regparam)\left(\frac{d^2}{d\regparam^2}\mfilt_j^{(\ell)}(\regparam)\right)\right]|\dft{\dVec}_j^{(\ell)}|^2 + \frac{2\noiseSD_\ell^2}{n} \sum_{j=1}^{n} \frac{d^2}{d\regparam^2}\left(\filt_j^{(\ell)}(\regparam)\right).
%\end{align}
%Second derivatives will be used later for an argument regarding convexity of the parameter selection functions. \par 
%For the GCV method, we can write \eqref{eq:Individual GCV} as 
%\begin{equation}
%\label{eq:Individual GCV 2}
%\G^{(\ell)}(\regparam) = \frac{\sum_{j=1}^{n} \left(\mfilt_j^{(\ell)}(\regparam)\right)^2|\dft{\dVec}_j^{(\ell)}|^2}{\left[1 - \frac{1}{n}\sum_{j=1}^{n} \filt_j^{(\ell)}(\regparam)\right]^2} = n^2 \frac{\sum_{j=1}^{n} \left(\mfilt_j^{(\ell)}(\regparam)\right)^2|\dft{\dVec}_j^{(\ell)}|^2}{\left[\sum_{j=1}^{n} \mfilt_j^{(\ell)}(\regparam)\right]^2}, \quad \ell = 1,\ldots,R.
%\end{equation}
%Recall from Section \ref{sec:GCV} that \eqref{eq:GCV Big 3} only holds if $\A^{(\ell)} = \A$, or equivalently $\mfilt_j^{(\ell)}(\regparam) = \mfilt_j(\regparam)$, for all $\ell = 1,\ldots,R$. Assuming this is the case, the first derivative of \eqref{eq:Individual GCV 2} is
%\begin{align}
%\frac{d}{d\regparam}\G^{(\ell)}(\regparam) &= \frac{n^2}{\left[\sum_{j=1}^{n} \mfilt_j^{(\ell)}(\regparam)\right]^4}  \Bigg(2\left[\sum_{j=1}^{n} \mfilt_j^{(\ell)}(\regparam)\left(\frac{d}{d\regparam}\mfilt_j^{(\ell)}(\regparam)\right)|\dft{\dVec}_j^{(\ell)}|^2\right]\left[\sum_{j=1}^{n} \mfilt_j^{(\ell)}(\regparam)\right]^2 \\
%&- \left[\sum_{j=1}^{n} \left(\mfilt_j^{(\ell)}(\regparam)\right)^2|\dft{\dVec}_j^{(\ell)}|^2\right]\Bigg[2\sum_{j=1}^{n}\mfilt_j^{(\ell)}(\regparam)\left(\frac{d}{d\regparam}\mfilt_j^{(\ell)}(\regparam)\right) \\
%&+ \sum_{j \neq k} \left(\frac{d}{d\regparam}\mfilt_j(\regparam)\right)\mfilt_k(\regparam) + \mfilt_j(\regparam)\left(\frac{d}{d\regparam}\mfilt_k(\regparam)\right)\Bigg]\Bigg).
%\end{align}
%
%The MDP function can be written as
%\begin{equation}
%\label{eq:Individual MDP 2}
%\D^{(\ell)}(\regparam) = \sum_{j=1}^{n} \left(\mfilt_j^{(\ell)}(\regparam)\right)^2|\dft{\dVec}_j^{(\ell)}|^2 - \noiseSD_\ell^2, \quad \ell = 1,\ldots,R.
%\end{equation}
%The first derivative of \eqref{eq:Individual MDP 2} with respect to $\regparam$ is 
%\begin{equation}
%\label{eq:Individual MDP Derivative}
%\frac{d}{d\regparam}\D^{(\ell)}(\regparam) = 2\sum_{j=1}^{n} \mfilt_j^{(\ell)}(\regparam)\left(\frac{d}{d\regparam}\mfilt_j^{(\ell)}(\regparam)\right)|\dft{\dVec}_j^{(\ell)}|^2.
%\end{equation}

% Before extending the MDP method to account for multiple data sets, some comments on the behavior of the MDP function \eqref{eq:MDP} are warranted. The term $\frac{1}{\mA}\|\rVec(\regparam)\|_2^2$ is a monotone increasing function of $\regparam$ for $\regparam > 0$, which can be seen by differentiating the summands of \eqref{eq:Fourier regularized residual} for $R = 1$:
% \[\frac{d}{d\regparam}\left(\left(\mfilt_j\right)^2\left(\dft{d}_j\right)^2\right) = 2 \left(\dft{d}_j\right)^2 \mfilt_j\left(\frac{d}{d\regparam}\mfilt_j\right) = 2 \left(\dft{d}_j\right)^2 \mfilt_j \frac{2\regparam\bm{\lambda_j}\bm{\delta_j}}{\left(\bm{\delta}_j + \regparam^2 \bm{\lambda}_j\right)^2} \geq 0, \quad \regparam > 0.\]
% The monotonicity of $\frac{1}{\mA}\|\rVec(\regparam)\|_2^2$ does not guarantee, however, the existence of a zero of $\D(\regparam)$. If the selected value of $\noiseSD^2$ is too large, then it is possible that $\D(\regparam) < 0$ for all $\regparam > 0$ and a root will not exist. This can be attributed to the limiting behavior of $\frac{1}{\mA}\|\rVec(\regparam)\|_2^2$, which is described by Lemma \ref{lem:Residual limit}.
% \begin{lemma}
% \label{lem:Residual limit}
% \[\lim_{\regparam\rightarrow\infty} \frac{1}{\mA}\|\rVec(\regparam)\|_2^2 \leq \frac{1}{\mA}\|\dVec\|_2^2.\]
% \end{lemma}
% \begin{proof}
% Writing $\frac{1}{\mA}\|\rVec(\regparam)\|_2^2$ in terms of filter functions \eqref{eq:Filter functions},
% \[\lim_{\regparam\rightarrow\infty} \frac{1}{\mA}\|\rVec(\regparam)\|_2^2 = \lim_{\regparam\rightarrow\infty} \frac{1}{\mA} \sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\dft{d}_j\right)^2 = \frac{1}{\mA} \sum_{j=1}^{n} \left(\lim_{\regparam\rightarrow\infty}\left(\mfilt_j\right)^2\right)\left(\dft{d}_j\right)^2.\]
% Expanding the denominator of $\left(\mfilt_j\right)^2$ makes the limit clear:
% \[\lim_{\regparam\rightarrow\infty} \left(\mfilt_j\right)^2 = \lim_{\regparam\rightarrow\infty}\frac{\regparam^4\bm{\lambda}_j}{\regparam^4\bm{\lambda}_j + 2\regparam^2\bm{\lambda}_j\bm{\delta}_j + \bm{\delta}_j^2} = \begin{cases}
% 1, & \bm{\lambda}_j \neq 0 \\
% 0, & \bm{\lambda}_j = 0
% \end{cases}.\]
% Therefore
% \[\lim_{\regparam\rightarrow\infty} \frac{1}{\mA}\|\rVec(\regparam)\|_2^2 = \frac{1}{\mA} \sum_{j=1}^{n} \left(\lim_{\regparam\rightarrow\infty}\left(\mfilt_j\right)^2\right)\left(\dft{d}_j\right)^2 \leq \frac{1}{\mA} \sum_{j=1}^{n} \left(\dft{d}_j\right)^2 = \frac{1}{\mA}\|\svd{\dVec}\|_2^2 = \frac{1}{\mA}\|\dVec\|_2^2,\]
% with the last equality following from $\svd{\dVec} = \trans{U}\dVec$ with orthogonal $U$.
% \end{proof}
% \noindent Lemma \ref{lem:Residual limit} implies that if the selected value of $\noiseSD^2$ is larger than $\frac{1}{\mA}\|\dVec\|_2^2$, $\D(\regparam)$ will not have a root for $\regparam > 0$ and the MDP method fails to select a regularization parameter. Sometimes a safety parameter $\safeparam > 0$ is introduced to modify the MDP function to
% \begin{equation}
% \label{eq:MDP Safety}
% \D(\regparam) = \frac{1}{\mA}\|\rVec(\regparam)\|_2^2 - \safeparam\noiseSD^2.
% \end{equation}
% to account for root-finding difficulties \cite{ABT,IRTools}, though selecting an appropriate value of $\safeparam$ is an ad hoc process and depends on the confidence of $\noiseSD^2$ as the true noise. The original MDP function is recovered from \eqref{eq:MDP Safety} when $\safeparam = 1$. \par
