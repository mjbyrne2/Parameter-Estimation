\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,amsthm,bm,enumerate,enumitem,mathtools}
\usepackage{geometry,graphicx,subcaption,sidecap,multirow}
\geometry{margin = 1 in}
\usepackage{color}
\usepackage{hyperref}

\renewcommand{\arraystretch}{1.5}	% Change of table scale factor

% Discrete notation:
\newcommand{\mA}{m}	% Number of rows in A
\newcommand{\rA}{r_A}	% Rank of A
\newcommand{\mL}{q}	% Number of rows in L
\newcommand{\rL}{r_L}	% Rank of L
\newcommand{\aVec}{\mathbf{a}}	% Vector
\newcommand{\bVec}{\mathbf{b}}	% Vector
\newcommand{\cVec}{\mathbf{c}}	% Vector
\newcommand{\dVec}{\mathbf{d}}	% Vector
\newcommand{\kVec}{\mathbf{k}}	% Vector
\newcommand{\pVec}{\mathbf{p}}	% Vector
\newcommand{\rVec}{\mathbf{r}}	% Vector
\newcommand{\fVec}{\mathbf{f}}	% Vector
\newcommand{\xVec}{\mathbf{x}}	% Vector
\newcommand{\xTrue}{\mathbf{x}_{\text{true}}}	% Vector (true solution)
\newcommand{\yVec}{\mathbf{y}}	% Vector
\newcommand{\tVec}{\mathbf{t}}	% Vector
\newcommand{\uVec}{\mathbf{u}}	% Vector
\newcommand{\wVec}{\mathbf{w}}	% Vector
\newcommand{\trans}[1]{{#1}^\mathsf{T}}	% Matrix transpose
\newcommand{\ctrans}[1]{{#1}^\mathsf{H}}	% Hermitian transpose
\newcommand{\inv}[1]{{#1}^{-1}}	% Inverse of a matrix
\newcommand{\pinv}[1]{{#1}^\dagger}	% Inverse of a matrix
\newcommand{\partition}{\omega}  % Partition values
\newcommand{\midpoint}{\partition_{\text{mid}}}   % Midpoint partition values
\DeclareMathOperator{\trace}{trace}		% Trace
\DeclareMathOperator{\diag}{diag}	% Diagonal matrix
\DeclareMathOperator{\rank}{rank}	% Rank of a matrix
\DeclareMathOperator{\range}{range}	% Range (space) of a matrix
\DeclareMathOperator{\nullspace}{null}	% Null (space) of a matrix
\DeclareMathOperator{\proj}{proj}	% Projection
\DeclareMathOperator{\pdet}{det^\dagger}	
\DeclareMathOperator{\sgn}{sgn}	% Signum function
\DeclareMathOperator{\alias}{A}	% Aliasing operator
\newcommand{\dct}[1]{\breve{#1}}	% DCT of vector
\newcommand{\dft}[1]{\widehat{#1}}	% DFT of vector
\DeclareMathOperator{\vcat}{vcat}	% Concatenation of vector
\DeclareMathOperator{\shift}{\sigma}	% Shift operator

% Regularization notation:
\newcommand{\regparam}{\alpha}  % Regularization parameter
\newcommand{\regparamVec}{\bm{\regparam}}   % Vector of regularization parameters
\newcommand{\regparamBig}{\widetilde{\regparam}}   % Big regularization parameter
\newcommand{\regparamVecBig}{\widetilde{\regparamVec}}   % Big regularization parameter
\newcommand{\R}{R_{\regparam}}	% Regularization matrix
\newcommand{\regf}{\fVec_{\regparam}}	% Regularized solution
\newcommand{\xReg}{\xVec(\regparam)}	% Regularized solution
\newcommand{\xWin}{\xVec_{\text{win}}}	% Windowed regularized solution
\newcommand{\xSol}{\xVec}	% True solution
\newcommand{\xBig}{\widetilde{\xVec}}	% Big regularized solution
\newcommand{\xWinBig}{\xBig_{\text{win}}}	% Big windowed regularized solution
\newcommand{\rBig}{\widetilde{\rVec}}	% Big regularized residual
\newcommand{\rWinBig}{\rBig_{\text{win}}}	% Big windowed regularized residual
\newcommand{\bBig}{\widetilde{\bVec}}	% Big blurred vector
\newcommand{\dBig}{\widetilde{\dVec}}	% Big data vector
\newcommand{\ABig}{\widetilde{A}}	% Big system matrix
\newcommand{\LBig}{\widetilde{L}}	% Big penalty matrix
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% Filter function:
\newcommand{\filt}{\phi}
\newcommand{\mfilt}{\psi}

% Statistics notation:
\newcommand{\noise}{\eta}	% Noise (single component/variable)
\newcommand{\noiseSD}{\sigma}	% Standard deviation
\newcommand{\noiseVec}{\bm{\noise}}	% Noise vector
\DeclareMathOperator{\Var}{Var}	% Variance
\DeclareMathOperator{\Cov}{Cov}	% Covariance
\DeclareMathOperator{\E}{E}	% Expected value
\renewcommand{\Re}{\operatorname{Re}}	% Real part
\renewcommand{\Im}{\operatorname{Im}}	% Imaginary part
\newcommand{\NCchi}{\chi'\:}	% Noncentral chi-squared
\newcommand{\zeroVec}{\bm{0}}	% Noise (single component/variable)

% SVD notation:
\newcommand{\singular}{s}	% Singular values
\newcommand{\svd}[1]{\widehat{#1}}	% Notation for (U^T)d

% UPRE derivation notation:
\newcommand{\rReg}{\rVec(\regparam)}	% Regularized residual
\newcommand{\rWin}{\rVec_{\text{win}}}	% Windowed regularized residual
\newcommand{\A}{A(\regparam)}	% Influence matrix
\newcommand{\U}{F_{\text{UPRE}}}	% UPRE function
\newcommand{\UBig}{\widetilde{F}_{\text{UPRE}}}	% Big UPRE function
\newcommand{\UAvg}{\overline{F}_{\text{UPRE}}}	% UPRE function for averaged data
\newcommand{\UWin}{F_{\substack{\text{UPRE} \\ \text{win}}}}	% Windowed UPRE function
\newcommand{\UWinBig}{\widetilde{F}_{\substack{\text{UPRE} \\ \text{win}}}}	% Big windowed UPRE function

% GCV derivation notation:
\newcommand{\G}{F_{\text{GCV}}}	% GCV function
\newcommand{\GAvg}{\overline{F}_{\text{GCV}}}	% GCV function for averaged data
\newcommand{\GWin}{F_{\substack{\text{GCV} \\ \text{win}}}}	% Windowed GCV function
\newcommand{\GBig}{\widetilde{F}_{\text{GCV}}}	% Big GCV function
\newcommand{\GWinBig}{\widetilde{F}_{\substack{\text{GCV} \\ \text{win}}}}	% Big windowed GCV function

% Discrepancy principle derivation notation:
\newcommand{\D}{F_{\text{MDP}}}	% Discrepancy principle function
\newcommand{\safeparam}{\epsilon}	% Safety parameter for the MDP method
\newcommand{\DAvg}{\overline{F}_{\text{MDP}}}	% MDP function for averaged data
\newcommand{\DBig}{\widetilde{F}_{\text{MDP}}}	% Big GCV function
\newcommand{\DWin}{F_{\substack{\text{MDP} \\ \text{win}}}}	% Windowed MDP function
\newcommand{\DWinBig}{\widetilde{F}_{\substack{\text{MDP} \\ \text{win}}}}	% Big windowed MDP function

% Rosie's commands
\newcommand{\comment}[1]{\textcolor{red}{ \textbf{Comment}: #1}}
\newcommand{\ToDo}[1]{\textcolor{green}{\textbf{#1}}}
\newtheorem{assumption}{Assumption}
% General Lemma
\newtheorem{lemma}{Lemma}[section]
% General Theorem
\newtheorem{theorem}{Theorem}[section]
% General Corollary
\newtheorem{corollary}{Corollary}[theorem]
% General Proposition
\newtheorem{proposition}{Proposition}[section]

\title{Adapting methods for selecting Tikhonov regularization parameters for multiple data sets}
\author{Michael Byrne and Rosemary Renaut}
\date{\today}

\begin{document}

\maketitle

%\begin{abstract}
%During the inversion of discrete linear systems, noise in data can be amplified and result in worthless solutions. To combat this  effect, characteristics of solutions that are considered desirable are mathematically implemented during inversion, which is a process called regularization. The influence of desired characteristics are controlled by non-negative regularization parameters. There are a number of methods used to select appropriate regularization parameter, as well as a number of methods used for inversion. In this paper, we consider the unbiased risk estimator method, generalized cross validation method, and the discrepancy principle as the means of selecting regularization parameters. Sometime multiple data sets describing the same physical phenomenon are available. The primary contribution of this paper is a comparison of methods that incorporate multiple data sets for the selection of regularization parameters.
%\end{abstract}

\tableofcontents
\newpage

\section{Introduction} \label{sec:Introduction}

In this paper, we seek solutions of 
\begin{equation}
\label{eq:Ax = b}
A\xVec = \dVec,
\end{equation}
where $A \in \mathbb{R}^{\mA \times n}$ is ill-conditioned and $\dVec$ is known. Specifically we consider the situation where the condition number of $A$ is large and the data available to us is $\dVec = \bVec + \noiseVec$, with $\noiseVec$ being a realization of a random vector and $\bVec = A\xTrue$. Even for small systems, direct matrix inversion is not recommended due to the noise in the data and the ill-conditioning of the system matrix $A$. An alternative approach is regularization, which aims to mathematically describe desired characteristics of a solution to produce a more well-posed problem. \par
A common regularization technique is Tikhonov regularization \cite{Tikh1963}, in which the regularized solution $\xReg$ is defined as
\begin{equation}
\label{eq:TikSol2}
\xVec(\regparam) = \argmin_{\xVec \in \mathbb{R}^n} \left\{\|A\xVec - \dVec\|_2^2 + \regparam^2\|L\xVec\|_2^2\right\}, \quad \regparam > 0, ~ L \in \mathbb{R}^{\mL \times n}.
\end{equation}
The scalar $\regparam$ is the regularization parameter and $L$ is an $\mL \times n$ matrix representation of a linear operator. The term $\|L\xVec\|_2^2$ is an example of a penalty function \cite{Vogel:2002}, and $L$ is called the penalty matrix. If $L = I_n$, the $n \times n$ identity matrix, then regularization via \eqref{eq:TikSol2} is called standard or zeroth-order Tikhonov regularization \cite{ABT}. Other choices of $L$ include approximations of first and second order derivative operators that depend upon assumed boundary conditions \cite{NeumannDCT,Strang1999,Vogel:2002}. \par 
The quality of $\xReg$ depends on the choice of both $\regparam$ and $L$. There are a number of methods for selecting $\regparam$ when $L$ has been fixed. Some methods, such as the Morozov discrepancy principle (MDP) method \cite{Morozov1966}, select regularization parameters as roots of functions. Other methods, like the unbiased predictive risk estimator (UPRE) method \cite{Mallows1973} and the generalized cross validation (GCV) method \cite{Wahba1977,Wahba1990}, select regularization parameters as minimizers of functions; if the function being considered is differentiable, then the method can be re-expressed as a root-finding method. There are methods that do not fall into the categories of minimization or root-finding problems; for example, the L-curve method selects a regularization parameter as the value that locates the point of maximum curvature of a function \cite{Hansen1992,HansenOLeary}. There has also been a considerable amount of research in selecting sets of regularization parameters for a pre-selected set of penalty matrices, a process called multiparameter regularization \cite{Brezinski2003,ChungEspanol2017,GazzolaNovati2013,LuPereverzev2011,Wood2002}. Approaches to multiparameter regularization using versions of the L-curve and MDP methods can be found in \cite{BelgeKilmerMiller2002} and \cite{Wang2012}, respectively. A multiparameter GCV method was also considered in \cite{ModarresiGolub1,ModarresiGolub2}. Multiparameter regularization can also be applied through windowing, either in the data domain or the frequency domain. The process of windowing wavelet coefficients has been considered in \cite{EasleyLabatePatel,StephanakisKollias}. Examples of windowed regularization in other frequency domains, such as those generated by discrete trigonometric transforms and the SVD, have been presented in \cite{ChungEasleyOLeary,ChungKilmerOLeary,KalkeSiltanen}. There has also been recent work on learning seminorms as regularization operators \cite{Holler2020LearningNR}. \par 
In this paper, we focus on adapting the UPRE, MDP, and GCV methods to accommodate multiple data sets for both uniparameter and multiparameter regularization. We then apply these concepts towards learning parameters for use with other data sets. Techniques for utilizing and analyzing multiple data sets permeates a multitude of scientific fields, such as geoscience \cite{GeoscienceML,Zobitz2020EfficientHD} and the detection of cancers \cite{MedicineML}. Many of these techniques, including the UPRE, MDP, and GCV methods, have statistical foundations \cite{StatLearning}. A comprehensive overview of data-driven approaches to inverse problems can be found in \cite{Arridge2019SolvingIP}, while specific examples of applying multiple data sets to the solution of inverse problems include \cite{ChungChungOLeary2011,ChungEspanol2017,HaberTenorio2003,KunischPock2013,TaroudakiOLeary2015,Learning2005}. \par 
The main contribution of this paper is to demonstrate how the functions associated with the UPRE, MDP, and GCV methods can be adapted to handle multiple data sets. We will use the term ``adapted'' in the body of the paper to refer to both these generalized methods and their corresponding functions. For example, the adapted UPRE method means the extension of the UPRE method for multiple data sets. There is a distinction between the adapted methods and using the averages of the parameter selection functions. We also use the adapted methods to learn parameters that can be used in regularizing other data sets. The development of the adapted methods is done in Section \ref{sec:Methods} and numerical results are shown in Section \ref{sec:Validation}. Sections \ref{sec:UPRE}, \ref{sec:MDP}, and \ref{sec:GCV} pertain to the UPRE, method, MDP method, and the GCV method, respectively.  \par 
In addition to developing these adapted parameter selection methods, some results will be presented regarding their relationship with the original method (i.e. the non-adapted methods). These results are presented at the end of each subsection in Section \ref{sec:Methods}. Comparisons are also made to other regularization parameter selection methods that use multiple data sets; see Section \ref{sec:Validation}. \par
Lastly, this paper introduces how multiple data sets can be used in conjunction with multiparameter regularization. Multiparameter versions of the GCV method are presented in \cite{ChungEasleyOLeary,ModarresiGolub2}; we extend these formulations to multiparameter versions of the UPRE and MDP methods. Results, summaries and directions for future work are considered in Section \ref{sec:Conclusion}. The same formulations presented in this paper could also be applied to other standard methods such as the L-curve method. We focus on the UPRE, MDP, and GCV methods to illustrate the general approach.

\section{Preliminaries} \label{sec:Preliminaries}

Even for problems resulting in large systems, the singular value decomposition (SVD) is a useful tool for analyzing methods of solving \eqref{eq:Ax = b}.  Assuming $A$ is a real $\mA \times n$ matrix, the SVD of $A$ is
\begin{equation}
\label{eq:SVD}
A = US\trans{V}
\end{equation}
where the $\mA \times \mA$ matrix $U$ and the $n \times n$ matrix $V$ are orthogonal and $S$ is a $\mA \times n$ diagonal matrix. The diagonal elements of $S$ are the singular values of $A$, denoted $\singular_j$ and satisfying $\singular_1 \geq \singular_2 \geq \ldots \geq \singular_{\rA} \geq 0$ with $\rA \leq \min\{\mA,n\}$. The columns of $U$ and $V$ will be denoted $U_{\cdot,j}$ and $V_{\cdot,j}$, respectively, and are known as the left and right singular vectors of $A$. If $A$ is square ($\mA = n$) and invertible, then $\inv{A} = V\inv{S}\trans{U}$ and the solution $\inv{A}\dVec$ can be written as
\begin{equation}
\label{eq:InvProd}
\inv{A}\dVec = VS^{-1}{\trans{U}}\dVec = \sum_{j=1}^{n} \frac{{\trans{(U_{\cdot,j})}}\dVec}{\singular_j}V_{\cdot,j} = \sum_{j=1}^{n} \frac{\svd{\dVec}_j}{\singular_j}V_{\cdot,j}
\end{equation}
where $\svd{\dVec} = \trans{U}\dVec$. \par 
Beyond the situation where $A$ is square and invertible, the pseudoinverse $\pinv{A} = V\pinv{S}\trans{U}$, where $\pinv{S} \in \mathbb{R}^{n \times \mA}$ is formed by reciprocating the nonzero diagonal elements of $\trans{S}$. The pseudoinverse, also called the Moore-Penrose generalized inverse \cite{Penrose1955}, is the unique matrix $\pinv{A} \in \mathbb{R}^{n \times \mA}$ that satisfies the following Moore-Penrose conditions \cite[p.~290]{GolubVanLoan2013}:
\begin{enumerate}[label={(\Roman*)}]
    \begin{minipage}{0.45\linewidth}
        \centering
        \item $A\pinv{A}A = A$, \label{MP Condition 1}
        \item $\pinv{A}A\pinv{A} = \pinv{A}$, \label{MP Condition 2}
    \end{minipage}
    \begin{minipage}{0.45\linewidth}
        \centering
        \item $\trans{\left(A\pinv{A}\right)} = A\pinv{A}$, \label{MP Condition 3}
        \item $\trans{\left(\pinv{A}A\right)} = \pinv{A}A$. \label{MP Condition 4}
    \end{minipage}
\end{enumerate}
Proposition \ref{prop:Special pseudoinverses} describes some special cases of the pseudoinverse.
\begin{proposition}
\label{prop:Special pseudoinverses}
Let $A \in \mathbb{R}^{\mA \times n}$ and let $\pinv{A} \in \mathbb{R}^{n \times \mA}$ be the pseudoinverse of $A$.
    \begin{enumerate}[label={(\roman*)}]
    \item If $\rank(A) = \mA$, then $\pinv{A} = \trans{A}\inv{\left(A\trans{A}\right)}$ and so $\pinv{A}$ is the right inverse of $A$. \label{prop:Right inverse}
    \item If $\rank(A) = n$, then $\pinv{A} = \inv{\left(\trans{A}A\right)}\trans{A}$ and so $\pinv{A}$ is the left inverse of $A$. \label{prop:Left inverse}
    \item If $A = \zeroVec_{\mA \times n}$, then $\pinv{A} = \zeroVec_{n \times \mA}$. \label{prop:Zero Pseudoinverse}
    \item If $\rank(A) = \mA = n$, then $\pinv{A} = \inv{A}$. \label{prop:Full inverse}
    \end{enumerate}
\end{proposition}
\begin{proof}
We prove part \ref{prop:Right inverse}. The proofs of parts \ref{prop:Left inverse} and \ref{prop:Zero Pseudoinverse} are analogous to that of part \ref{prop:Right inverse}, and part \ref{prop:Full inverse} follows from the definition of $\pinv{S}$ using $A = US\trans{V}$.
\begin{enumerate}
    \item[\ref{prop:Right inverse}] Since $A$ has full row rank, $A\trans{A}$ is invertible. Let $B = \trans{A}\inv{\left(A\trans{A}\right)}$. The Moore-Penrose conditions are satisfied as follows:
    \begin{enumerate}
        \item[\ref{MP Condition 1}] $ABA = \left(A\trans{A}\right)\inv{\left(A\trans{A}\right)}A = 
        A$,
        \item[\ref{MP Condition 2}] $BAB = \trans{A}\inv{\left(A\trans{A}\right)}\left(A\trans{A}\right)\inv{\left(A\trans{A}\right)} = \trans{A}\inv{\left(A\trans{A}\right)} = B$,
        \item[\ref{MP Condition 3}] $\trans{\left(AB\right)} = \trans{\left(\left(A\trans{A}\right)\inv{\left(A\trans{A}\right)}\right)} = I_{m \times m} = \left(A\trans{A}\right)\inv{\left(A\trans{A}\right)} = AB$,
        \item[\ref{MP Condition 4}] $\trans{\left(BA\right)} = \trans{\left(\trans{A}\inv{\left(A\trans{A}\right)}A\right)} = \trans{A}\inv{\left(A\trans{A}\right)}A = BA$.
    \end{enumerate}
\end{enumerate}
As a final remark regarding part \ref{prop:Zero Pseudoinverse} of Proposition \ref{prop:Special pseudoinverses}, any matrix other than $\zeroVec_{n \times \mA}$ would fail to satisfy \ref{MP Condition 2} of the Moore-Penrose conditions.
\end{proof}

\noindent The pseudoinverse can also be used with orthogonal projections onto fundamental subspaces; some applications are described in Proposition \ref{prop:Subspace projections}.

\begin{proposition}
\label{prop:Subspace projections}
Let $A \in \mathbb{R}^{\mA \times n}$, $P \in \mathbb{R}^{n \times n}$ be an orthogonal projection matrix, $\pinv{A} \in \mathbb{R}^{n \times m}$ be the pseudoinverse of $A$, and let $\rA = \rank(A)$.
\begin{enumerate}[label={(\roman*)}]
    \item $A\pinv{A}$ is the orthogonal projection onto $\range(A)$. \label{prop:Range projector}
    \item $\pinv{A}A$ is the orthogonal projection onto $\range\left(\trans{A}\right)$. \label{prop:Transpose range projector}
    \item $\pinv{P} = P$. \label{prop:Projection pseudoinverse}
    \item $P\pinv{\left(AP\right)} = \pinv{\left(AP\right)}$. \label{prop:Left projection}
    \item $\pinv{\left(PA\right)}P = \pinv{\left(PA\right)}$. \label{prop:Right projection}
\end{enumerate}
\end{proposition}
\begin{proof}
We prove parts \ref{prop:Range projector} and \ref{prop:Left projection}. The proof of part \ref{prop:Projection pseudoinverse} follows immediately from the Moore-Penrose conditions and the properties of orthogonal projections. The proof of parts \ref{prop:Transpose range projector} and \ref{prop:Right projection} are similar to that of parts \ref{prop:Range projector} and \ref{prop:Left projection}, respectively. 
\begin{enumerate}
    \item[\ref{prop:Range projector}] Let $B = A\pinv{A}$.  From \cite[p.~82]{GolubVanLoan2013}, we must show that $B$ is idempotent ($B^2 = B$), symmetric, and that $\range(B) = \range(A)$. We can use the SVD $A = US\trans{V}$ to demonstrate all three properties. Condition \ref{MP Condition 2} of the Moore-Penrose conditions gives the idempotence of $B$:
    \[B^2 = \left(A\pinv{A}\right)\left(A\pinv{A}\right) = A\left(\pinv{A}A\pinv{A}\right) = A\pinv{A} = B.\]
    For symmetry, we have that
    \[\trans{B} = \trans{\left(A\pinv{A}\right)} = \trans{\left(US\pinv{S}\trans{U}\right)} = U\trans{\left(S\pinv{S}\right)}\trans{U}.\]
    By the definition of $\pinv{S}$, the matrix $S\pinv{S} \in \mathbb{R}^{\mA \times \mA}$ is
    \[S\pinv{S} = \begin{bmatrix}
    I_{\rA} & \zeroVec \\
    \zeroVec & \zeroVec
    \end{bmatrix}.\]
    Thus $S\pinv{S}$ is symmetric and 
    \[\trans{B} = U\trans{\left(S\pinv{S}\right)}\trans{U} = US\pinv{S}\trans{U} = B,\]
    showing that $B$ is symmetric. If we carry out the multiplication $B = US\pinv{S}\trans{U}$, then
    \[B = U_{\rA}\trans{U}_{\rA}\]
    where $U_{\rA}$ consists of the first $\rA$ columns of $U$. Since the first $\rA$ columns of $U$ form an orthonormal basis for $\range(A)$ \cite[p.~340]{Leon2010}, $B$ being equal to $ U_{\rA}\trans{U}_{\rA}$ shows that $\range(B) = \range(A)$. Therefore, $B$ is the orthogonal projection onto $\range(A)$.
    \item[\ref{prop:Left projection}] Let $B = AP$ and $C = P\pinv{\left(AP\right)}$. It will be shown that $C$ satisfies the Moore-Penrose conditions.
    \begin{enumerate}
        \item[\ref{MP Condition 1}] Substituting $B$ and $C$ into the product $BCB$, we have 
        \[BCB = APP\pinv{\left(AP\right)}AP = AP\pinv{\left(AP\right)}AP.\] 
        By definition, $\pinv{\left(AP\right)}$ satisfies Moore-Penrose condition \ref{MP Condition 1}; thus $AP\pinv{\left(AP\right)}AP = AP = B$.
        \item[\ref{MP Condition 2}] Checking the product $CBC$, we have 
        \[CBC = P\pinv{\left(AP\right)}APP\pinv{\left(AP\right)} = P\pinv{\left(AP\right)}AP\pinv{\left(AP\right)}.\]
        $\pinv{\left(AP\right)}$ satisfies Moore-Penrose condition \ref{MP Condition 2}, and so
        \[P\left(\pinv{\left(AP\right)}AP\pinv{\left(AP\right)}\right) = P\pinv{\left(AP\right)} = C.\]
        \item[\ref{MP Condition 3}] First, we have that \[\trans{\left(BC\right)} = \trans{\left(APP\pinv{\left(AP\right)}\right)} = \trans{\left(AP\pinv{\left(AP\right)}\right)} = AP\pinv{\left(AP\right)},\]
        where the last equality results from the symmetry of $AP\pinv{\left(AP\right)}$. Writing $P = P^2$, we have
        \[\trans{\left(BC\right)} = AP^2\pinv{\left(AP\right)} = \left(AP\right)P\pinv{\left(AP\right)} = BC.\]
        \item[\ref{MP Condition 4}] We begin with
        \[\trans{\left(CB\right)} = \trans{\left(P\pinv{\left(AP\right)}AP\right)} = P\trans{A}\pinv{\left(P\trans{A}\right)}P.\]
        Writing the leading $P$ as $P = P^2$ and noting that $P\trans{A}\pinv{\left(P\trans{A}\right)}$ is symmetric, we have
        \[P\trans{A}\pinv{\left(P\trans{A}\right)}P = P\trans{\left(P\trans{A}\pinv{\left(P\trans{A}\right)}\right)}P = P\pinv{\left(AP\right)}AP^2 = P\pinv{\left(AP\right)}AP = CB.\]
    \end{enumerate}
    Therefore $C = P\pinv{\left(AP\right)}$ satisfies the Moore-Penrose conditions, meaning that $P\pinv{\left(AP\right)}$ is the pseudoinverse of $B = AP$.
\end{enumerate}
\end{proof}

When working with the pseudoinverse $\pinv{A}$, it can be convenient to write
\begin{equation}
\label{eq:Compact SVD}
A = U_{\rA}S_{\rA}\trans{V}_{\rA}
\end{equation}
where $U_{\rA} \in \mathbb{R}^{\mA \times \rA}$ and $V_{\rA} \in \mathbb{R}^{n \times \rA}$ consist of the first $\rA$ columns of $U$ and $V$, respectively, and $S_{\rA} = \diag(\singular_1,\ldots,\singular_{\rA}) \in \mathbb{R}^{\rA \times \rA}$ is invertible. Factorization \eqref{eq:Compact SVD} is called the compact SVD of $A$ \cite{ABT,Leon2010}. As mentioned in the proof of Proposition \ref{prop:Special pseudoinverses}, the columns of $U_{\rA}$ form an orthonormal basis for $\range(A)$. Similarly, the columns of $V_{\rA}$ form an orthonormal basis for $\range(\trans{A})$. Writing $U = [U_{\rA} ~ U_0]$ and $V = [V_{\rA} ~ V_0]$, the fundamental theorem of linear algebra \cite{Strang1993} implies that the columns $U_0$  and $V_0$ form orthonormal bases for $\nullspace(\trans{A})$ and $\nullspace(A)$, respectively. With the compact SVD, the pseudoinverse of $A$ can be expressed as 
\begin{equation}
\label{eq:Pseudoinverse}
    \pinv{A} = V_{\rA}\inv{S}_{\rA}\trans{U}_{\rA}.
\end{equation}

Let $\pinv{\xVec} = \pinv{A}\dVec = V\pinv{S}\svd{\dVec}$ denote the solution of $A\xVec = \dVec$ obtained using the pseudoinverse $\pinv{A}$. The summation representation of $\pinv{\xVec}$ is the same as \eqref{eq:InvProd} except that $\rA$ is the upper limit of summation. Table \ref{tab:Cases for A} summarizes the situations involving $\pinv{\xVec}$ as a solution to $A\xVec = \dVec$. In the case where $\nullspace(A) \neq \{\zeroVec\}$, infinitely many solutions to $A\xVec = \dVec$ can be constructed as $\pinv{\xVec} + \xVec_0$ where $\xVec_0 \in \nullspace(A)$ is arbitrary.

\begin{table}[ht!]
  \begin{center}
    \caption{Characteristics for using $\pinv{\xVec} = \pinv{A}\dVec$ as a solution to $A\xVec = \dVec$.}
    \label{tab:Cases for A}
    \begin{tabular}{|c|c|c|c|}
    \hline 
      \multirow{2}{*}{$A$} & $\mA < n$ & \multirow{2}{*}{$\mA = n$} & $\mA > n$ \\ 
       & (Underdetermined system) & & (Overdetermined system) \\ \hline
       \multirow{4}{*}{Full rank} & $\dim(\nullspace(A)) = n - \rA$ & $\nullspace(A) = \{\zeroVec\}$  & $\nullspace(A) = \{\zeroVec\}$ \\ 
       & $\nullspace(\trans{A}) = \{\zeroVec\}$ & $\nullspace(\trans{A}) = \{\zeroVec\}$ & $\dim(\nullspace(\trans{A})) = \mA - \rA$ \\
       & $\pinv{\xVec}$ not unique & $\pinv{\xVec}$ unique & $\pinv{\xVec}$ unique \\
       & $A\pinv{\xVec} = \dVec$ & $A\pinv{\xVec} = \dVec$ & $A\pinv{\xVec} = \proj_{\range(A)}\dVec$ \\ \hline
      \multirow{4}{*}{Rank-deficient} &  \multicolumn{3}{c|}{$\dim(\nullspace(A)) = n - \rA$}  \\ 
      & \multicolumn{3}{c|}{$\dim(\nullspace(\trans{A})) = \mA - \rA$} \\
      & \multicolumn{3}{c|}{$\pinv{\xVec}$ not unique} \\
      & \multicolumn{3}{c|}{$A\pinv{\xVec} = \proj_{\range(A)}\dVec$} \\ \hline
    \end{tabular}
  \end{center}
\end{table}

In any case for $A$, the summands in \eqref{eq:InvProd} are numerically unstable for small $\singular_j$ when the terms $\left|\svd{\dVec}_j\right|$ do not decay as fast as the $\singular_j$; this describes the discrete Picard condition \cite{Hansen:98}. For an ill-conditioned matrix $A$, forming \eqref{eq:InvProd} will often result in a meaningless solution. A common approach to overcome numerical instabilities is to multiply the summands in \eqref{eq:InvProd} by filter functions $\filt$ that depend upon $\singular_j$ and a non-negative regularization parameter $\regparam$, a process called regularization. By doing so, an approximate solution is
\begin{equation}
\label{eq:ApproxSol}
\xVec(\regparam) := \sum_{j=1}^{\rA} \filt(\regparam,\singular_j) \frac{\svd{\dVec}_j}{\singular_j}V_{\cdot,j}  = V\Phi{S}^\dagger\svd{\dVec},
\end{equation}
where the matrix $\Phi$ is diagonal with $\Phi_{j,j} = \filt(\regparam,\singular_j)$ for $j = 1,\ldots,{n}$. The most desired property of the filter functions is that $\filt(\singular_j)/\singular_j \approx 1$  for large values of $\singular_j$ and $\filt(\singular_j)/\singular_j \approx 0$ for small values of $\singular_j$. A specific example of a filter function is
\begin{equation}
\label{eq:TikFilt}
\filt(\regparam,\singular_j)  = \frac{\singular_j^2}{\singular_j^2 + \regparam^2}
\end{equation}
which is known as the standard Tikhonov filter function. The use of the Tikhonov filter function to generate an approximate solution is known as Tikhonov regularization \cite{Tikh1963}; in terms of an SVD, the obtained solution is
\begin{equation}
\label{eq:TikSol}
\xVec(\regparam) = \sum_{j = 1}^{\rA} \frac{\singular_j \svd{\dVec}_j}{\singular_j^2 + \regparam^2}V_{\cdot,j} = \sum_{j = 1}^{\rA} \filt(\regparam,\singular_j)\frac{\svd{\dVec}_j}{\singular_j}V_{\cdot,j}.
\end{equation}
An alternative representation of the above Tikhonov solution is
\begin{equation}
\label{eq:Damped LS}
\xVec(\regparam) = \argmin_{\xVec \in \mathbb{R}^n} \left\{\|A\xVec - \dVec\|_2^2 + \regparam^2\|\xVec\|_2^2\right\},
\end{equation}
which is a solution to a damped least squares problem \cite{ABT}. The solutions \eqref{eq:TikSol} and \eqref{eq:Damped LS} are solutions to the ordinary least squares problem
\begin{equation}
\label{eq:Ordinary LS}
\min_{\xVec \in \mathbb{R}^n} \left\{\left\|
\begin{bmatrix}
A \\
\regparam I_n
\end{bmatrix}\xVec - 
\begin{bmatrix}
\dVec \\
\zeroVec
\end{bmatrix}
\right\|_2^2\right\},
\end{equation}
where the system matrix has full rank for all $\regparam > 0$. Using the SVD of $A$, the normal equations 
\[(\trans{A}A + \regparam^2 I_n)\xVec = \trans{A}\dVec\]
corresponding to \eqref{eq:Ordinary LS} simplify to
\[(\trans{S}S + \regparam^2 I_n)\trans{V}\xVec = \trans{S}\svd{\dVec}.\]
Representation \eqref{eq:TikSol} is then obtained by noting that $(\trans{S}S + \regparam^2 I_n)$ is a diagonal matrix that is guaranteed to be non-singular for $\regparam > 0$. If $A$ has full column rank and $\mA \geq n$, then $\trans{S}S + \regparam^2 I_n$ is non-singular even when $\regparam = 0$. \par
Representation \eqref{eq:Damped LS} follows from selecting $L$ in \eqref{eq:TikSol2} to be $I_n$. Analogous to \eqref{eq:Ordinary LS}, the solution \eqref{eq:TikSol2} can be expressed in block form as
\begin{equation}
\xVec(\regparam) = \argmin_{\xVec \in \mathbb{R}^n} \left\{\left\| \begin{bmatrix}
A \\
\regparam L
\end{bmatrix}\xVec - \begin{bmatrix}
\dVec \\
\bm{0}
\end{bmatrix} \right\|_2^2\right\}.
\label{eq:TikSol3}
\end{equation}
If the matrix $L$ in \eqref{eq:TikSol2} is non-singular, then the substitutions $\mathbf{y} = L\xVec$, $B = A{L}^{-1}$, and $\cVec = \dVec$ give
\begin{equation}
\mathbf{y}(\regparam) = \argmin_{\mathbf{y} \in \mathbb{R}^n} \left\{\|B\mathbf{y} - \cVec\|_2^2 + \regparam^2\|\mathbf{y}\|_2^2\right\}.
\label{eq:TikSol Standard Form}
\end{equation}
This is known as the standard form of the regularization problem. Once $\mathbf{y}$ is obtained from \eqref{eq:TikSol Standard Form}, the final solution is recovered by $\xVec = L^{-1}\mathbf{y}$.  However, there are many examples of matrices $L$ that are singular, such as finite difference matrices that approximate derivative operators. In such cases, \eqref{eq:TikSol2} can still be transformed into the standard form \eqref{eq:TikSol Standard Form}; this transformation was originally given by Eld\'{e}n \cite{Elden} and is discussed in \cite{Hansen:98}. We begin by introducing an $A$-weighted generalized inverse of $L$:
\begin{equation}
\label{eq:A-weighted inverse of L}
    \pinv{L}_A := \left(I - \pinv{\left(A\left(I - \pinv{L}L\right)\right)}A\right)\pinv{L}, \quad I \in \mathbb{R}^{n \times n}.
\end{equation}
The definition of $\pinv{L}_A$ is somewhat natural, as evidenced by Proposition \ref{prop:Pseudoinverse of L}.

\begin{proposition}
\label{prop:Pseudoinverse of L}
Given $A \in \mathbb{R}^{\mA \times n}$, $L \in \mathbb{R}^{\mL \times n}$, and $\pinv{L}_A$ defined by \eqref{eq:A-weighted inverse of L}, if $\mL \geq n$ and $\rank(L) = n$ then $\pinv{L}_A = \pinv{L}$.
\end{proposition}
\begin{proof}
Since $\rank(L) = n$, part \ref{prop:Left inverse} of Proposition \ref{prop:Special pseudoinverses} gives $\pinv{L} = \inv{\left(\trans{L}L\right)}\trans{L}$. Thus $\pinv{L}L = I_n$, and so
\[\pinv{L}_A = \left(I_n - \pinv{\left(A\left(I_n - I_n\right)\right)}A\right)\pinv{L} = \left(I_n - \pinv{\left(\zeroVec_{\mA \times n}\right)}A\right)\pinv{L}.\]
$\pinv{\left(\zeroVec_{\mA \times n}\right)} = \zeroVec_{n \times \mA}$ from part \ref{prop:Zero Pseudoinverse} of Proposition \ref{prop:Special pseudoinverses}, and therefore
\[\pinv{L}_A = \left(I_n - \zeroVec_{n \times \mA}A\right)\pinv{L} = \left(I_n - \zeroVec_{n \times n}\right)\pinv{L} = \pinv{L}.\]
\end{proof}
\noindent By letting $\xVec_0 = \pinv{\left(A\left(I_n - \pinv{L}L\right)\right)}\dVec$, we can define the substitutions
\begin{equation}
\label{eq:Standard Form Substitutions}
    \yVec = L\xVec, \quad B = A\pinv{L}_A, \quad \cVec = \dVec - A\xVec_0.
\end{equation}
These substitutions transform \eqref{eq:TikSol2} into the standard form \eqref{eq:TikSol Standard Form}, while the solution $\xVec$ can be obtained from $\xVec = \pinv{L}_A \yVec + \xVec_0$. The transformation proceeds by the simultaneous addition and subtraction of $A\xVec_0$ as follows
\[A\xVec - \dVec = A\xVec - A\xVec_0 - \dVec + A\xVec_0 = A\left(\xVec - \xVec_0\right) - \left(\dVec - A\xVec_0\right) = A\pinv{L}_A \yVec - \cVec = B\yVec - \cVec.\]
Proposition \ref{prop:Nullspace of L} shows that $\xVec_0$ indeed lies within the nullspace of $L$.

\begin{proposition}
\label{prop:Nullspace of L}
Given $A \in \mathbb{R}^{\mA \times n}$, $L \in \mathbb{R}^{\mL \times n}$, and $\dVec \in \mathbb{R}^{\mA}$, let
\[\xVec_0 = \pinv{\left(A\left(I - \pinv{L}L\right)\right)}\dVec.\]
Then $\xVec_0 \in \nullspace(L)$.
\end{proposition}
\begin{proof}
Let $B = I_n - \pinv{L}L$. Part \ref{prop:Transpose range projector} of Proposition \ref{prop:Subspace projections} states that $\pinv{L}L$ is the orthogonal projection onto $\range(\trans{L})$. Thus $B$ is the orthogonal projection onto $\nullspace(L)$. From part \ref{prop:Left projection} of Proposition \ref{prop:Subspace projections}, we can write $L\xVec_0$ as $L\xVec_0 = L\pinv{\left(AB\right)}\dVec = LB\pinv{\left(AB\right)}\dVec$. Substituting back $B = I_n - \pinv{L}L$ into the product $LB$, we have
\[LB = L\left(I_n - \pinv{L}L\right) = L - L\pinv{L}L.\]
$\pinv{L}$ satisfies Moore-Penrose condition \ref{MP Condition 1}, meaning $L - L\pinv{L}L = L - L = \zeroVec$ where $\zeroVec \in \mathbb{R}^{\mL \times n}$. Therefore
\[L\xVec_0 = LB\pinv{\left(AB\right)}\dVec = \zeroVec\pinv{\left(AB\right)}\dVec = \zeroVec \in \mathbb{R}^{\mL},\]
and so $\xVec_0 \in \nullspace(L)$.
\end{proof}

\subsection{Generalized Singular Value Decomposition} \label{sec:GSVD}
In light of having to consider two (often very different) matrices $A$ and $L$ in formulations such as \eqref{eq:TikSol2} and \eqref{eq:TikSol3}, a brief discussion of the GSVD is worthwhile. The discussion closely follows the presentation of the GSVD in \cite{ABT}, which uses the assumption that $\nullspace(A) \cap \nullspace(L) = \{\zeroVec\}$. This condition means that the block system matrix in \eqref{eq:TikSol3} has full column rank for the sake of a unique regularized solution $\xVec(\regparam)$. Given real matrices $A$ and $L$ of size $\mA \times n$ and $\mL \times n$, respectively, the decompositions
\begin{equation}
\label{eq:GSVD}
A = U\Delta\trans{X}, \quad L = V\Lambda\trans{X}
\end{equation}
exist where $U$ is an $\mA \times \mA$ orthogonal matrix, $V$ is a $\mL \times \mL$ orthogonal matrix, $X$ is an $n \times n$ non-singular matrix, and $\Lambda$ is a $\mL \times n$ diagonal matrix with diagonal elements
\[\Lambda_{1,1} \geq \Lambda_{2,2} \geq \ldots \geq \Lambda_{\mL,\mL} \geq 0.\]
The only elements of the $\mA \times n$ matrix $\Delta$ that are possibly non-zero are
\begin{equation}
\label{eq:k-diagonal}
 0 \leq \Delta_{1,k+1} \leq \Delta_{2,k+2} \leq \ldots \leq \Delta_{\mA,k+\mA} \leq 1, \quad k = \max \{0,n - \mA\}.
\end{equation}
One convenient property of the GSVD is that $\trans{\Delta}\Delta + \trans{\Lambda}\Lambda = I_n$ \cite[p.~104]{ABT}. The actual structure of $\Delta$ depends upon the dimension of $A$. If $\mA \geq n$, then $\Delta$ is a diagonal matrix. Let a $k$-diagonal matrix be defined as an $\mA \times n$ matrix with nonzero entries given by \eqref{eq:k-diagonal} when $\mA < n$ (which implies $k \neq 0$). In other words, a $k$-diagonal matrix is a rectangular matrix where the only entries that are possibly nonzero are located on the $k$th upper diagonal. With this definition, $\Delta$ is a $k$-diagonal matrix if $\mA < n$. \par 
An application of the decompositions \eqref{eq:GSVD} is that the normal equations 
\[(\trans{A}A + \regparam^2 \trans{L}L)\xVec(\regparam) = \trans{A}\dVec\] corresponding to \eqref{eq:TikSol3} can be simplified as 
\[(X\trans{\Delta}\Delta\trans{X} + \regparam^2 X\trans{\Lambda}\Lambda\trans{X})\xVec(\regparam) = X\trans{\Delta}\trans{U}\dVec,\]
which gives 
\begin{equation}
    \label{eq:GSVD Normal Eq}
    (\trans{\Delta}\Delta + \regparam^2 \trans{\Lambda}\Lambda)\trans{X}\xVec(\regparam) = \trans{\Delta}\svd{\dVec}
\end{equation}
due to the invertibility of $X$. \par
Additionally, the assumption that the block system matrix in \eqref{eq:TikSol3} has full column rank means that $\trans{\Delta}\Delta + \regparam^2 \trans{\Lambda}\Lambda$ is non-singular for $\regparam > 0$. In such a case, equation \eqref{eq:GSVD Normal Eq} can be solved as
\begin{equation}
    \label{eq:GSVD Normal Eq Sol}
    \xVec(\regparam) = Y\inv{\left(\trans{\Delta}\Delta + \regparam^2 \trans{\Lambda}\Lambda\right)}\trans{\Delta}\svd{\dVec},
\end{equation}
where $Y$ is the inverse of $\trans{X}$. A representation of the regularized solution similar to \eqref{eq:TikSol} can also be obtained:
\begin{equation}
\label{eq:GSVD Solution 1}
\xVec(\regparam) = \sum_{j = 1}^{n} \frac{\delta_j \svd{\dVec}_{j+k}}{\delta^2_j + \regparam^2 \lambda^2_j}Y_{\cdot,j}
\end{equation}
with $\bm{\delta} = \sqrt{\diag(\trans{\Delta}\Delta)}$ and $\bm{\lambda} = \sqrt{\diag(\trans{\Lambda}\Lambda)}$ (the square roots being applied element-wise).  A filter function $\filt_j$ and complement $\mfilt_j$ similar to \eqref{eq:TikFilt} can be introduced to further simplify notation:
\begin{equation}
\label{eq:Filter functions}
\filt_j\left(\regparam,\frac{\delta_j}{\lambda_j}\right) = \frac{\delta^2_j}{\delta^2_j + \regparam^2 \lambda^2_j}, \quad \mfilt_j\left(\regparam,\frac{\delta_j}{\lambda_j}\right) = 1 - \filt_j\left(\regparam,\frac{\delta_j}{\lambda_j}\right) = \frac{\regparam^2 \lambda^2_j}{\delta^2_j + \regparam^2 \lambda^2_j}.
\end{equation}
Using $\filt_j$, the solution \eqref{eq:GSVD Solution 1} can be written as
\begin{equation}
\label{eq:GSVD Solution 2}
\xVec(\regparam) = \sum_{j = 1}^{n} \frac{\delta^2_j}{\delta^2_j + \regparam^2 \lambda^2_j} \frac{\svd{\dVec}_{j+k}}{\delta_j} Y_{\cdot,j} = \sum_{j = 1}^{n} \filt_j\left(\regparam,\frac{\delta_j}{\lambda_j}\right) \frac{\svd{\dVec}_{j+k}}{\delta_j} Y_{\cdot,j}.
\end{equation} 
In terms of the generalized singular values $\gamma_j = \delta_j/\lambda_j$ for $j = 1,\ldots,n$, the solution \eqref{eq:GSVD Solution 1} can also be written as
\begin{equation}
\label{eq:GSVD Solution 3}
\xVec(\regparam) = \sum_{j = 1}^{n} \filt_j\left(\regparam,\gamma_j\right) \frac{\svd{\dVec}_{j+k}}{\delta_j} Y_{\cdot,j},
\end{equation}
which generalizes \eqref{eq:TikSol}. As a final representation and a return to matrix form, we can let $\Phi(\regparam) \in \mathbb{R}^{n \times n}$ diagonal matrix with
\[\left[\Phi(\regparam)\right]_{j,j} = \filt\left(\regparam,\gamma_j\right), \quad j = 1,\ldots,n.\]
Using $\Phi(\regparam)$, we can express representation \eqref{eq:GSVD Solution 3} more compactly as
\[\xVec(\regparam) = Y\Phi(\regparam)\pinv{\Delta}\svd{\dVec}.\]

As noted in \cite{ABT}, there are situations in which some generalized singular values $\gamma_j$ are arbitrarily large or infinite. In such cases, the corresponding filter factors $\filt_j$ are set to 1. Similarly, if $\gamma_j = 0$ or is arbitrarily small, then the corresponding summands in \eqref{eq:GSVD Solution 3} are counted as zero. Lastly, the generalized singular values are ordered such that $\gamma_1 \leq \ldots \leq \gamma_n$, which is due to the order of the diagonal elements of $\diag(\trans{\Delta}\Delta)$ and $\diag(\trans{\Lambda}\Lambda)$. In contrast, the singular values associated with the standard SVD are arranged in descending order.

\subsection{Multiparameter Tikhonov regularization} \label{sec:Multiparameter}
We follow the approach in \cite{ChungEasleyOLeary} by defining $P$ vectors $\wVec^{(p)} \in \mathbb{R}^n$ that contain non-negative weights which satisfy
\begin{equation}
\label{eq:Weights}
\sum_{p=1}^{P} \wVec_j^{(p)} = 1, \quad j = 1,\ldots,n.
\end{equation}
Defining windows $W^{(p)} = \diag\left(\wVec^{(p)}\right)$ for $p = 1,\ldots,P$, we have that that $\sum_{p=1}^P W^{(p)} = I_n$. A regularization parameter $\regparam^{(p)}$ can be selected for each window $W^{(p)}$ so that a windowed regularized solution can be constructed as
\begin{equation}
\label{eq:TikSolWindow}
\xWin(\regparamVec) = \sum_{p=1}^P V\inv{\left[\trans{S}S + (\regparam^{(p)})^2\right]}W^{(p)}\trans{S}\dft{\dVec}.
\end{equation}
The windows must be selected before choosing corresponding regularization parameters; as such, some types of windows will be discussed. \par
A specific class of windows are windows that do not overlap. Windows $W^{(p)}$ are defined to be non-overlapping if the components of their corresponding weight vectors $\wVec^{(p)}$ satisfy
\begin{equation}
\label{eq:Non-overlapping window condition}
    \wVec_j^{(p)} \in \{0,1\}, \qquad j = 1,\ldots,n, \quad p = 1,\ldots,P.
\end{equation}
In words, condition \eqref{eq:Non-overlapping window condition} means that for each $j = 1,\ldots,n$, there is exactly one $p \in \{1,\ldots,P\}$ such that $\wVec_j^{(p)} = 1$. When working with non-overlapping windows, the pigeonhole principle \cite{DummitFoote3} can be used to show that there will exist indices $p$ such that $\wVec^{(p)} = \zeroVec$ if $P > n$. \par
Perhaps the simplest way of choosing the components of $\wVec^{(p)}$ is to first choose $P+1$ values $\omega^{(0)} \geq \ldots \geq \omega^{(P)}$ such that $\omega^{(0)} \geq \singular_1$ and $\singular_n > \omega^{(P)}$. For $p = 1,\ldots,P$, we could then set
\begin{equation}
\label{eq:Non-overlapping windows}
\wVec_j^{(p)} = \begin{cases}
1, & \omega^{(p-1)} \geq \singular_j > \omega^{(p)} \\
0, & \text{otherwise.}
\end{cases}
\end{equation}
There are some advantages of using \eqref{eq:Non-overlapping windows}. One advantage is that singular values of similar magnitude are grouped together. Another advantage is that the windows do not overlap. Non-overlapping windows allows for the multiparameter estimation functions discussed in Section \ref{sec:Methods} to decoupled into linear combinations of single parameter functions. Choosing $\partition^{(1)},\ldots,\partition^{(P-1)}$ to be the $P-1$ linearly-spaced or logarithmically-spaced points between $\singular_1$ and $\singular_n$ and then setting $\partition^{(0)} = \singular_1$ and $\partition^{(P)} < \singular_n$ is an example of how to use \eqref{eq:Non-overlapping windows}. \par 
Partition values $\partition^{(0)} \geq \ldots \geq \partition^{(P)}$ can also be used to generate overlapping windows. For example, cosine windows are defined in \cite{ChungEasleyOLeary} by using midpoints
\begin{equation}
\label{eq:Midpoints}
    \midpoint^{(p)} = \frac{\partition^{(p-1)} + \partition^{(p)}}{2}, \quad p = 1,\ldots,P,
\end{equation}
so that
\begin{equation}
\label{eq:Middle cosine windows}
    \wVec_j^{(p)} = \begin{dcases}
    \cos^2\left(\frac{\frac{\pi}{2}\left(\singular_j - \midpoint^{(p)}\right)}{\midpoint^{(p-1)} - \midpoint^{(p)}}\right) & \midpoint^{(p-1)} \geq \singular_j > \midpoint^{(p)}, \\
    \cos^2\left(\frac{\frac{\pi}{2}\left(\midpoint^{(p)} - \singular_j\right)}{\midpoint^{(p)} - \midpoint^{(p+1)}}\right) & \midpoint^{(p)} \geq \singular_j > \midpoint^{(p+1)}, \\
    0 & \text{otherwise,}
    \end{dcases} \qquad p = 2,\ldots,P-1.
\end{equation}
The first and $P$th weight vectors can be defined by 
\begin{equation}
\label{eq:First cosine window}
     \wVec_j^{(1)} = \begin{dcases}
    1 & \omega^{(0)} \geq \singular_j > \midpoint^{(1)}, \\
    \cos^2\left(\frac{\frac{\pi}{2}\left(\midpoint^{(1)} - \singular_j\right)}{\midpoint^{(1)} - \midpoint^{(2)}}\right) & \midpoint^{(1)} \geq \singular_j > \midpoint^{(2)}, \\
    0 & \text{otherwise}
    \end{dcases}
\end{equation}
and
\begin{equation}
\label{eq:Last cosine window}
    \wVec_j^{(P)} = \begin{dcases}
    \cos^2\left(\frac{\frac{\pi}{2}\left(\singular_j - \midpoint^{(P)}\right)}{\midpoint^{(P-1)} - \midpoint^{(P)}}\right) & \midpoint^{(P-1)} \geq \singular_j > \midpoint^{(P)}, \\
    1 & \midpoint^{(P)} \geq \singular_j > \omega^{(P)}, \\
    0 & \text{otherwise,}
    \end{dcases}
\end{equation}
respectively.
\begin{proposition}
\label{prop:Cosine windows}
The cosine windows given by \cite{ChungEasleyOLeary}, presented in lines \eqref{eq:Midpoints} through \eqref{eq:Last cosine window}, satisfy
\[\sum_{p=1}^{P} \wVec_j^{(p)} = 1, \quad j = 1,\ldots,n.\]
\end{proposition}
\begin{proof}
Let $j \in \{1,\ldots,n\}$ be fixed. If either $\omega^{(0)} \geq \singular_j > \midpoint^{(1)}$ or $\midpoint^{(P)} \geq \singular_j > \omega^{(P)}$, the result follows immediately from \eqref{eq:First cosine window} or \eqref{eq:Last cosine window}, respectively. Now suppose that $\midpoint^{(1)} \geq \singular_j > \midpoint^{(P)}$. Then there exists exactly one value $p \in \{1,\ldots,P-1\}$ such that $\midpoint^{(p)} \geq \singular_j > \midpoint^{(p+1)}$. By definition of the cosine windows, $\wVec^{(p)}$ and $\wVec^{(p+1)}$ are the only weight vectors such that their $j$th components are nonzero. It must be shown that $\wVec^{(p)}_j + \wVec^{(p+1)}_j = 1$. From \eqref{eq:Middle cosine windows}, the $j$th component of $\wVec^{(p)}$ is
\[\wVec^{(p)}_j = \cos^2\left(\frac{\frac{\pi}{2}\left(\midpoint^{(p)} - \singular_j\right)}{\midpoint^{(p)} - \midpoint^{(p+1)}}\right)\]
and the $j$th component of $\wVec^{(p+1)}$ is
\[\wVec^{(p+1)}_j =  \cos^2\left(\frac{\frac{\pi}{2}\left(\singular_j - \midpoint^{(p+1)}\right)}{\midpoint^{(p)} - \midpoint^{(p+1)}}\right).\]
Using the identity $\cos(\theta) = \sin\left(\frac{\pi}{2} - \theta\right)$, we have that
\begin{align*}
\wVec^{(p+1)}_j &= \sin^2\left(\frac{\pi}{2} - \frac{\frac{\pi}{2}\left(\singular_j - \midpoint^{(p+1)}\right)}{\midpoint^{(p)} - \midpoint^{(p+1)}}\right) \\
&= \sin^2\left(\frac{\frac{\pi}{2}\left(\midpoint^{(p)} - \midpoint^{(p+1)}\right) - \frac{\pi}{2}\left(\singular_j - \midpoint^{(p+1)}\right)}{\midpoint^{(p)} - \midpoint^{(p+1)}}\right) \\
&= \sin^2\left(\frac{\frac{\pi}{2}\left(\midpoint^{(p)} - \singular_j\right)}{\midpoint^{(p)} - \midpoint^{(p+1)}}\right).
\end{align*}
Therefore,
\[\wVec^{(p)}_j + \wVec^{(p+1)}_j = \cos^2\left(\frac{\frac{\pi}{2}\left(\midpoint^{(p)} - \singular_j\right)}{\midpoint^{(p)} - \midpoint^{(p+1)}}\right) +  \sin^2\left(\frac{\frac{\pi}{2}\left(\midpoint^{(p)} - \singular_j\right)}{\midpoint^{(p)} - \midpoint^{(p+1)}}\right) = 1.\]
\end{proof}
Given a penalty matrix $L \neq I$, a windowed regularized solution similar to \eqref{eq:TikSolWindow} can be obtained using the GSVD. Given the decompositions $A = U\Delta\trans{X}$ and $L = V\Lambda\trans{X}$ described in Section \ref{sec:GSVD} and windows $W^{(p)}$ for $p = 1,\ldots,P$, a corresponding windowed regularized solution is
\begin{equation}
\label{eq:Windowed GSVD Solution}
    \xWin(\regparamVec) = \sum_{p=1}^P Y\inv{\left[\trans{\Delta}\Delta + \left(\regparam^{(p)}\right)^2 \trans{\Lambda}\Lambda\right]}W^{(p)}\trans{\Delta}\dft{\dVec}
\end{equation}
with $Y$ again being the inverse of $\trans{X}$. In terms of the generalized singular values $\gamma_j$, \eqref{eq:Windowed GSVD Solution} can be written as
\begin{equation}
\label{eq:Windowed GSVD Solution Sum}
    \xWin(\regparamVec) = \sum_{j=1}^n \left(\sum_{p=1}^P \filt_j\left(\regparam^{(p)},\gamma_j\right)\wVec^{(p)}_{j}\right)  \frac{\dft{\dVec}_{j}}{\delta_j} Y_{\cdot,j}.
\end{equation}
However, care must be taken when selecting the weight vectors $\wVec^{p}$ in light of the fact that the generalized singular values are arranged in ascending order.

\subsection{Multiple data sets} \label{sec:Multiple data sets}
We now consider the situation where we have a collection of data sets $\{\dVec^{(r)}\}_{r=1}^R$ where 
\begin{equation}
\label{eq:Big vectors}
\dVec^{(r)} = \bVec^{(r)} + \noiseVec^{(r)} = {A^{(r)}}\xVec^{(r)} + \noiseVec^{(r)}, \quad \noiseVec^{(r)} \sim \mathcal{N}(\bm{0},\Sigma^{(r)}), \qquad r = 1,\ldots,R.
\end{equation}
For given regularization parameters $\regparam^{(r)}$ and penalty matrices $L^{(r)}$, Tikhonov regularization can be performed to produce regularized solutions $\xVec(\regparam^{(r)})$ that minimize the functionals
\begin{equation}
T^{(r)}(\xVec;\regparam^{(r)}) := \|A^{(r)}\xVec - \dVec^{(r)}\|_2^2 + \left(\regparam^{(r)}\right)^2\|L^{(r)}\xVec\|_2^2.
\end{equation}
A parameter selection method is generally utilized to select the regularization parameter for each system. Instead, let $\dBig$ be the vector formed by vertically concatenating the data sets $\{\dVec^{(r)}\}_{r=1}^R$ and define the functional
\begin{equation}
\label{eq:Big functional}
\widetilde{T}\left(\widetilde{\xVec};\widetilde{\regparam}\right) = \|\ABig\widetilde{\xVec} - \widetilde{\dVec}\|_2^2 + \widetilde{\regparam}^2\|\widetilde{L}\widetilde{\xVec}\|_2^2,
\end{equation}
where $\widetilde{\regparam}$ is a single regularization parameter. This construction implies that $\xBig$, $\bBig$, $\widetilde{\noiseVec}$ are vertical concatenations of vectors in $\{\bVec^{(r)}\}_{r=1}^R$ and $\{\noiseVec^{(r)}\}_{r=1}^R$, respectively. The matrices $\ABig$ and $\LBig$ are block diagonal matrices with diagonal blocks $\{A^{(r)}\}_{r=1}^R$ and $\{L^{(r)}\}_{r=1}^R$, respectively. By the definition of the 2-norm and the construction of \eqref{eq:Big functional}, we can also write
\begin{equation}
\label{eq:Big functional 2}
\widetilde{T}\left(\xBig;\widetilde{\regparam}\right) = \sum_{r=1}^R \left(\|A^{(r)}\xVec^{(r)} - \dVec^{(r)}\|_2^2 + \widetilde{\regparam}^2\|L^{(r)} \xVec^{(r)}\|_2^2\right) = \sum_{r=1}^R T^{(r)}\left(\xVec^{(r)};\widetilde{\regparam}\right).
\end{equation}
In regards to selecting regularization parameters, the advantage of regularizing via \eqref{eq:Big functional} is that we only have to select one parameter instead of $R$ parameters (one for each data set). Table \ref{tab:System assumptions} contains the assumptions used here for the development of parameter selections methods for multiple data sets. The notation $\vcat$ is used for brevity and denotes vertical concatenation of column vectors. The built-in MATLAB command $\mathtt{vertcat}$ can be used for vertical concatenation of vectors or matrices of appropriate dimension. Assumption \ref{Assumption_System} summarizes the set-up established in \eqref{eq:Big vectors}.

\begin{table}[ht!]
  \begin{center}
    \caption{Summary of assumptions and constructions used for $r = 1,\ldots,R$.}
    \label{tab:System assumptions}
    \begin{tabular}{|c|c|}
    \hline
    $\xBig = \vcat\left(\xVec^{(1)},\ldots,\xVec^{(R)}\right)$ & $\widetilde{\Sigma} = \diag\left(\Sigma^{(1)},\ldots,\Sigma^{(R)}\right)$ \\
    \hline
    $\dBig = \vcat\left(\dVec^{(1)},\ldots,\dVec^{(R)}\right)$ & $\ABig = \diag\left(A^{(1)},\ldots,A^{(R)}\right)$ \\
    \hline
    $\widetilde{\noiseVec} = \vcat\left(\noiseVec^{(1)},\ldots,\noiseVec^{(R)}\right)$ & $\LBig = \diag\left(L^{(1)},\ldots,L^{(R)}\right)$ \\
    \hline
    \end{tabular}
  \end{center}
\end{table}

\begin{assumption}
\label{Assumption_System}
For $r = 1,\ldots,R$, assume that $\bVec^{(r)} = {A^{(r)}}\xVec^{(r)}$, $\dVec^{(r)} = \bVec^{(r)} + \noiseVec^{(r)}$, and $\noiseVec^{(r)} \sim \mathcal{N}(\zeroVec^{(r)},\Sigma^{(r)})$. The vectors $\bVec^{(r)}$, $\dVec^{(r)}$, and $\noiseVec^{(r)}$ are of length $\mA_r$ and $\xVec^{(r)}$ is of length $n_r$.
\end{assumption}

To obtain an explicit representation of $\xBig(\regparamBig)$, the GSVD can be utilized. A GSVD of each pair $(A^{(r)},L^{(r)})$ can obtained for $r = 1,\ldots,R$ so that
\begin{equation}
    A^{(r)} = U^{(r)}\Delta^{(r)}\trans{\left(X^{(r)}\right)}, \quad L^{(r)} = V^{(r)}\Lambda^{(r)}\trans{\left(X^{(r)}\right)}.
\end{equation}
These decompositions then allow for the definition of block decomposition matrices described in Table \ref{tab:Block GSVD Matrices}.
\begin{table}[ht!]
  \begin{center}
    \caption{Matrices constructed from the GSVD's of $(A^{(r)},L^{(r)})$ can obtained for $r = 1,\ldots,R$.}
    \label{tab:Block GSVD Matrices}
    \begin{tabular}{|c|c|}
    \hline
    $\widetilde{U} = \diag\left(U^{(1)},\ldots,U^{(R)}\right)$ & $\widetilde{V} = \diag\left(V^{(1)},\ldots,V^{(R)}\right)$ \\
    \hline
    $\widetilde{\Delta} = \diag\left(\Delta^{(1)},\ldots,\Delta^{(R)}\right)$ & $\widetilde{\Lambda} = \diag\left(\Lambda^{(1)},\ldots,\Lambda^{(R)}\right)$ \\
    \hline
    \multicolumn{2}{|c|}{$\trans{\widetilde{X}} = \diag\left(\trans{\left(X^{(1)}\right)},\ldots,\trans{\left(X^{(R)}\right)}\right)$} \\
    \hline
    \end{tabular}
  \end{center}
\end{table}
We are now able to express $\ABig$ and $\LBig$ as
\begin{equation}
    \label{eq:Adapted A and L GSVD}
    \ABig = \widetilde{U}\widetilde{\Delta}\trans{\widetilde{X}}, \quad \LBig = \widetilde{V}\widetilde{\Lambda}\trans{\widetilde{X}}.
\end{equation}
Analogous to representation \eqref{eq:GSVD Normal Eq Sol} for a single system, the solution $\xBig(\regparamBig)$ can be written as 
\begin{equation}
    \label{eq:GSVD Big Normal Eq Sol}
    \xBig(\regparamBig) = \widetilde{Y}\inv{\left(\trans{\widetilde{\Delta}}\widetilde{\Delta} + \widetilde{\regparam}^2 \trans{\widetilde{\Lambda}}\widetilde{\Lambda}\right)}\trans{\widetilde{\Delta}}\svd{\dBig}
\end{equation}
where $\widetilde{Y}$ is the inverse of $\widetilde{X}$ and $\svd{\dBig} = \trans{\widetilde{U}}\dBig$. \par
Some of the parameter estimation methods considered in this paper rely upon the statistical properties of the noise in the data, so the statistics of $\widetilde{\noiseVec}$ will be addressed. Though \eqref{eq:Big vectors} indicates that the random vectors $\{\noiseVec^{(r)}\}_{r=1}^R$ are assumed to have zero mean, we can relax this assumption so that $\noiseVec^{(r)} \sim \mathcal{N}(\bm{\mu}^{(r)},\Sigma^{(r)})$ for all $r = 1,\ldots,R$. The distribution of $\widetilde{\noiseVec}$ is then given by Lemma \ref{lem:Concatenation of Normal Noise}, which follows from the properties of the multivariate normal distribution. 
\begin{lemma}
\label{lem:Concatenation of Normal Noise}
Let $\{\noiseVec^{(r)}\}_{r=1}^R$ be a collection of mutually independent random vectors with $\noiseVec^{(r)} \sim \mathcal{N}(\bm{\mu}^{(r)},\Sigma^{(r)})$ for each $r = 1,\ldots,R$, and let $\widetilde{\noiseVec} = \vcat(\noiseVec^{(1)},\ldots,\noiseVec^{(R)})$. Then $\widetilde{\noiseVec} \sim \mathcal{N}(\widetilde{\bm{\mu}},\widetilde{\Sigma})$ where $\widetilde{\bm{\mu}} = \vcat(\bm{\mu}^{(r)},\ldots,\bm{\mu}^{(R)})$ and $\widetilde{\Sigma} = \diag(\Sigma^{(1)},\ldots,\Sigma^{(R)})$.
\end{lemma}
\noindent Lemma \ref{lem:Concatenation of Normal Noise} is also useful in determining quantities like $\E(\|\widetilde{\noiseVec}\|_2^2)$, which is accomplished as needed in Sections \ref{sec:UPRE} and \ref{sec:MDP}. \par
We conclude Section \ref{sec:Multiple data sets} by enumerating the additional underlying assumptions that will be utilized in Section \ref{sec:Methods}.

\begin{assumption}
\label{Assumption_Matrices}
For $r = 1,\ldots,R$, assume that $A^{(r)} = A$ and $L^{(r)} = L$.
\end{assumption}  

\begin{assumption}
\label{Assumption_Noise}
Given $\noiseVec^{(r)} \sim \mathcal{N}(\zeroVec^{(r)},\Sigma^{(r)})$ for $r = 1,\ldots,R$, assume that $\zeroVec^{(r)} = \zeroVec_{\mA \times 1}$ and $\Sigma^{(r)} = \noiseSD_r^2 I_{\mA}$.
\end{assumption}
\noindent In particular, Assumptions \ref{Assumption_System} and \ref{Assumption_Matrices} imply that $n_r = n$ for $r = 1,\ldots,R$. Assumption \ref{Assumption_Noise} can relaxed so that $\Sigma^{(r)}$ is any diagonal matrix $D^{(r)}$, in which case a whitening transformation $C^{(r)}$ could applied so that $\xi^{(r)} = C^{(r)}\noiseVec^{(r)}$ is distributed $\mathcal{N}(\zeroVec_{\mA \times 1},I_{\mA})$. For example, one could use the ZCA transformation $C^{(r)} = \left(\Sigma^{(r)}\right)^{-1/2}$ \cite{BellSejnowski}.

\subsection{Multiparameter regularization for multiple data sets} \label{sec:Adapted regularization}

The ultimate situation we consider is the case where multiparameter regularization is applied to multiple data sets. Letting $\regparamVec^{(r)}$ be the regularization parameters used for multiparameter regularization applied to the $r$th system described by \eqref{eq:Big vectors}, we can independently construct regularized solutions
\[\xWin^{(r)}\left(\regparamVec^{(r)}\right) = \sum_{p=1}^{P_r} Y^{(r)}\inv{\left[\trans{\left(\Delta^{(r)}\right)}\Delta^{(r)} + \left(\regparam^{(r)}_p\right)^2 \trans{\left(\Lambda^{(r)}\right)}\Lambda^{(r)}\right]}W^{(p)}_{(r)}\trans{\left(\Delta^{(r)}\right)}\dft{\dVec}^{(r)}\]
where $W^{(p)}_{(r)}$ is the $p$th window for the $r$th system, $\regparam^{(r)}_p$ is the regularization parameter for the $p$th window, and $P_r$ is the number of windows. Each system can have its own set of windows, meaning that there are a total of $RP_r$ regularization parameters. However, the primary assumption we make moving forward is that the windows $W^{(p)}_{(r)}$ for each system are the same, i.e $W^{(p)}_{(r)} = W^{(p)}$ for all $r = 1,\ldots,R$ and $p = 1,\ldots,P$. This implies that $\regparamVec^{(r)}$ are all vectors of length $P$ and there are a total of $RP$ parameters. \par
Analogous to the uniparameter case for multiple data sets, we define $\xWinBig(\regparamVecBig)$ as the vertical concatenation of the $\xWin^{(r)}\left(\regparamVecBig\right)$ for $r = 1,\ldots,R$, where $\regparamVecBig \in \mathbb{R}^P$ is a single vector of parameters. Using the constructions from Table \ref{tab:Block GSVD Matrices}, we have
\[\xWinBig(\regparamVecBig) = \sum_{p=1}^{P} \widetilde{Y}\inv{\left[\trans{\widetilde{\Delta}}\widetilde{\Delta} + \widetilde{\regparam}_p^2 \trans{\widetilde{\Lambda}}\widetilde{\Lambda}\right]}\widetilde{W}^{(p)}\trans{\widetilde{\Delta}}\dft{\dBig}\]
where $\widetilde{W}^{(p)} = \diag(W^{(p)},\ldots,W^{(p)})$ has $R$ diagonal blocks.

\section{Parameter selection methods} \label{sec:Methods}
As stated in Section \ref{sec:Introduction}, there are a variety of methods for selecting regularization parameters and we illustrate some limited examples. The UPRE and MDP methods are popular choices for situations in which the variance of the noise in the data is known. In contrast, the GCV and L-curve methods are common choices when the variance is unknown. However, all four methods involve the norm of a term $\rReg$ called the regularized residual. In the uniparameter case involving a single data point $\dVec$, the regularized residual is defined as
\begin{equation}
\label{eq:Regularized Residual}
\rReg = A\xReg - \dVec.
\end{equation}

We extend the notation introduced in Section \ref{sec:Multiparameter} for multiparameter regularization involving a single data point to define a windowed regularized residual
\begin{equation}
\label{eq:Windowed regularized residual}
\rWin(\regparamVec) = A\xWin(\regparamVec) - \dVec
\end{equation}
where the windowed regularized $\xWin(\regparamVec)$ is given by \eqref{eq:Windowed GSVD Solution} assuming the use of the GSVD. If instead we have multiple data points but each is used with uniparameter regularization, we can extend the notation from Section \ref{sec:Multiple data sets} (see Table \ref{tab:System assumptions}) so as to define a regularization solution
\begin{equation}
\label{eq:Big regularized residual}
\rBig(\widetilde{\regparam}) = \ABig\xBig(\widetilde{\regparam}) - \dBig. 
\end{equation}
where \eqref{eq:GSVD Big Normal Eq Sol} is a representation of $\xBig(\widetilde{\regparam})$. \par 
The ultimate situation is multiparameter regularization as applied to multiple data. For this situation, we combine the notations of Sections \ref{sec:Multiparameter} through \ref{sec:Adapted regularization} to construct a regularized residual $\rWinBig(\regparamVecBig)$ defined as
\begin{equation}
\label{eq:Windowed Big regularized residual}
\rWinBig(\regparamVecBig) = \ABig\xWinBig(\regparamVecBig) - \dBig. 
\end{equation}

As a final remark, the development of the adapted UPRE, MDP, and GCV methods also rely on Assumption \ref{Assumption_System} and the constructions contained in Table \ref{tab:System assumptions}. By Lemma \ref{lem:Concatenation of Normal Noise}, this assumption implies that $\widetilde{\noiseVec} \sim \mathcal{N}\left(\widetilde{\zeroVec},\widetilde{\Sigma}\right)$ with $\widetilde{\zeroVec} \in \mathbb{R}^N$ and $N = \sum_{r=1}^{R} n_r$. 

\subsection{Unbiased Predictive Risk Estimator} \label{sec:UPRE}
The UPRE method was developed in 1973 by Mallows and considers the statistical relationship between predictive error $\pVec(\regparam)$ and the regularized residual $\rVec(\regparam)$; these quantities are defined respectively as
\begin{equation}
\label{eq:Predictive Error and Regularized Residual}
\pVec(\regparam) = A(\xReg - \xSol), \quad \rVec(\regparam) = A\xReg - \dVec.
\end{equation}
The standard UPRE function for Tikhonov regularization is
\begin{equation}
\label{eq:UPRE}
\U(\alpha) = \frac{1}{\mA}\|\rVec(\regparam)\|_2^2 + \frac{2\noiseSD^2}{\mA}\trace(A_\regparam) - \noiseSD^2
\end{equation}
where $\A = A(\trans{A}A + \regparam^2\trans{L}L)^{-1}\trans{A}$ is the influence matrix and $\noiseVec \sim \mathcal{N}(\bm{0},\noiseSD^2 I_{\mA})$. The function $\U(\regparam)$ is an unbiased estimator of the expected value of $\frac{1}{\mA}\|\pVec(\regparam)\|_2^2$, a quantity called the predictive risk. The UPRE method selects $\regparam_{\textrm{UPRE}} = \argmin_{\regparam > 0} \U(\regparam)$. \par
We first derive the UPRE function for Tikhonov regularization under the more general condition that $\noiseVec \sim \mathcal{N}(\bm{0},\Sigma)$. To this end, we can then use the following lemma, which is a modification of the Trace Lemma stated in \cite[p.~98]{Vogel:2002}.
\begin{lemma}
\label{lem:Generalized Trace Lemma}
Let $\fVec \in \mathbb{R}^m$ be a constant vector, $\noiseVec$ be a real random $n$-vector with $\noiseVec \sim \mathcal{N}(\bm{\mu},\Sigma)$, $B \in \mathbb{R}^{m \times n}$, and let $\langle \cdot,\cdot \rangle$ be the standard Euclidean inner product. Then
\[\E(\|\fVec + B\noiseVec\|_2^2) = \|\fVec\|_2^2 + 2\sum_{j=1}^{n} (\trans{\fVec}B)_j \mu_j + \trace\left(B\Sigma\trans{B}\right).\]
\end{lemma}
\begin{proof}
By the linearity of the expected value operator and inner product,
\[\E(\|\fVec + B\noiseVec\|_2^2) = \E(\langle \fVec + B\noiseVec, \fVec + B\noiseVec\rangle) =  \E(\|\fVec\|_2^2) + 2\E(\langle \fVec, B\noiseVec\rangle) + \E(\|B\noiseVec\|_2^2).\]
$\E(\|\fVec\|_2^2) = \|\fVec\|_2^2$ because $\fVec$ is a constant vector. Moreover, the definition of the Euclidean inner product can be used to write $\E(\langle \fVec, B\noiseVec\rangle)$ as $\E(\sum_{j=1}^n (\trans{\fVec}B)_j \noise_j) = \sum_{j=1}^n (\trans{\fVec}B)_j \E(\noise_j)$. Thus,
\begin{align*}
\E(\|\fVec + B\noiseVec\|_2^2) &= \|\fVec\|_2^2 + 2\sum_{j=1}^{n} (\trans{\fVec}B)_j \E(\noise_j) + \E(\|B\noiseVec\|_2^2) \\
&= \|\fVec\|_2^2 + 2\sum_{j=1}^{n} (\trans{\fVec}B)_j \mu_j + \E(\|B\noiseVec\|_2^2).
\end{align*}
Focusing on $\E(\|B\noiseVec\|_2^2)$, we can write
\[\E(\|B\noiseVec\|_2^2) = \E\left(\sum_{j=1}^n (B\noiseVec)_j^2\right) = \sum_{j=1}^n \E((B\noiseVec)_j^2) = \sum_{j=1}^n \E(y_j^2)\]
where $\mathbf{y} = B\noiseVec$.
Since $\noiseVec \sim \mathcal{N}(\bm{\mu},\Sigma)$, $\mathbf{y} \sim \mathcal{N}(B\bm{\mu},B\Sigma\trans{B})$ \cite{Rao1973}. Lastly, $\E(y_j^2) = \Var(y_j) = (B\Sigma\trans{B})_{j,j}$ for each $j = 1,\ldots,n$. Therefore $\sum_{j=1}^n \E(y_j^2) = \sum_{j=1}^n \Var(y_j) = \trace(B\Sigma\trans{B})$ and
\[\E(\|\fVec + B\noiseVec\|_2^2) = \|\fVec\|_2^2 + 2\sum_{j=1}^{n} (\trans{\fVec}B)_j \mu_j + \trace(B\Sigma\trans{B}).\]
\end{proof}

\noindent Applying Lemma \ref{lem:Generalized Trace Lemma} to the norm of $\pVec(\regparam)$ and noting that $\E(\noiseVec) = \zeroVec$ produces
\begin{equation}
\label{eq:New Predicitive Risk 1}
\E\left(\frac{1}{\mA}\left\|\pVec(\regparam)\right\|_2^2\right) = \frac{1}{\mA}\left\|\left(\A - I_{\mA}\right)A\xVec\right\|_2^2 + \frac{1}{\mA}\trace\left(\A\Sigma\trans{A}(\regparam)\right).
\end{equation}
The regularized residual $\rVec(\regparam)$ can be rewritten as
\begin{equation}
\label{eq:New Regularized Residual}
\rVec(\regparam) = \left(\A - I_{\mA}\right)A\xVec + \left(\A - I_{\mA}\right)\noiseVec,
\end{equation}
and so applying Lemma \ref{lem:Generalized Trace Lemma} to the norm of $\rVec(\regparam)$ yields
\[\E\left(\frac{1}{\mA}\left\|\rVec(\regparam)\right\|_2^2\right) = \frac{1}{\mA}\left\|\left(\A - I_{\mA}\right)A\xVec\right\|_2^2 + \frac{1}{\mA}\trace\left(\trans{\left(\A - I_{\mA}\right)}\Sigma\left(\A - I_{\mA}\right)\right).\]
The trace term can be expanded as
\begin{align*}
    &\trace\left(\trans{\left(\A - I_{\mA}\right)}\Sigma\left(\A - I_{\mA}\right) \right) \\
    &= \trace\left(\trans{A}(\regparam)\Sigma\A\right) - \trace\left(\trans{A}(\regparam)\Sigma\right) - \trace\left(\Sigma\A\right) + \trace\left(\Sigma\right).
\end{align*}
The cyclic property of the trace operator and the fact that $\Sigma$ and $\A = A(\trans{A}A + \regparam^2\trans{L}L)^{-1}\trans{A}$ are symmetric matrices give
\[\trace\left(\trans{\left(\A - I_{\mA}\right)}\Sigma\left(\A - I_{\mA}\right)\right) = \trace\left(\A\Sigma\trans{A}(\regparam)\right) - 2\trace\left(\Sigma\A\right) + \trace\left(\Sigma\right),\]
and so \eqref{eq:New Predicitive Risk 1} can be expressed as
\begin{equation}
\label{eq:New Predictive Risk 2}
\E\left(\frac{1}{\mA}\left\|\pVec(\regparam)\right\|_2^2\right) = \E\left(\frac{1}{\mA}\left\|\rVec(\regparam)\right\|_2^2\right) + \frac{2}{\mA}\trace\left(\Sigma\A\right) - \frac{1}{\mA}\trace\left(\Sigma\right).
\end{equation}
Analogous to the standard UPRE function, we can define $\UBig(\regparam)$ as
\begin{equation}
\label{eq:UPRE 2}
\UBig(\regparam) = \frac{1}{\mA}\left\|\rVec(\regparam)\right\|_2^2 + \frac{2}{\mA}\trace\left(\Sigma\A\right) - \frac{1}{\mA}\trace\left(\Sigma\right).
\end{equation}
so that $\UBig(\regparam)$ is an unbiased estimator of predictive risk. The standard UPRE function \eqref{eq:UPRE} is recovered from \eqref{eq:UPRE 2} if $\Sigma = \noiseSD^2 I_{\mA}$.

\begin{proposition}
Using the constructions in Table \ref{tab:System assumptions} under Assumption \ref{Assumption_System} with $n_r = n$ for all $r = 1,\ldots,R$, the UPRE function $\widetilde{U}(\regparam)$ for the data sets $\{\dVec^{(r)}\}_{r=1}^R$ is
\begin{equation}
\label{eq:Averaged UPRE}
\UBig(\regparam) = \frac{1}{R} \sum_{r=1}^R \U^{(r)}(\regparam),
\end{equation}
where
\begin{equation}
\label{eq:Individual UPRE}
\U^{(r)}(\regparam) = \frac{1}{\mA}\left\|\rVec^{(r)}(\regparam)\right\|_2^2 + \frac{2}{\mA} \trace\left(\Sigma^{(r)} A^{(r)}(\regparam)\right) - \frac{1}{\mA} \trace\left(\Sigma^{(r)}\right).
\end{equation}
with
\begin{equation}
\label{eq:Single Regularized Residual}
    \rVec^{(r)} = A^{(r)}\xVec^{(r)}(\regparam) - \dVec^{(r)}.
\end{equation}
\end{proposition}
\begin{proof}
A block diagonal influence matrix $\ABig(\regparam)$ can be defined as
\[\ABig(\regparam) = \ABig(\trans{\ABig}\ABig + \regparam^2\trans{\widetilde{L}}\widetilde{L})^{-1}\trans{\ABig}\]
with diagonal blocks
\begin{equation}
\label{eq:Diagonal blocks}
A^{(r)}_\regparam = A^{(r)}\left(\trans{\left(A^{(r)}\right)}A^{(r)} + \regparam^2\trans{\left(L^{(r)}\right)}L^{(r)}\right)^{-1}\trans{\left(A^{(r)}\right)}, \quad r = 1,\ldots,R.
\end{equation}
By further defining $\rBig(\regparam) = \ABig\widetilde{\xVec}(\regparam) - \widetilde{\dVec}$ and noting that Lemma \ref{lem:Concatenation of Normal Noise} implies the vertical concatenation $\widetilde{\noiseVec}$ of $\{\noiseVec^{(r)}\}_{r=1}^R$ is distributed $\mathcal{N}(\zeroVec,\widetilde{\Sigma})$ with $\zeroVec \in \mathbb{R}^N$ and $\widetilde{\Sigma} = \diag(\Sigma^{(1)},\ldots,\Sigma^{(R)})$, the UPRE function \eqref{eq:UPRE 2} can be directly applied to produce
\begin{equation}
\label{eq:UPRE 3}
\UBig(\regparam) = \frac{1}{N}\left\|\rBig(\regparam)\right\|_2^2 + \frac{2}{N}\trace\left(\widetilde{\Sigma}\ABig(\regparam)\right) - \frac{1}{N}\trace\left(\widetilde{\Sigma}\right).
\end{equation}
The block diagonal structure of both $\widetilde{\Sigma}$ and $\ABig(\regparam)$ allow for the trace terms in \eqref{eq:UPRE 3} to be written as sums involving the diagonal blocks:
\begin{equation}
\label{eq:UPRE 4}
\UBig(\regparam) = \frac{1}{N}\left\|\rBig(\regparam)\right\|_2^2 + \frac{2}{N} \sum_{r=1}^R \trace\left(\Sigma^{(r)}A^{(r)}(\regparam)\right) - \frac{1}{N} \sum_{r=1}^R \trace\left(\Sigma^{(r)}\right).
\end{equation}
The squared 2-norm of $\rBig(\regparam)$ can also be expressed as a sum so that
\begin{align*}
\UBig(\regparam) &= \frac{1}{N} \sum_{r=1}^R \left\|\rVec^{(r)}(\regparam)\right\|_2^2 + \frac{2}{N} \sum_{r=1}^R \trace\left(\Sigma^{(r)} A^{(r)}(\regparam)\right) - \frac{1}{N} \sum_{r=1}^R \trace\left(\Sigma^{(r)}\right) \\
&= \frac{1}{R} \sum_{r=1}^R \left(\frac{1}{\mA}\left\|\rVec^{(r)}(\regparam)\right\|_2^2 + \frac{2}{\mA} \trace\left(\Sigma^{(r)} A^{(r)}(\regparam)\right) - \frac{1}{\mA} \trace\left(\Sigma^{(r)}\right)\right) \\
&= \frac{1}{R} \sum_{r=1}^R \U^{(r)}(\regparam),
\end{align*}
where
\[\U^{(r)}(\regparam) = \frac{1}{\mA}\left\|\rVec^{(r)}(\regparam)\right\|_2^2 + \frac{2}{\mA} \trace\left(\Sigma^{(r)} A^{(r)}(\regparam)\right) - \frac{1}{\mA} \trace\left(\Sigma^{(r)}\right), \quad r = 1,\ldots,R.\]
\end{proof}
\noindent In other words, $\UBig(\regparam)$ is the average of $\{\U^{(r)}(\regparam)\}_{r=1}^R$. The UPRE method for multiple data sets is to let $\widetilde{\regparam}_{\textrm{UPRE}} = \argmin_{\regparam > 0} \UBig(\regparam)$. As a final remark, \eqref{eq:Individual UPRE} is equivalent to \eqref{eq:UPRE} under Assumption \ref{Assumption_Noise}. \par
A representation of \eqref{eq:Averaged UPRE} will now be presented that involves generalizations of the filter functions \eqref{eq:Filter functions}. In an effort to streamline this presentation, we will use the version of the GSVD outlined in Section \ref{sec:GSVD} (which uses the assumption that the system matrix in \eqref{eq:TikSol3} has full column rank). Specifically, we make the following assumption.
\begin{assumption}
\label{Assumption_Diagonalization}
We assume that there exist matrices $U$, $V$, and $\trans{X}$ such that $A^{(r)} = U{\Delta^{(r)}}\trans{X}$ and  $L^{(r)} = V{\Lambda^{(r)}}\trans{X}$ for all $r = 1,\ldots,R$. 
\end{assumption}
\noindent While this is an extremely strong and unrealistic assumption (the belief that such a factorization does not exist in general is expressed in \cite{Brezinski2003}), it certainly becomes realistic under Assumption \ref{Assumption_Matrices}. We use Assumption \ref{Assumption_Diagonalization} to simply present the following derivation in the most general setting. An analogous approach could be taken if the matrices $A^{(r)}$ and $L^{(r)}$ can be simultaneously diagonalized with respect to a orthogonal/unitary transformation, such as the discrete cosine transform (DCT) or discrete Fourier transform (DFT); test problems using the DCT and DFT are considered in Section \ref{sec:Validation}. \par
From Assumption \ref{Assumption_Diagonalization}, we define the generalized filter functions to be
\begin{equation}
\label{eq:Filter functions 2}
\filt^{(r)}_j = \frac{\left(\delta^{(r)}_j\right)^2}{\left(\delta^{(r)}_j\right)^2 + \regparam^2 \left(\lambda^{(r)}_j\right)^2}, \quad \mfilt^{(r)}_j = 1 - \filt^{(r)}_j = \frac{\regparam^2 \left(\lambda^{(r)}_j\right)^2}{\left(\delta^{(r)}_j\right)^2 + \regparam^2 \left(\lambda^{(r)}_j\right)^2}
\end{equation}
where $\bm{\delta}^{(r)} = \sqrt{\diag(\trans{(\Delta^{(r)})}\Delta^{(r)})}$ and $\bm{\lambda}^{(r)} = \sqrt{\diag(\trans{(\Lambda^{(r)})}\Lambda^{(r)})}$ for each $r = 1,\ldots,R$. The matrix $\A^{(r)}$ given by \eqref{eq:Diagonal blocks} can be expressed as
\begin{equation}
\label{eq:Fourier diagonalization 2}
\A^{(r)} = U\Delta^{(r)}\left[\trans{(\Delta^{(r)})}\Delta^{(r)} + \regparam^2\trans{(\Lambda^{(r)})}\Lambda^{(r)}\right]^{-1}\trans{(\Delta^{(r)})}\trans{U}, \qquad r = 1,\ldots,R.
\end{equation}
Under Assumption \ref{Assumption_Noise}, $\trace(\Sigma^{(r)}) = n\noiseSD^2_r$ and the similarity invariance of the trace operator gives
\begin{equation}
\label{eq:Trace}
\trace\left(\Sigma^{(r)}\A^{(r)}\right) = \noiseSD_r^2 \trace\left(\Delta^{(r)}\left[\trans{(\Delta^{(r)})}\Delta^{(r)} + \regparam^2\trans{(\Lambda^{(r)})}\Lambda^{(r)}\right]^{-1}\trans{(\Delta^{(r)})}\right) = \noiseSD_r^2 \sum_{j=1}^{n} \filt^{(r)}_j.
\end{equation}
Another benefit of using \eqref{eq:Fourier diagonalization 2} is that we can write
\begin{align}
\frac{1}{\mA}\|\rVec^{(r)}(\regparam)\|_2^2 &= \frac{1}{\mA}\left\|U\left(\Delta^{(r)}\left[\trans{(\Delta^{(r)})}\Delta^{(r)} + \regparam^2\trans{(\Lambda^{(r)})}\Lambda^{(r)}\right]^{-1}\trans{(\Delta^{(r)})} - I_{\mA}\right)\trans{U}\dVec^{(r)}\right\|_2^2 \nonumber \\
&= \frac{1}{\mA}\left\|\left(\Delta^{(r)}\left[\trans{(\Delta^{(r)})}\Delta^{(r)} + \regparam^2\trans{(\Lambda^{(r)})}\Lambda^{(r)}\right]^{-1}\trans{(\Delta^{(r)})} - I_{\mA}\right)\dft{\dVec}^{(r)}\right\|_2^2 \nonumber \\
&= \frac{1}{\mA}\sum_{j=1}^{n} \left(\mfilt^{(r)}_j\right)^2\left(\dft{\dVec}_j^{(r)}\right)^2,
\label{eq:Fourier regularized residual}
\end{align}
where $\dft{\dVec}^{(r)} = \trans{U}\dVec^{(r)}$ for $r = 1,\ldots,R$. Therefore, the UPRE function \eqref{eq:Averaged UPRE} for multiple data sets can be written in terms of the following representation for individual UPRE functions:
\begin{equation}
\label{eq:Individual UPRE 2}
\U^{(r)}(\regparam) = \frac{1}{\mA} \sum_{j=1}^{n} \left(\mfilt^{(r)}_j\right)^2\left(\dft{\dVec}_j^{(r)}\right)^2 + \frac{2\noiseSD^2_r}{\mA} \sum_{j=1}^{n} \filt^{(r)}_j - \noiseSD^2_r, \quad r = 1,\ldots,R.
\end{equation}

We conclude Section \ref{sec:UPRE} by showing a relationship between the adapted UPRE method that uses \eqref{eq:Averaged UPRE} and the UPRE method as simply applied to averaged data. 

\begin{proposition}
Under Assumptions \ref{Assumption_System} through \ref{Assumption_Diagonalization} with data vectors $\{\dVec^{(r)}\}_{r=1}^R$, let 
\begin{equation}
\label{eq:Averaged data}
\aVec = \frac{1}{R}\sum_{r=1}^R \dVec^{(r)},
\end{equation}
$\dft{\aVec} = \trans{U}\aVec$, and let
\begin{equation}
\label{eq:UPRE of Average}
\UAvg(\regparam) = \frac{1}{\mA}\sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\dft{\aVec}_j\right)^2 + \frac{2}{\mA} \left(\frac{1}{R^2} \sum_{r=1}^R \noiseSD_r^2\right) \left(\sum_{j=1}^{n} \filt_j\right) - \frac{1}{R^2} \sum_{r=1}^R\noiseSD_r^2.
\end{equation}
Then
\begin{equation}
\label{eq:UPRE Bound}
\widetilde{U}(\regparam) = \frac{1}{R} \sum_{r=1}^R \U^{(r)}(\regparam) \leq R \UAvg(\regparam), \qquad \regparam \geq 0.
\end{equation}
\end{proposition}
\begin{proof}
The consequence of Assumptions \ref{Assumption_System} through \ref{Assumption_Noise} is that $\filt_j^{(r)} = \filt_j$ (and $\mfilt_j^{(r)} = \mfilt_j$) for all $r = 1,\ldots,R$. Using representation \eqref{eq:Individual UPRE 2}, we can write \eqref{eq:Averaged UPRE} as
\begin{align*}
\frac{1}{R} \sum_{r=1}^R \U^{(r)}(\regparam) &= \frac{1}{R} \sum_{r=1}^R \left[\frac{1}{\mA}\sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\dft{\dVec}_j^{(r)}\right)^2 + \frac{2\noiseSD_r^2}{\mA} \sum_{j=1}^{n} \filt_j - \noiseSD_r^2\right] \\
&= \frac{1}{R} \sum_{r=1}^R \left(\frac{1}{\mA}\sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\dft{\dVec}_j^{(r)}\right)^2\right) + \frac{1}{R} \sum_{r=1}^R \left(\frac{2\noiseSD_r^2}{\mA} \sum_{j=1}^{n} \filt_j\right) - \frac{1}{R} \sum_{r=1}^R\noiseSD_r^2 \\
&= \frac{1}{\mA}\sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\frac{1}{R} \sum_{r=1}^R \left(\dft{\dVec}_j^{(r)}\right)^2\right) + \frac{2}{\mA} \left(\frac{1}{R} \sum_{r=1}^R \noiseSD_r^2\right) \left(\sum_{j=1}^{n} \filt_j\right) - \frac{1}{R} \sum_{r=1}^R\noiseSD_r^2.
\end{align*}
Multiplying through by $R/R$ then yields
\begin{align}
\label{eq:Non-average UPRE}
&\frac{1}{R} \sum_{r=1}^R \U^{(r)}(\regparam) \nonumber \\
&= R\left[\frac{1}{\mA}\sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\frac{1}{R^2} \sum_{r=1}^R \left(\dft{\dVec}_j^{(r)}\right)^2\right) + \frac{2}{\mA} \left(\frac{1}{R^2} \sum_{r=1}^R \noiseSD_r^2\right) \left(\sum_{j=1}^{n} \filt_j\right) - \frac{1}{R^2} \sum_{r=1}^R\noiseSD_r^2\right].
\end{align}
Note that the term $\frac{1}{R^2} \sum_{r=1}^R\noiseSD_r^2$ is the variance of $\frac{1}{R} \sum_{r=1}^R \noiseVec^{(r)}$ since the random vectors $\{\noiseVec^{(r)}\}_{r=1}^R$ are mutually independent. However, \eqref{eq:Non-average UPRE} is not equal to $R$ times the UPRE function as applied to the average of the data
\[\aVec = \frac{1}{R}\sum_{r=1}^R \dVec^{(r)} = \frac{1}{R} \sum_{r=1}^R \bVec^{(r)} + \frac{1}{R} \sum_{r=1}^R \noiseVec^{(r)}\]
because of the term $\frac{1}{R^2} \sum_{r=1}^R \left(\dft{\dVec}_j^{(r)}\right)^2$. If the UPRE method was used with the average $\aVec$, then
\begin{equation}
\label{eq:Coefficients of Average}
\left(\dft{\aVec}_j\right)^2 = \frac{1}{R^2}\left(\sum_{r=1}^R \dft{\dVec}_j^{(r)}\right)^2 \leq \frac{1}{R^2} \sum_{r=1}^R \left(\dft{\dVec}_j^{(r)}\right)^2, \qquad j = 1,\ldots,n.
\end{equation}
Thus the result \eqref{eq:Non-average UPRE} shows that for
\[\UAvg(\regparam) = \frac{1}{\mA}\sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\dft{\aVec}_j\right)^2 + \frac{2}{\mA} \left(\frac{1}{R^2} \sum_{r=1}^R \noiseSD_r^2\right) \left(\sum_{j=1}^{n} \filt_j\right) - \frac{1}{R^2} \sum_{r=1}^R\noiseSD_r^2,\]
i.e. $\UAvg(\regparam)$ is the UPRE function as applied to the average $\aVec$ of the data $\{\dVec^{(r)}\}_{r=1}^R$, we have that
\[\widetilde{U}(\regparam) = \frac{1}{R} \sum_{r=1}^R \U^{(r)}(\regparam) \leq R \UAvg(\regparam), \qquad \regparam \geq 0\]
\end{proof}
\noindent Therefore the adapted UPRE function is truly distinct from simply applying the UPRE method to the average of the data, and the bound \eqref{eq:UPRE Bound} demonstrates a relationship between the two modalities.

\subsubsection{Multiparameter UPRE}
We now derive a multiparameter version of the UPRE function for a single data set $\dVec = A\xVec + \noiseVec$ with $A \in \mathbb{R}^{\mA \times n}$ and $\noiseVec \sim \mathcal{N}(\zeroVec,\noiseSD^2 I_{\mA})$. For $p = 1,\ldots,P$, let $\wVec^{(p)}$ be weight vectors satisfying \eqref{eq:Weights} and let $W^{(p)} = \diag(\wVec^{(p)})$. For the regularized solution \eqref{eq:TikSolWindow}, the regularization matrix $R_\text{win}(\regparamVec)$ is
\begin{equation}
\label{eq:Windowed Reg Matrix}
    R_\text{win}(\regparamVec) = \sum_{p=1}^P V\inv{\left(\trans{S}S + \left(\regparam^{(p)}\right)^2\right)}W^{(p)}\trans{S}\trans{U}.
\end{equation}
The corresponding influence matrix $A_\text{win}(\regparamVec) = AR_\text{win}(\regparamVec)$ is then
\begin{equation}
\label{eq:Windowed Influence Matrix}
    A_\text{win}(\regparamVec) = US\left[\sum_{p=1}^P V\inv{\left(\trans{S}S + \left(\regparam^{(p)}\right)^2\right)}W^{(p)}\right]\trans{S}\trans{U}. 
\end{equation}
The windowed regularized residual $\rVec_\text{win}(\regparamVec) = (A_\text{win}(\regparamVec) - I_{\mA})\dVec$ can be written as
\begin{equation}
    \rVec_\text{win}(\regparamVec) = U\left(S\left[\sum_{p=1}^P V\inv{\left(\trans{S}S + \regparam_{p}^2\right)}W^{(p)}\right]\trans{S} - I_{\mA}\right)\dft{\dVec}.
\end{equation}
Thus the multiparameter UPRE function for problem $\dVec = A\xVec + \noiseVec$ is 
\begin{equation}
\label{eq:Multiparameter UPRE}
    \UWin(\regparamVec) = \frac{1}{\mA}\left\|\rVec_\text{win}(\regparamVec)\right\|_2^2 + \frac{2\noiseSD^2}{\mA}\trace\left(A_\text{win}(\regparamVec)\right) - \noiseSD^2.
\end{equation}
Using $\mfilt(\regparam,\singular) := 1 - \filt(\regparam,\singular)$ with $\filt(\regparam,\singular)$ defined by \eqref{eq:TikFilt}, $\|\rVec_\text{win}(\regparamVec)\|_2^2$ can be expressed as
\begin{equation}
\label{eq:Windowed Regularized Residual}
    \left\|\rVec_\text{win}(\regparamVec)\right\|_2^2 = \sum_{j=1}^{n} \left(\sum_{p=1}^{P} w_j^{(p)} \mfilt\left(\regparam^{(p)},\singular_j\right) \right)^2 \left(\dft{d}_j\right)^2.
\end{equation}
Similarly, the trace of the windowed influence matrix can be written as
\begin{equation}
    \trace\left(A_\text{win}(\regparamVec)\right) = \sum_{j=1}^{n} \left(\sum_{p=1}^{P} w_j^{(p)} \filt\left(\regparam^{(p)},\singular_j\right) \right).
\end{equation}
Thus \eqref{eq:Multiparameter UPRE} becomes
\begin{equation}
\label{eq:Multiparameter UPRE 2}
    \UWin(\regparamVec) = \frac{1}{\mA}\sum_{j=1}^{n} \left(\sum_{p=1}^{P} w_j^{(p)} \mfilt\left(\regparam^{(p)},\singular_j\right) \right)^2 \left(\dft{d}_j\right)^2 + \frac{2\noiseSD^2}{\mA}\sum_{j=1}^{n} \left(\sum_{p=1}^{P} w_j^{(p)} \filt\left(\regparam^{(p)},\singular_j\right) \right) - \noiseSD^2.
\end{equation}

The MDP and GCV methods, described in Sections \ref{sec:MDP} and \ref{sec:GCV} respectively, involve the norm of a regularized residual. The exact form of this term depends upon the setting. In the single parameter, single data set case, the regularized residual $\rVec(\regparam)$ is given by \eqref{eq:Predictive Error and Regularized Residual}. In the single parameter, multiple data set case, the norm of the regularized residual $\rBig(\regparam)$ can be written as a sum of terms $\|\rVec^{(p)}(\regparam)\|_2^2$ with $\rVec^{(p)}(\regparam)$ given by \eqref{eq:Single Regularized Residual}. For a multiparameter problem with a single data set, we use $\|\rVec_{\text{win}}(\regparamVec)\|_2^2$ given by \eqref{eq:Windowed Regularized Residual}. Proposition \ref{prop:Non-overlapping windows} describes how $\|\rVec_{\text{win}}(\regparamVec)\|_2^2$ can be decomposed when working with non-overlapping windows.

\begin{proposition}
\label{prop:Non-overlapping windows}
If the weight vectors $\{\wVec^{(p)}\}_{p=1}^{P}$ satisfy \eqref{eq:Non-overlapping window condition}, then $\|\rVec_{\text{win}}(\regparamVec)\|_2^2$ can be written as
\[\left\|\rVec_{\text{win}}(\regparamVec)\right\|_2^2 = \sum_{p=1}^{P} \left\|\rVec^{(p)}(\regparam)\right\|_2^2\]
where
\[\left\|\rVec^{(p)}(\regparam)\right\|_2^2 = \sum_{j=1}^{n} \left( \wVec_j^{(p)} \mfilt\left(\regparam^{(p)},\singular_j\right)\dft{d}_j\right)^2..\]
\end{proposition}
\begin{proof}
Using \eqref{eq:Windowed Regularized Residual}, we can write $\|\rVec_{\text{win}}(\regparamVec)\|_2^2$ as
\[\left\|\rVec_{\text{win}}(\regparamVec)\right\|_2^2 = \sum_{j=1}^{n} \left(\sum_{p=1}^{P} \wVec_j^{(p)} \mfilt\left(\regparam^{(p)},\singular_j\right) \right)^2 \left(\dft{d}_j\right)^2.\]
Since the weight vectors $\{\wVec^{(p)}\}_{p=1}^{P}$ satisfy \eqref{eq:Non-overlapping window condition}, for each index $j$ there exists exactly one index $p$ such that $\wVec^{(p)}_j \neq 0$. Thus the sum over $p$ has only one nonzero summand for each index $j$, meaning we can write
\[\left(\sum_{p=1}^{P} \wVec_j^{(p)} \mfilt\left(\regparam^{(p)},\singular_j\right) \right)^2 = \sum_{p=1}^{P} \left( \wVec_j^{(p)} \mfilt\left(\regparam^{(p)},\singular_j\right)\right)^2.\]
Therefore $\|\rVec_{\text{win}}(\regparam)\|_2^2$ can be rewritten through a change of summation so that
\[\|\rVec_{\text{win}}(\regparamVec)\|_2^2 = \sum_{p=1}^{P} \left[ \sum_{j=1}^{n} \left( \wVec_j^{(p)} \mfilt\left(\regparam^{(p)},\singular_j\right) \right)^2 \left(\dft{d}_j\right)^2\right] = \sum_{p=1}^{P} \left\|\rVec^{(p)}(\regparam)\right\|_2^2\]
with
\[\left\|\rVec^{(p)}(\regparam)\right\|_2^2 = \sum_{j=1}^{n} \left( \wVec_j^{(p)} \mfilt\left(\regparam^{(p)},\singular_j\right)\dft{d}_j\right)^2.\]
\end{proof}

\subsection{Morozov's discrepancy principle} \label{sec:MDP}
Similar to the UPRE method, the MDP method relies on knowledge of the variance of $\noiseVec$. If $\noise \sim \mathcal{N}(\bm{0},\noiseSD^2 I_{\mA})$, then the MDP function is 
\begin{equation}
\label{eq:MDP}
\D(\regparam) = \frac{1}{\mA}\|\rVec(\regparam)\|_2^2 - \noiseSD^2.
\end{equation}
The function \eqref{eq:MDP} stems from the observation that if $\xReg = \xVec$, then $\E(\frac{1}{\mA}\|\rVec(\regparam)\|_2^2) = E(\frac{1}{\mA}\|\noiseVec\|_2^2) = \noiseSD^2$. The MDP method selects $\regparam_{\text{MDP}}$ as the zero of $\D(\regparam)$.  \par 
Before extending the MDP method to account for multiple data sets, some comments on the behavior of the MDP function \eqref{eq:MDP} are warranted. The term $\frac{1}{\mA}\|\rVec(\regparam)\|_2^2$ is a monotone increasing function of $\regparam$ for $\regparam > 0$, which can be seen by differentiating the summands of \eqref{eq:Fourier regularized residual} for $R = 1$:
\[\frac{d}{d\regparam}\left(\left(\mfilt_j\right)^2\left(\dft{\dVec}_j\right)^2\right) = 2 \left(\dft{\dVec}_j\right)^2 \mfilt_j\left(\frac{d}{d\regparam}\mfilt_j\right) = 2 \left(\dft{\dVec}_j\right)^2 \mfilt_j \frac{2\regparam\bm{\lambda_j}\bm{\delta_j}}{\left(\bm{\delta}_j + \regparam^2 \bm{\lambda}_j\right)^2} \geq 0, \quad \regparam > 0.\]
The monotonicity of $\frac{1}{\mA}\|\rVec(\regparam)\|_2^2$ does not guarantee, however, the existence of a zero of $\D(\regparam)$. If the selected value of $\noiseSD^2$ is too large, then it is possible that $\D(\regparam) < 0$ for all $\regparam > 0$ and a root will not exist. This can be attributed to the limiting behavior of $\frac{1}{\mA}\|\rVec(\regparam)\|_2^2$, which is described by Lemma \ref{lem:Residual limit}.
\begin{lemma}
\label{lem:Residual limit}
\[\lim_{\regparam\rightarrow\infty} \frac{1}{\mA}\|\rVec(\regparam)\|_2^2 \leq \frac{1}{\mA}\|\dVec\|_2^2.\]
\end{lemma}
\begin{proof}
Writing $\frac{1}{\mA}\|\rVec(\regparam)\|_2^2$ in terms of filter functions \eqref{eq:Filter functions},
\[\lim_{\regparam\rightarrow\infty} \frac{1}{\mA}\|\rVec(\regparam)\|_2^2 = \lim_{\regparam\rightarrow\infty} \frac{1}{\mA} \sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\dft{d}_j\right)^2 = \frac{1}{\mA} \sum_{j=1}^{n} \left(\lim_{\regparam\rightarrow\infty}\left(\mfilt_j\right)^2\right)\left(\dft{d}_j\right)^2.\]
Expanding the denominator of $\left(\mfilt_j\right)^2$ makes the limit clear:
\[\lim_{\regparam\rightarrow\infty} \left(\mfilt_j\right)^2 = \lim_{\regparam\rightarrow\infty}\frac{\regparam^4\bm{\lambda}_j}{\regparam^4\bm{\lambda}_j + 2\regparam^2\bm{\lambda}_j\bm{\delta}_j + \bm{\delta}_j^2} = \begin{cases}
1, & \bm{\lambda}_j \neq 0 \\
0, & \bm{\lambda}_j = 0
\end{cases}.\]
Therefore
\[\lim_{\regparam\rightarrow\infty} \frac{1}{\mA}\|\rVec(\regparam)\|_2^2 = \frac{1}{\mA} \sum_{j=1}^{n} \left(\lim_{\regparam\rightarrow\infty}\left(\mfilt_j\right)^2\right)\left(\dft{d}_j\right)^2 \leq \frac{1}{\mA} \sum_{j=1}^{n} \left(\dft{d}_j\right)^2 = \frac{1}{\mA}\|\svd{\dVec}\|_2^2 = \frac{1}{\mA}\|\dVec\|_2^2,\]
with the last equality following from $\svd{\dVec} = \trans{U}\dVec$ with orthogonal $U$.
\end{proof}
\noindent Lemma \ref{lem:Residual limit} implies that if the selected value of $\noiseSD^2$ is larger than $\frac{1}{\mA}\|\dVec\|_2^2$, $\D(\regparam)$ will not have a root for $\regparam > 0$ and the MDP method fails to select a regularization parameter. Sometimes a safety parameter $\safeparam > 0$ is introduced to modify the MDP function to
\begin{equation}
\label{eq:MDP Safety}
\D(\regparam) = \frac{1}{\mA}\|\rVec(\regparam)\|_2^2 - \safeparam\noiseSD^2.
\end{equation}
to account for root-finding difficulties \cite{ABT,IRTools}, though selecting an appropriate value of $\safeparam$ is an ad hoc process and depends on the confidence of $\noiseSD^2$ as the true noise. The original MDP function is recovered from \eqref{eq:MDP Safety} when $\safeparam = 1$. \par
The MDP function for the more general assumption that $\noiseVec \sim \mathcal{N}(\zeroVec,\Sigma)$ will now be presented. Since $\E(\frac{1}{\mA}\|\rVec(\regparam)\|_2^2) = E(\frac{1}{\mA}\|\noiseVec\|_2^2)$ and $E(\|\noiseVec\|_2^2) = \trace(\Sigma)$, the corresponding MDP function is
\begin{equation}
\label{eq:General MDP}
\DBig(\regparam) = \frac{1}{\mA}\|\rVec(\regparam)\|_2^2 - \frac{1}{\mA}\trace\left(\Sigma\right).
\end{equation}
Function \eqref{eq:General MDP} can be applied to data $\{\dVec\}_{r=1}^R$ with constructions described in Table \ref{tab:System assumptions}. 

\begin{proposition}
Using the constructions in Table \ref{tab:System assumptions} under Assumption \ref{Assumption_System} with $n_\ell = n$ for all $r = 1,\ldots,R$, the MDP function $\DBig(\regparam)$ for the data sets $\{\dVec^{(r)}\}_{r=1}^R$ is
\begin{equation}
\label{eq:Averaged MDP}
\DBig(\regparam) = \frac{1}{R} \sum_{r=1}^R \D^{(r)}(\regparam),
\end{equation}
where
\begin{equation}
\label{eq:Individual MDP}
\D^{(r)}(\regparam) = \D^{(r)}(\regparam) = \frac{1}{\mA}\|\rVec^{(r)}(\regparam)\|_2^2 - \frac{1}{\mA}\trace\left(\Sigma^{(r)}\right), \quad r = 1,\ldots,R
\end{equation}
with $\rVec^{(r)} = A^{(r)}\xVec^{(r)}(\regparam) - \dVec^{(r)}$.
\end{proposition}

\begin{proof}
We first define
\begin{equation}
\label{eq:Big MDP}
\DBig(\regparam) = \frac{1}{N}\|\rBig(\regparam)\|_2^2 - \frac{1}{N}\trace\left(\widetilde{\Sigma}\right).
\end{equation}
By defining the individual MDP functions by \eqref{eq:Individual MDP}, the MDP function \eqref{eq:Big MDP} for the large system can be written as an average by exploiting the block structure of $\ABig(\regparam)$ and $\widetilde{\Sigma}$:
\begin{align*}
\DBig(\regparam) &= \frac{1}{N}\sum_{r=1}^R \|\rVec^{(r)}(\regparam)\|_2^2 - \frac{1}{N}\sum_{r=1}^R \trace\left(\Sigma^{(r)}\right) \nonumber \\
&= \frac{1}{R}\sum_{r=1}^R \frac{1}{\mA}\|\rVec^{(r)}(\regparam)\|_2^2 - \frac{1}{R}\sum_{r=1}^R \frac{1}{\mA}\trace\left(\Sigma^{(r)}\right) \nonumber \\
&= \frac{1}{R}\sum_{r=1}^R \D^{(r)}(\regparam).
\end{align*}
\end{proof}

The MDP method for multiple data then defines $\widetilde{\regparam}_{\textrm{MDP}}$ as the zero of $\DBig(\regparam)$. Analogous to the UPRE method, \eqref{eq:Individual MDP} is equivalent to \eqref{eq:MDP} under Assumption \ref{Assumption_Noise}. A safety parameter $\safeparam$ can also be included in the trace terms of \eqref{eq:Big MDP} and \eqref{eq:Individual MDP} for more control over selected parameters. \par 
As with the adapted UPRE method, the adapted MDP method is distinct from the MDP method as applied to the averaged data \eqref{eq:Averaged data}.

\begin{proposition}
Under Assumptions \ref{Assumption_System} through \ref{Assumption_Diagonalization} with data vectors $\{\dVec^{(r)}\}_{r=1}^R$, let $\aVec$ be defined by \eqref{eq:Averaged data}, $\dft{\aVec} = \trans{U}\aVec$, and let
\begin{equation}
\label{eq:MDP of Average}
\DAvg(\regparam) = \frac{1}{\mA}\sum_{j=1}^{n} \left(\mfilt_j\right)^2 |\dft{\aVec}_j|^2 - \frac{1}{R^2}\sum_{r=1}^R \noiseSD_{r}^2
\end{equation}
Then
\begin{equation}
\label{eq:MDP Bound}
\DBig(\regparam) = \frac{1}{R} \sum_{r=1}^R \D^{(r)}(\regparam) \leq R \DAvg(\regparam), \qquad \regparam \geq 0.
\end{equation}
\end{proposition}
\begin{proof}
Under these assumptions, for the MDP method we have $\DBig(\regparam)$ is equal to
\begin{align*}
    \frac{1}{R} \sum_{r=1}^R \D^{(r)}(\regparam) &= \frac{1}{R}\sum_{r=1}^R \left[\frac{1}{\mA}\|\rVec^{(r)}(\regparam)\|_2^2 - \noiseSD_{r}^2\right] \\
    &= R\left[\frac{1}{\mA}\sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\frac{1}{R^2} \sum_{r=1}^R |\dft{\dVec}_j^{(r)}|^2\right) - \frac{1}{R^2}\sum_{r=1}^R \noiseSD_{r}^2\right]
\end{align*}
so that using \eqref{eq:MDP of Average} results in the bound \eqref{eq:MDP Bound}.
\end{proof}

\subsubsection{Multiparameter MDP} \label{sec:Multiparameter MDP}
The multiparameter MDP function is
\begin{equation}
\label{eq:Windowed MDP}
    \DWin(\regparamVec) = \left\|\rVec_\text{win}(\regparamVec)\right\|_2^2 - \noiseSD^2
\end{equation}
where $\rVec_\text{win}(\regparamVec)$ is defined by \eqref{eq:Windowed Regularized Residual}. Analogous to the single parameter MDP method, the multiparameter MDP method is to choose $\regparamVec \in \mathbb{R}_+^p$ such that $\DWin(\regparamVec) = 0$.

\subsection{Generalized cross validation} \label{sec:GCV}
Unlike the UPRE and MDP methods, the GCV method does not depend upon knowledge of the noise variance. For a single data set $\dVec = A\xVec + \noiseVec$, the GCV function is defined as
\begin{equation}
\label{eq:GCV}
\G(\regparam) = \frac{\frac{1}{\mA}\|\rReg\|_2^2}{\left[\frac{1}{\mA}\trace(I_{\mA} - \A)\right]^2} = \frac{\frac{1}{\mA}\|\rReg\|_2^2}{\left[1 - \frac{1}{\mA}\trace(\A)\right]^2}.
\end{equation}
The GCV method determines $\regparam$ such that $\regparam_{\text{GCV}} = \argmin_{\regparam > 0} \G(\regparam)$.

\begin{proposition}
Using the constructions in Table \ref{tab:System assumptions} under Assumption \ref{Assumption_System} with $n_r = n$ for all $r = 1,\ldots,R$, the GCV function $\DBig(\regparam)$ for the data sets $\{\dVec^{(r)}\}_{r=1}^R$ is
\begin{equation}
\label{eq:GCV Big 2}
\GBig(\regparam) = \frac{\frac{1}{N}\sum_{r=1}^R \|\rVec^{(r)}(\regparam)\|_2^2}{\left[1 - \frac{1}{N}\sum_{r=1}^R \trace\left(A^{(r)}(\regparam)\right)\right]^2}.
\end{equation}
\end{proposition}
\begin{proof}
We define the GCV function for multiple data sets as
\begin{equation}
\label{eq:GCV Big}
\GBig(\regparam) = \frac{\frac{1}{N}\|\rBig(\regparam)\|_2^2}{\left[1 - \frac{1}{N}\trace\left(\ABig(\regparam)\right)\right]^2}.
\end{equation}
From the definition of $\widetilde{\mathbf{r}}_\regparam$ and the block structure $\ABig(\regparam)$, \eqref{eq:GCV Big} can be rewritten as \eqref{eq:GCV Big 2}.
\end{proof} 
\noindent The GCV method for multiple data sets defines $\widetilde{\regparam}_{\textrm{GCV}} = \argmin_{\regparam > 0} \GBig(\regparam)$. \par 
In contrast to the UPRE and MDP methods, the GCV function \eqref{eq:GCV Big 2} for multiple data sets is not equal to the mean of individual GCV functions
\begin{equation}
\label{eq:Individual GCV}
\G^{(r)}(\regparam) = \frac{\frac{1}{\mA}\|\rVec^{(r)}(\regparam)\|_2^2}{\left[1 - \frac{1}{\mA}\trace\left(\A^{(r)}\right)\right]^2}, \quad r = 1,\ldots,R.
\end{equation}
However, if we assume that $A^{(r)} = A$ and $L^{(r)} = L$ for each $r = 1,\ldots,R$, then $\A^{(r)} = \A$ for each $r = 1,\ldots,R$ as well. In such case, \eqref{eq:GCV Big 2} can be expressed as
\begin{equation}
\label{eq:Averaged GCV}
\GBig(\regparam) = \frac{\frac{1}{N}\sum_{r=1}^R \|\rVec^{(r)}(\regparam)\|_2^2}{\left[1 - \frac{1}{N}\sum_{r=1}^R \trace\left(\A\right)\right]^2} = \frac{1}{R}\frac{\sum_{r=1}^R \frac{1}{\mA} \|\rVec^{(r)}(\regparam)\|_2^2}{\left[1 - \frac{1}{\mA} \trace\left(\A\right)\right]^2} = \frac{1}{R}\sum_{r=1}^R \G^{(r)}(\regparam).
\end{equation}
Note that \eqref{eq:Individual GCV} is equivalent to \eqref{eq:GCV} without Assumption \ref{Assumption_Noise}, which is to be expected since the GCV method does not rely on knowledge of the noise variance. \par 
A bound analogous to \eqref{eq:UPRE Bound} and \eqref{eq:MDP Bound} can be obtained for the adapted GCV function.

\begin{proposition}
Under Assumptions \ref{Assumption_System} through \ref{Assumption_Diagonalization} with data vectors $\{\dVec^{(r)}\}_{r=1}^R$, let $\aVec$ be defined by \eqref{eq:Averaged data}, $\dft{\aVec} = \trans{U}\aVec$, and let
\begin{equation}
\label{eq:GCV of Average}
\GAvg(\regparam) = \frac{\frac{1}{\mA}\sum_{j=1}^{n} \left(\mfilt_j\right)^2|\dft{\aVec}_j|^2}{\left[1 - \frac{1}{\mA}\trace\left(\A\right)\right]^2}.
\end{equation}
Then
\begin{equation}
\label{eq:GCV Bound}
\GBig(\regparam) = \frac{1}{R} \sum_{r=1}^R \G^{(r)}(\regparam) \leq R \GAvg(\regparam), \qquad \regparam \geq 0.
\end{equation}
\end{proposition}

\begin{proof}
With these assumptions,
\begin{align}
    \GBig(\regparam) &= \frac{1}{R} \sum_{r=1}^R \G^{(r)}(\regparam) \\
    &= \frac{1}{R}\sum_{r=1}^R \left[\frac{\frac{1}{\mA}\|\rVec^{(r)}(\regparam)\|_2^2}{\left[1 - \frac{1}{\mA}\trace\left(\A\right)\right]^2}\right] \\
    &= R\frac{\frac{1}{\mA}\sum_{j=1}^{n} \left(\mfilt_j\right)^2\left(\frac{1}{R^2} \sum_{r=1}^R |\dft{\dVec}_j^{(r)}|^2\right)}{\left[1 - \frac{1}{\mA}\trace\left(\A\right)\right]^2},
\end{align}
again where the last equality is obtained through multiplication by $\frac{R}{R}$. The definition of $\GAvg(\regparam)$ and \eqref{eq:Coefficients of Average} then results in the bound \eqref{eq:GCV Bound}.
\end{proof}

As a closing remark regarding the GCV method, it is important to note that the assumption $A^{(r)} = A$ and $L^{(r)} = L$ for each $r = 1,\ldots,R$ was needed for result \eqref{eq:Averaged GCV}. In contrast, no such assumption is necessary for the corresponding UPRE and MDP results, which respectively are \eqref{eq:Averaged UPRE} and \eqref{eq:Averaged MDP}.

\subsubsection{Multiparameter GCV} \label{sec:Multiparameter GCV}
The multiparameter GCV function is
\begin{equation}
\label{eq:Windowed GCV}
    \GWin(\regparamVec) = \frac{\frac{1}{\mA}\|\rVec_{\text{win}}(\regparam)\|_2^2}{\left[1 - \frac{1}{\mA}\trace(\A)\right]^2}.,
\end{equation}
again where $\rVec_\text{win}(\regparamVec)$ is defined by \eqref{eq:Windowed Regularized Residual}. As with the single parameter GCV method, the multiparameter GCV method is to choose $\regparamVec \in \mathbb{R}_+^p$ such that $\GWin(\regparamVec) = 0$. $\trans{A}_{\text{win}}$

\section{Validation} \label{sec:Validation}
For a basis of comparison, parameters were selected as minimizers of the function
\begin{equation}
\label{eq:Learning function}
\widetilde{F}_{\text{best}}(\regparam) = \frac{1}{R}\sum_{r=1}^R \left\|\xVec^{(r)}(\regparam) - \xVec^{(r)}\right\|_2^2,
\end{equation} 
which was considered in \cite{ChungEspanol2017}. A simpler way of selecting regularization parameters is to minimize
\begin{equation}
\label{eq:Best function}
F_{\text{best}}^{(r)}(\regparam) = \left\|\xVec^{(r)}(\regparam) - \xVec^{(r)}\right\|_2^2, \quad r = 1,\ldots,R.
\end{equation}
Both functions \eqref{eq:Learning function} and \eqref{eq:Best function} rely on knowledge of the true solutions $\{\xVec^{(r)}\}_{r=1}^R$. Parameters chosen as minimizers of \eqref{eq:Best function} are ``best" in the sense of minimizing the norm of the error of a single regularized solution. It is important to note that minimization of \eqref{eq:Learning function} results in a single parameter depending on all $R$ data vectors, while minimizing \eqref{eq:Best function} results in a single parameter for each of the $R$ data vectors. The relationship between \eqref{eq:Learning function} and \eqref{eq:Best function} can be expressed as
\[\widetilde{F}_{\text{best}}(\regparam) = \frac{1}{R}\sum_{r=1}^R F_{\text{best}}^{(r)}(\regparam).\]
\indent To evaluate the effectiveness of both the adapted single-parameter and multiparameter selection methods described in Section \ref{sec:Methods}, three test problems were considered. For all three problems, data vectors were considered as a set of training data and resulting parameters were then applied to validation data set. A one-dimensional problem serves as a proof of concept and uses MRI data that is built into MATLAB, which is shown in Figure \ref{fig:MRI 1D}. The other problems are two-dimensional; one is constructed from the MATLAB MRI data, while the other utilizes images of the planet Mercury obtained by the MESSENGER space probe. The MESSENGER images are available to the public courtesy of NASA, the Johns Hopkins University Applied Physics Laboratory, and the Carnegie Institution of Washington \cite{MESSENGER}. \par
For both problems, the following definition of signal-to-noise ratio (SNR) is used as a measurement for noise content in the images:
\begin{equation}
\label{eq:SNR}
\text{SNR} = 10\log_{10}\left(\frac{\mathcal{P}_{\text{signal}}}{\mathcal{P}_{\text{noise}}}\right).
\end{equation}
In the discrete setting, the average power $\mathcal{P}$ of a vector $\xVec$ of length $n$ is defined as $\frac{1}{\mA}\|\xVec\|^2_2$. Using this definition for vectors $\bVec$ and $\noiseVec$, $\mathcal{P}_{\text{signal}} = \frac{1}{\mA}\|\bVec\|^2_2$ and $\mathcal{P}_{\text{noise}} = \frac{1}{\mA}\|\noiseVec\|^2_2$ and so the quotient in the logarithm is $\|\bVec\|_2^2/\|\noiseVec\|_2^2$. If $\bVec$ is a matrix representing an image, in which case $\noiseVec$ is a realization of a random matrix, the 2-norms can be replaced by the Frobenius norm.

\subsection{One-dimensional problem} \label{sec:1D}
The one-dimensional test problem uses the MRI data built into MATLAB, which can be accessed by the command \texttt{load mri.mat}. Five horizontal slices were selected and reformatted as a single image (Figure \ref{fig:MRI 1D}) of dimensions $256 \times 536$. 
\begin{figure}[ht]
\includegraphics[scale=0.32]{Figures/Full_MRI_Data.eps}
\caption{MRI data formed by reformatting the MATLAB built-in MRI data. Each vector was blurred using a Gaussian kernel and normal noise was added; the resulting vectors are shown at the bottom. The dimension of all three images is $256 \times 536$.}
\label{fig:MRI 1D}
\end{figure}
The default dimension of each horizontal MRI slice accessible using \texttt{load mri.mat} is $128 \times 128$. Linear interpolation was used to double the number of rows; the number of columns of the concatenated MRI slices was trimmed to to eliminate leading and trailing zero columns. The columns of the image were then multiplied by a symmetric Toeplitz matrix, approximating a Fredholm integral equation of the first kind with a Gaussian kernel of mean 0 and variance 1 (each pixel representing a unit square in the continuous problem). The resulting MRI image is vertically blurred; see Figure \ref{fig:MRI 1D}. SNR values were selected randomly between 6 and 7 for each blurred column, and realizations of normal noise with the corresponding variances were added to blurred columns. The first $536/2 = 268$ columns of Figure \ref{fig:MRI 1D} served as the training set, while the remaining 268 columns serve as a validation set.  The following $255 \times 256$ penalty matrix $L_1$ was used, which represents an approximation of a first derivative operator:
\[L_1 = \begin{bmatrix}
-1 & 1 & &  \\
 & \ddots &  \ddots & \\
 & & -1 & 1 \\
\end{bmatrix}.\]
The resulting system matrix in \eqref{eq:TikSol3} has full column rank, and so applying the normal equations in terms of the GSVD results in unique solutions. \par 
Figure \ref{fig:Parameters 1D MRI} illustrates a comparison of the methods in terms of the regularization parameters $\regparam$ selected as the number of training vectors increases. 

\begin{figure}[ht]
\includegraphics[scale=0.36]{Figures/Parameters1D_mri}
\caption{Trend of parameters selected by each adapted method as the number of training vectors increases. Note that the parameters essentially stabilize by about 40 training vectors out of the possible 268.}
\label{fig:Parameters 1D MRI}
\end{figure}

\noindent When the number of training vectors is small (e.g. between 1 and 10 from Figure \ref{fig:Parameters 1D MRI}), the parameters selected by all four methods change significantly. This can be attributed to the fact that all of the methods select a parameter that is either a root or a minimum of an average of functions. For a small amount of training vectors, each additional vector has more influence over the behavior of the adapted function. The parameters do seem to stabilize as the number of training vectors reach a certain point; in the problem being considered here, the parameters stabilize by about 40 training sets. The stabilization of the parameter most likely follows from the fact that each vector is part of the same type of image (MRI). It would seem unreasonable to expect the same stabilization when using vectors pulled from dissimilar images. \par 
Before looking at the relative errors of the regularized solutions, another observation regarding Figure \ref{fig:Parameters 1D MRI} can be made. While the parameters selected using the learning function \eqref{eq:Learning function} and the adapted UPRE and GCV methods are close (the maximum difference in the parameters does not exceed 0.5), the adapted MDP method consistently selected significantly larger parameters. A direct consequence is that the regularized solutions have higher relative errors. \par
Figure \ref{fig:Errors 1D MRI} shows the relative errors of the regularized solutions corresponding to the parameters from each method. 

\begin{figure}[ht]
\includegraphics[scale=0.36]{Figures/Errors1D_mri}
\caption{Boxplots of the relative errors of each regularized solution versus the validation set. A parameter was selected using the full training set for each method, and the parameters of each method were used to generate regularized solutions from the validation set.}
\label{fig:Errors 1D MRI}
\end{figure}

\noindent Instead of using a relatively small amount of training vectors, like 40 out of 268 in Figure \ref{fig:Parameters 1D MRI}, the full training set was used to select a parameter from each method. These four parameters were then used to generate a regularized solution for each vector in the validation set, and the relative errors were computed against the true solutions. First, the relative errors obtained by the learning method and adapted UPRE and GCV methods are quite similar; the means of the relative errors are close, and there is a collection of upper outliers. In contrast, the adapted MDP method has a slightly larger mean relative error. However, the most striking visual characteristic from Figure \ref{fig:Errors 1D MRI} is the number of outliers in the MDP relative errors. The largest relative error obtained using the adapted MDP method is about twice the largest relative error of the other three methods. \par 
The one-dimensional test problem demonstrates that the adapted methods have potential for selecting viable regularization parameters that can be applied to multiple sets of data. The adapted UPRE and GCV performed competitively against the learning method, which relies on knowledge of the true training solutions. As stated previously, the success of the adapted methods is predicated on the similarity of the data sets being considered. However, this is not an unreasonable condition because data from a given experiment would hopefully vary little without significant change in the experimental set-up.

\subsection{Two-dimensional problems} \label{sec:2D}
The two-dimensional test problem again uses the MRI data built into MATLAB. However, the data sets now consist of images of size $256 \times 256$. A total of 20 default MRI images were used and split into training and validation sets containing 10 images each. For each MRI image, seven more images were generate through the use of random rigid transformations (rotations and translations) so that the training set and validation set each contained 80 images. A rotation angle $\theta$ and a translation angle $\phi$ were selected randomly between 0 and $2\pi$. Another scalar $c$ was drawn randomly between 0 and 64 so that the translation vector $\tVec = \trans{[c\cos(\phi),c\sin(\phi)]}$ does not have a magnitude that exceeds 64 pixels. Figure \ref{fig:MRI Transformations} shows examples of true MRI images used in the training set. 

\begin{figure}[ht]
\includegraphics[scale=0.36]{Figures/MRI_Transformations}
\caption{Eight examples of true versions of training MRI images, each of size $256 \times 256$. The top left image is a default MRI image, while the remaining seven were generated from random rigid transformations. Nearest-neighbor interpolation was used when transformed pixel locations did not align with the rectangular grid.}
\label{fig:MRI Transformations}
\end{figure}

Circularly convolution of each image with a Gaussian kernel of mean 0 and variance 1 in each direction was then carried out to blur the images. Lastly, SNR values were randomly selected between 8 and 10 and realizations of normal noise with corresponding variance were added to the blurred images. \par 
Since circular convolution was utilized, the DFT was the primary tool for this MRI test problem as opposed to the GSVD. To ensure simultaneous diagonalization of the block system matrices $A$ and $L$, periodic boundary conditions were assumed so that both $A$ and $L$ are block-circulant-with-circulant-blocks (BCCB); see \cite{NeumannDCT,Vogel:2002}. The penalty matrix $L$ was selected to be the BCCB version of the discrete negative Laplacian operator, which is an approximation of the continuous Laplacian operator \cite{DebnathMikusinski2005,LeVeque2007}. The BCCB structure of $A$ and $L$ allows for simultaneous diagonalization using two-dimensional DFT for numerical efficiency. \par 
Figure \ref{fig:Parameters 2D} shows the parameters selected using each method. 
\begin{figure}[ht]
\includegraphics[scale=0.36]{Figures/Parameters2D_mri}
\caption{hello}
\label{fig:Parameters 2D}
\end{figure}
In contrast to the one-dimensional case, the adapted MDP method returned parameters closest to the parameters chosen based on the machine learning method. The adapted UPRE and GCV method selected similar parameters. Another observation from Figure \ref{fig:Parameters 2D} is that the parameter obtained from each method are stable with respect to the number of images considered. This suggest that only small subset of the training set is needed to obtain regularization parameters that can be then applied to a full validation set. Figure \ref{fig:Errors 2D} contains boxplots of the relative errors of the regularized solutions for the validation set using 10 training images. 
\begin{figure}[ht]
\includegraphics[scale=0.36]{Figures/Errors2D_mri}
\caption{hello}
\label{fig:Errors 2D}
\end{figure}
The errors of the regularized solutions obtained using the adapted MDP method are close to those of the machine learning method.

\begin{figure}
\label{fig:MESSENGER True}
\includegraphics[scale=0.36]{Figures/MESSENGER}
\caption{Selected images used for MESSENGER two-dimensional test problem. Available courtesy of NASA/Johns  Hopkins  University  Applied  Physics  Laboratory/Carnegie Institution of Washington \cite{MESSENGER}.}
\end{figure}

\subsection{Multiparameter regularization}

\subsubsection{Two-dimensional problems}

\section{Conclusions and future work} \label{sec:Conclusion}
In conclusion, this paper demonstrate a means by which UPRE, MDP, and GCV methods can be extended to accommodate multiple data sets. The most general forms of functions associate with these methods are \eqref{eq:UPRE 3}, \eqref{eq:Big MDP}, \eqref{eq:GCV Big}, and , respectively. The adapted UPRE and MDP methods are novel in that their corresponding functions can be written as an average of the individual functions associate with each data set. None of the three adapted methods require knowledge of true solutions unlike the machine learning approach defined by \eqref{eq:Learning function}, though the MRI test problems demonstrate that the adapted methods can perform competitively. \par
There are various directions of future investigation. Perhaps the most significant results could be obtained through approaches of determining the necessary number of data sets for a stabilization of selected parameters. Figure \ref{fig:Parameters 1D MRI} and Figure \ref{fig:Parameters 2D} suggest that obtained parameters stabilize fairly fast in relation to the number of data sets incorporate into the method functions. Another direction of future work would be to use the adapted methods on data sets containing images of differing objects. The MRI images used here are all visualizations of a human brain; it would be interesting to see how these methods perform for drastically different data sets.

\bibliographystyle{siam}
\bibliography{Parameter-Estimation}

\end{document}
