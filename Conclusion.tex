\chapter{Conclusions and future work} \label{ch:Conclusion}
Overall the numerical results demonstrate some potential viability of downsampling to obtain regularization parameters. At the least, the graphs in Sections \ref{sec:DFT} and \ref{sec:DCT} show that the statistics of the regularization parameters obtained have structure. The DFT and DCT versions of the UPRE, GCV, and discrepancy principle functions in Chapter \ref{ch:Parameter estimation methods} are useful when the two transforms are to be used in the inversion process. The corresponding statistical results of the function in Chapter \ref{ch:Parameter estimation methods}, such as \eqref{eq:UPRE Var Sum Simple}, could be useful from an analytic standpoint. \par 
There are many possible directions of future work. The most important is to quantify the statistics of the regularization parameters when downsampling is applied. Fortunately the DFT and DCT are topics which have been thoroughly investigated in a number of settings. For example, there are explicit connections between downsampling and aliasing with regard to the DFT presented in \cite[Ch.~7]{AudioDFT}; versions of these results could be derived for DCT.  Another means of quantification could be to utilize the statistics of the parameter estimation functions and try to estimate intervals in which $\regparam$ will be found for each downsampling level. In some information about the frequency content (in regard to the DFT or DCT) of the continuous solution $f(t)$ is available, then the information can be used to select an appropriate downsampling level to capture the overall behavior of the solution; the ideal result would be to explicitly show how this process effects the resulting value of $\regparam$. \par 
Another direction of future work would be to investigate the situation where multiple data sets are available. The UPRE, GCV, and discrepancy principle functions for the availability of multiple data sets are \eqref{eq:SpectralUPREavg}, \eqref{eq:SpectralGCVsum}, and \eqref{eq:SpectralDPavg}, respectively. While the assumption of multiple data sets could produce some interesting results, this assumption is not appropriate for all applications. One issue is that repeated measurements cost time and additional resources. Another issue is that repeated measurements could alter the solution itself. For example, computed tomographic imaging used X-rays to obtain measurements regarding the internal structure of the object \cite[p.~12-14]{ABT} In the object in question is a human being, repeated measurements can result in cell damage. \par 
As a final and overarching direction for future work, three-dimensional problems will be considered. Examples of setting that produce three-dimensional inverse problems include computed tomography and subsurface imaging by gravitational measurements \cite{ABT}. The primary challenges of working with three-dimensional problems are the numerical structure of the problems themselves, such as how to correctly express the problems as matrix-vector product, as well as the computational costs of solving the problems. However, three-dimensional problems could be the types of problems where downsampling is most useful. To illustrate this intuition, if the unit cube $[0,1] \times [0,1] \times [0,1]$ is discretized by using the vector $[0,\frac{1}{4},\frac{1}{2},\frac{3}{4}]$ along each dimension, then downsampling to the vector $[0,\frac{1}{2}]$ reduces the number of points from 64 to 8. While the quantitative effects of downsampling should be determined before moving to three-dimensional problems, three-dimensional problems possess the most potential for demonstrating these effects.
